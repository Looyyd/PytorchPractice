{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.373289Z",
     "start_time": "2024-01-15T21:10:41.174819Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.638023Z",
     "start_time": "2024-01-15T21:10:44.264322Z"
    }
   },
   "id": "655c4f8ad84a6e65"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(n_states, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.match_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.match_channels(x)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out + residual\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, n_actions, conv_layers=None, dropout_p=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        if conv_layers is None:\n",
    "            conv_layers = [16, 32]\n",
    "        layers = []\n",
    "        in_channels = 4  # Initial number of channels\n",
    "\n",
    "        for out_channels in conv_layers:\n",
    "            layers.append(ConvBlock(in_channels, out_channels))\n",
    "            #if dropout_p is not None:\n",
    "                #layers.append(nn.Dropout(dropout_p))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(conv_layers[-1] * input_size, n_actions)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        if dropout_p is not None:\n",
    "            self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "            #if self.dropout_p is not None:\n",
    "            #    x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.dropout_p is not None:\n",
    "            x = self.dropout(x)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.698866Z",
     "start_time": "2024-01-15T21:10:44.354766Z"
    }
   },
   "id": "fc04ac7142783e42"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "8004"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter count\n",
    "model_to_count = ConvNet(n_states, n_actions)\n",
    "sum(p.numel() for p in model_to_count.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.797071Z",
     "start_time": "2024-01-15T21:10:44.379769Z"
    }
   },
   "id": "ac6c3e4432fc9106"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def actions_from_q_values(q_values, epsilon):\n",
    "    \"\"\"\n",
    "    Selects actions according to epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        # Apply softmax to convert q_values into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(q_values, dim=1)\n",
    "        \n",
    "        # Sample actions based on the probabilities\n",
    "        actions = np.array([torch.multinomial(p, 1).item() for p in probabilities]).cpu().numpy()\n",
    "        return actions\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.802033Z",
     "start_time": "2024-01-15T21:10:44.390439Z"
    }
   },
   "id": "826dd32ca6657566"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.845952Z",
     "start_time": "2024-01-15T21:10:44.394826Z"
    }
   },
   "id": "475f097e7f7a6a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def preprocess_state(position, map_layout):\n",
    "    nrows, ncols = len(map_layout), len(map_layout[0])   # Corrected for variable column length\n",
    "    num_statuses = 4      # Four statuses including the agent's position\n",
    "\n",
    "    state_tensor = np.zeros((nrows, ncols, num_statuses))\n",
    "\n",
    "    # Decode map layout\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3}  # Start 'S' also considered safe '0'\n",
    "\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            # Set the appropriate index in the one-hot vector\n",
    "            state_tensor[i, j, layout_to_val[map_layout[i][j]]] = 1\n",
    "\n",
    "    # Convert position to 2D coordinates and update in state tensor\n",
    "    row, col = divmod(position, ncols)\n",
    "    # Resetting the cell to a blank state before marking the current position\n",
    "    state_tensor[row, col] = np.array([0, 0, 0, 0])\n",
    "    state_tensor[row, col, 2] = 1  # Marking the current position with one-hot encoding\n",
    "\n",
    "    # Rearrange the tensor to (channels, rows, columns)\n",
    "    state_tensor = np.transpose(state_tensor, (2, 0, 1))\n",
    "\n",
    "    return torch.tensor(state_tensor, dtype=torch.float)\n",
    "\n",
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.962217Z",
     "start_time": "2024-01-15T21:10:44.403880Z"
    }
   },
   "id": "3501c2ac8576ba78"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "print(env.desc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:44.986721Z",
     "start_time": "2024-01-15T21:10:44.413881Z"
    }
   },
   "id": "abd04d238cc92983"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, device='cpu'):\n",
    "        self.alpha = alpha\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = torch.zeros(capacity, device=device)\n",
    "        self.precomputed_probabilities = None  # New attribute\n",
    "        self.next_priority_idx = 0\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, experiences):\n",
    "        # Convert a batch of experiences to tensors\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        states = torch.stack(states).to(self.device)\n",
    "        actions = torch.tensor(actions, device=self.device, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32)\n",
    "        next_states = torch.stack(next_states).to(self.device)\n",
    "        dones = torch.tensor(dones, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        max_priority = self.priorities.max().item()\n",
    "        for idx in range(len(experiences)):\n",
    "            self.buffer.append((states[idx], actions[idx], rewards[idx], next_states[idx], dones[idx]))\n",
    "            self.priorities[self.next_priority_idx] = max_priority\n",
    "            self.next_priority_idx = (self.next_priority_idx + 1) % len(self.priorities)\n",
    "        self._update_probabilities()\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == 0 or self.precomputed_probabilities is None:\n",
    "            return torch.empty(0), torch.empty(0, dtype=torch.long), torch.empty(0)\n",
    "\n",
    "        indices = torch.multinomial(self.precomputed_probabilities, batch_size, replacement=True)\n",
    "\n",
    "        # Extracting the samples based on indices\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in indices])\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.stack(rewards)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.stack(dones)\n",
    "\n",
    "        weights = (len(self.buffer) * self.precomputed_probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones), indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        self.priorities[indices] = torch.tensor(errors, device=self.device) + 1e-5\n",
    "        self._update_probabilities()\n",
    "\n",
    "    def _update_probabilities(self):\n",
    "        # Update the precomputed probabilities based on current priorities\n",
    "        priorities = self.priorities[:len(self.buffer)].clamp(min=1e-5)\n",
    "        self.precomputed_probabilities = priorities ** self.alpha\n",
    "        self.precomputed_probabilities /= self.precomputed_probabilities.sum()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.040069Z",
     "start_time": "2024-01-15T21:10:44.422941Z"
    }
   },
   "id": "e883ae8bf3476fd1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def calculate_intermediate_reward(current_state, next_state, env, visited_states, forward_step_reward=0, visited_step_reward=0, in_place_reward=0):\n",
    "    \"\"\"\n",
    "    Calculate intermediate reward based on movement towards the goal.\n",
    "    \"\"\"\n",
    "    if next_state == current_state:\n",
    "        return in_place_reward\n",
    "    elif next_state in visited_states:\n",
    "        return visited_step_reward \n",
    "    else:\n",
    "        return forward_step_reward if next_state > current_state else 0\n",
    "\n",
    "\n",
    "def create_vectorized_environments(env_name, n_envs, random_map, is_slippery, size=4):\n",
    "    if random_map:\n",
    "        envs = [gym.make(env_name, desc=generate_random_map(size=size), is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    else:\n",
    "        envs = [gym.make(env_name, is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    return envs\n",
    "\n",
    "def create_environment(random_map, is_slippery, size=4):\n",
    "    if random_map:\n",
    "        return gym.make('FrozenLake-v1', desc=generate_random_map(size=size), is_slippery=is_slippery)\n",
    "    else:\n",
    "        return gym.make('FrozenLake-v1', is_slippery=is_slippery)\n",
    "\n",
    "\n",
    "def update_model_using_replay_buffer(buffer:PrioritizedReplayBuffer, model, model2, optimizer, loss_fn, gamma, device, gradient_clipping_max_norm=10.0, batch_size=256, beta=0.4, return_grad_norm=False):\n",
    "    model.train()\n",
    "    \n",
    "    # Sample a batch of transitions from the replay buffer\n",
    "    (states_batch, actions_batch, rewards, next_states_batch, dones), indices, weights = buffer.sample(batch_size, beta=beta)\n",
    "    \n",
    "    # to device\n",
    "    states_batch = states_batch.to(device)\n",
    "    actions_batch = actions_batch.to(device)\n",
    "    rewards = rewards.to(device)\n",
    "    next_states_batch = next_states_batch.to(device)\n",
    "    dones = dones.to(device)\n",
    "    weights = weights.to(device)\n",
    "    \n",
    "    # Compute the target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_q_values = model2(next_states_batch).gather(1, torch.argmax(model(next_states_batch), dim=1).unsqueeze(-1)).squeeze()\n",
    "        target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "    \n",
    "    \n",
    "    # Reshape actions_batch from [64] to [64, 1] to match the dimensions\n",
    "    actions_batch = actions_batch.unsqueeze(-1)\n",
    "    # Compute current Q-values using the sampled states and actions\n",
    "    current_q_values = model(states_batch).gather(1, actions_batch).squeeze(-1)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = loss_fn(current_q_values, target_q_values)\n",
    "    \n",
    "    # Calculate TD error for priority update\n",
    "    errors = torch.abs(current_q_values - target_q_values).detach().cpu().numpy()\n",
    "    buffer.update_priorities(indices, errors)\n",
    "    \n",
    "    loss = (loss * weights).mean()\n",
    "    \n",
    "    # Rest of your optimization logic\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clipping_max_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if return_grad_norm:\n",
    "        grad_norm = sum(torch.norm(param.grad)**2 for param in model.parameters() if param.grad is not None)\n",
    "    else:\n",
    "        grad_norm = None\n",
    "    \n",
    "    return loss, grad_norm\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, create_model, optimizer, loss_fn, gamma, epsilon_start, epsilon_decay, num_steps, device,\n",
    "                n_states, random_map=False, is_slippery=False, hole_reward=0, forward_step_reward=0,\n",
    "                visited_step_reward=0, in_place_reward=0, step_reward=0, minimum_epsilon=0.05,\n",
    "                gradient_clipping_max_norm=10.0, batch_size=64, buffer_alpha=0.6, beta=0.4, beta_increment=0.001,\n",
    "                model2_step_update_frequency=750):\n",
    "    n_envs = batch_size // 4\n",
    "    max_steps = n_states * 2\n",
    "    map_size = int(n_states ** 0.5)\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    last_eval_success_rate = 0  # metric that is sparsely updated\n",
    "    weight_norm = sum(torch.norm(param) ** 2 for param in model.parameters() if param.dim() > 1)\n",
    "    bias_norm = sum(torch.norm(param) ** 2 for param in model.parameters() if param.dim() == 1)\n",
    "\n",
    "    percentage_update = 5\n",
    "    plot_update_frequency = int(num_steps * (percentage_update / 100))  # update plots every % of episodes\n",
    "    if plot_update_frequency == 0:\n",
    "        plot_update_frequency = 1\n",
    "\n",
    "    # double Q learning\n",
    "    model2 = create_model(n_states, n_actions).to(device)\n",
    "    model2.load_state_dict(model.state_dict())\n",
    "    model2.eval()\n",
    "\n",
    "    # Prioritized Experience Replay\n",
    "    buffer = PrioritizedReplayBuffer(capacity=10000, alpha=buffer_alpha)\n",
    "    min_buffer_size = 1000\n",
    "\n",
    "    envs = create_vectorized_environments('FrozenLake-v1', n_envs, random_map, is_slippery, size=map_size)\n",
    "    layouts = convert_layout_to_tensor([env.desc for env in envs])\n",
    "    states = [env.reset()[0] for env in envs]\n",
    "    visited_states = [set() for _ in envs]\n",
    "    episode_steps_count = [0] * n_envs\n",
    "\n",
    "    # Training loop\n",
    "    for step in tqdm(range(num_steps)):\n",
    "        if step % model2_step_update_frequency == 0:\n",
    "            model2.load_state_dict(model.state_dict())\n",
    "            model2.eval()\n",
    "\n",
    "        if beta < 1:\n",
    "            beta += beta_increment\n",
    "        else:\n",
    "            beta = 1\n",
    "\n",
    "        # account for steps and visited states\n",
    "        for i in range(len(envs)):\n",
    "            episode_steps_count[i] += 1\n",
    "            visited_states[i].add(states[i])\n",
    "\n",
    "        # Preprocess all states and convert them into tensors\n",
    "        step_state_tensors = update_start_positions(layouts, states).to(device)\n",
    "\n",
    "        # Compute Q-values for the entire batch\n",
    "        with torch.no_grad():\n",
    "            step_q_values = model(step_state_tensors)\n",
    "\n",
    "        # Iterate over each environment to select actions\n",
    "        step_actions = []\n",
    "        for i, (q_values, env) in enumerate(zip(step_q_values, envs)):\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = torch.argmax(q_values).item()\n",
    "            step_actions.append(action)\n",
    "\n",
    "        step_next_states, step_rewards, step_next_dones = [], [], []\n",
    "        for i, (env, action) in enumerate(zip(envs, step_actions)):\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            # Custom reward logic\n",
    "            if done and reward == 0:  # Agent fell into a hole\n",
    "                reward = hole_reward\n",
    "            else:\n",
    "                # Additional logic to calculate reward for moving towards the goal\n",
    "                reward += calculate_intermediate_reward(states[i], next_state, env, visited_states[i],\n",
    "                                                        forward_step_reward, visited_step_reward, in_place_reward)\n",
    "\n",
    "            reward += step_reward\n",
    "            step_next_states.append(next_state)\n",
    "            step_rewards.append(reward)\n",
    "            step_next_dones.append(done)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        next_state_to_push = update_start_positions(layouts, step_next_states).to(device)\n",
    "        experiences = []\n",
    "        for state, action, reward, next_state, done in zip(step_state_tensors, step_actions, step_rewards,\n",
    "                                                           next_state_to_push, step_next_dones):\n",
    "            experiences.append((state, action, reward, next_state, done))\n",
    "        # Push the batch of experiences to the buffer\n",
    "        buffer.push(experiences)\n",
    "\n",
    "            \n",
    "        if len(buffer) > min_buffer_size:\n",
    "            loss, _ = update_model_using_replay_buffer(buffer, model, model2, optimizer, loss_fn, gamma, device, gradient_clipping_max_norm, batch_size, beta=beta)\n",
    "\n",
    "            # log metrics to wandb\n",
    "            wandb.log({\"loss\": loss.item(),\n",
    "                       \"weight_norm\": weight_norm.item(),\n",
    "                       \"bias_norm\": bias_norm.item(),\n",
    "                       #\"grad_norm\": grad_norm.item(),\n",
    "                       \"epsilon\": epsilon,\n",
    "                       \"last_eval_success_rate\": last_eval_success_rate,\n",
    "                       \"beta\": beta})\n",
    "        #outside of buffer sampling\n",
    "\n",
    "        # Update states of ongoing indices\n",
    "        for i, (next_state, done) in enumerate(zip(step_next_states, step_next_dones)):\n",
    "            if not done and episode_steps_count[i] < max_steps:\n",
    "                states[i] = next_state\n",
    "            else:\n",
    "                # generate new env\n",
    "                envs[i].close()  # TODO: is this necessary?\n",
    "                envs[i] = create_environment(random_map, is_slippery, size=map_size)\n",
    "                batched_desc = [envs[i].desc]\n",
    "                batched_convertion = convert_layout_to_tensor(batched_desc)\n",
    "                unbatched_conversion = batched_convertion[0]\n",
    "                layouts[i] = unbatched_conversion\n",
    "                states[i] = envs[i].reset()[0]\n",
    "                visited_states[i] = set()\n",
    "                episode_steps_count[i] = 0\n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > minimum_epsilon:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if step > 0 and (step % plot_update_frequency == 0 or step == num_steps - 1):\n",
    "            # update some metrics  \n",
    "            last_eval_success_rate = evaluate_model(model, 1000, device, n_states, batch_size, is_slippery, random_map)\n",
    "            weight_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() > 1)\n",
    "            bias_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() == 1) \n",
    "            \n",
    "    # Close all environments\n",
    "    for env in envs:\n",
    "        env.close()\n",
    "\n",
    "    return model, buffer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.046918Z",
     "start_time": "2024-01-15T21:10:44.426861Z"
    }
   },
   "id": "ca47908718e26a43"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def evaluate_model(model, min_eval_episodes, device, n_states, batch_size=64, is_slippery=False, random_map=False):\n",
    "    map_size = int(n_states ** 0.5)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        successful_episodes = 0\n",
    "        total_evaluated = 0\n",
    "        envs = create_vectorized_environments('FrozenLake-v1', batch_size, random_map, is_slippery, size=map_size)\n",
    "        states = [env.reset()[0] for env in envs]\n",
    "        layouts = convert_layout_to_tensor([env.desc for env in envs])\n",
    "        dones = [False] * len(envs)\n",
    "        max_steps = n_states//2 # probably failing if it takes half the map to reach the goal\n",
    "        episode_steps_count = [0] * len(envs)\n",
    "\n",
    "        while total_evaluated < min_eval_episodes:\n",
    "            state_tensors = update_start_positions(layouts, states).to(device)\n",
    "            q_values = model(state_tensors)\n",
    "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "            for i in range(len(envs)):\n",
    "                if not dones[i]:\n",
    "                    next_state, reward, done, _, _ = envs[i].step(actions[i])\n",
    "                    states[i] = next_state\n",
    "                    episode_steps_count[i] += 1\n",
    "\n",
    "                    if done or episode_steps_count[i] >= max_steps:\n",
    "                        successful_episodes += 1 if reward >= 1 else 0\n",
    "                        total_evaluated += 1\n",
    "\n",
    "                        if total_evaluated < min_eval_episodes:\n",
    "                            envs[i] = create_environment(random_map, is_slippery, size=map_size)\n",
    "                            batched_desc = [envs[i].desc]\n",
    "                            batched_convertion = convert_layout_to_tensor(batched_desc)\n",
    "                            unbatched_conversion = batched_convertion[0]\n",
    "                            layouts[i] = unbatched_conversion\n",
    "                            states[i] = envs[i].reset()[0]\n",
    "                            episode_steps_count[i] = 0\n",
    "                            dones[i] = False\n",
    "                        else:\n",
    "                            dones[i] = True\n",
    "        success_rate = successful_episodes / total_evaluated\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.057877Z",
     "start_time": "2024-01-15T21:10:44.445578Z"
    }
   },
   "id": "bca62607ea444a77"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    wandb.init(project=\"frozenlake_slipperry_optuna_reward_search_convnet_random_map\",\n",
    "               name=f\"trial_{trial.number}\",\n",
    "               config=trial.params,\n",
    "               reinit=True)\n",
    "    # find rewards\n",
    "    hole_reward = trial.suggest_float(\"hole_reward\", -1, 0)\n",
    "    forward_step_reward = trial.suggest_float(\"forward_step_reward\", 0, 1)\n",
    "    visited_step_reward = trial.suggest_float(\"visited_step_reward\", -1, 0)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.0001\n",
    "    gamma = 0.99\n",
    "    epsilon = 0.8\n",
    "    epsilon_decay = 0.999\n",
    "    num_episodes = 2500\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    model = ConvNet(n_states, n_actions).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "    random_map = True\n",
    "    is_slippery = False\n",
    "\n",
    "    #TODO: update parameters\n",
    "    trained_model, _ = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, random_map=random_map, is_slippery=is_slippery,\n",
    "                                hole_reward=hole_reward, forward_step_reward=forward_step_reward, visited_step_reward=visited_step_reward)\n",
    "    \n",
    "    num_eval_episodes = 50 \n",
    "    success_rate = evaluate_model(trained_model, num_eval_episodes, device, n_states, is_slippery, random_map)\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.071978Z",
     "start_time": "2024-01-15T21:10:44.453644Z"
    }
   },
   "id": "da966d3235295555"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nstudy = optuna.create_study(direction=\\'maximize\\')\\nstudy.optimize(objective, n_trials=50)\\n\\nprint(\"Best trial:\")\\ntrial = study.best_trial\\nprint(f\" Value: {trial.value}\")\\nprint(\" Params: \")\\nfor key, value in trial.params.items():\\n    print(f\"    {key}: {value}\")\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value: {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.078050Z",
     "start_time": "2024-01-15T21:10:44.475490Z"
    }
   },
   "id": "380fbbe2086bcac1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.95\n",
    "epsilon = 0.6   \n",
    "epsilon_decay = 0.999\n",
    "weight_decay = 1e-4\n",
    "dropout_p = 0.8\n",
    "forward_step_reward = 0.02\n",
    "visited_step_reward = 0\n",
    "hole_reward = 0 #-0.5\n",
    "in_place_reward = 0\n",
    "step_reward = 0# -0.03\n",
    "min_epsilon = 0.05\n",
    "gradient_clipping_max_norm = 2.0\n",
    "batch_size= 128\n",
    "buffer_alpha=0.3\n",
    "beta=0.4\n",
    "beta_increment=0\n",
    "model2_step_update_frequency = 750\n",
    "layers = [16, 32, 64]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.097212Z",
     "start_time": "2024-01-15T21:10:44.475813Z"
    }
   },
   "id": "907a4ada3bd611a3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "random_map = True\n",
    "is_slippery = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.104675Z",
     "start_time": "2024-01-15T21:10:44.480717Z"
    }
   },
   "id": "40fe2a1e4824ecb7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Model Size in VRAM: 0.12 MB\n",
      "Parameter Count: 30,788 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_steps = 10_000\n",
    "n_actions = env.action_space.n\n",
    "size = 4\n",
    "n_states = size*size\n",
    "def create_convnet_model(n_states, n_actions):\n",
    "    return ConvNet(n_states, n_actions,conv_layers=layers, dropout_p=dropout_p)\n",
    "model = create_convnet_model(n_states, n_actions).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "parameter_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# Estimate size in VRAM\n",
    "model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "model_size_megabytes = model_size_bytes / (1024 ** 2)\n",
    "print(f\"Estimated Model Size in VRAM: {model_size_megabytes:.2f} MB\")\n",
    "\n",
    "print(f\"Parameter Count: {parameter_count:,} \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:45.347958Z",
     "start_time": "2024-01-15T21:10:44.488567Z"
    }
   },
   "id": "8ea311dc1ebc0804"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mloyd\u001B[0m (\u001B[33mloyd-team\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 848 getcaller\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m wandb\u001B[38;5;241m.\u001B[39minit(project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrozenlake_slipperry_batches_convnet_random_map\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      2\u001B[0m            name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying train with the same batch of 128 envs, without making new envs\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m            reinit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m            config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m: learning_rate,\n\u001B[1;32m      5\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m\"\u001B[39m: gamma,\n\u001B[1;32m      6\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepsilon\u001B[39m\u001B[38;5;124m\"\u001B[39m: epsilon,\n\u001B[1;32m      7\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepsilon_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m: epsilon_decay,\n\u001B[1;32m      8\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m: weight_decay,\n\u001B[1;32m      9\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: dropout_p,\n\u001B[1;32m     10\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_steps,\n\u001B[1;32m     11\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandom_map\u001B[39m\u001B[38;5;124m\"\u001B[39m: random_map,\n\u001B[1;32m     12\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_slippery\u001B[39m\u001B[38;5;124m\"\u001B[39m: is_slippery,\n\u001B[1;32m     13\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforward_step_reward\u001B[39m\u001B[38;5;124m\"\u001B[39m: forward_step_reward,\n\u001B[1;32m     14\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvisited_step_reward\u001B[39m\u001B[38;5;124m\"\u001B[39m: visited_step_reward,\n\u001B[1;32m     15\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhole_reward\u001B[39m\u001B[38;5;124m\"\u001B[39m: hole_reward,\n\u001B[1;32m     16\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min_place_reward\u001B[39m\u001B[38;5;124m\"\u001B[39m: in_place_reward,\n\u001B[1;32m     17\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep_reward\u001B[39m\u001B[38;5;124m\"\u001B[39m: step_reward,\n\u001B[1;32m     18\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mminimum_epsilon\u001B[39m\u001B[38;5;124m\"\u001B[39m: min_epsilon,\n\u001B[1;32m     19\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgradient_clipping_max_norm\u001B[39m\u001B[38;5;124m\"\u001B[39m: gradient_clipping_max_norm,\n\u001B[1;32m     20\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_size,\n\u001B[1;32m     21\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuffer_alpha\u001B[39m\u001B[38;5;124m\"\u001B[39m: buffer_alpha,\n\u001B[1;32m     22\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta\u001B[39m\u001B[38;5;124m\"\u001B[39m: beta,\n\u001B[1;32m     23\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta_increment\u001B[39m\u001B[38;5;124m\"\u001B[39m: beta_increment,\n\u001B[1;32m     24\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel2_step_update_frequency\u001B[39m\u001B[38;5;124m\"\u001B[39m: model2_step_update_frequency,\n\u001B[1;32m     25\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayers\u001B[39m\u001B[38;5;124m\"\u001B[39m: layers})\n\u001B[1;32m     26\u001B[0m model, _ \u001B[38;5;241m=\u001B[39m train_model(model, create_convnet_model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_steps, device, n_states, \n\u001B[1;32m     27\u001B[0m                     random_map\u001B[38;5;241m=\u001B[39mrandom_map, is_slippery\u001B[38;5;241m=\u001B[39mis_slippery,\n\u001B[1;32m     28\u001B[0m                     forward_step_reward\u001B[38;5;241m=\u001B[39mforward_step_reward,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m                     beta_increment\u001B[38;5;241m=\u001B[39mbeta_increment,\n\u001B[1;32m     39\u001B[0m                     model2_step_update_frequency\u001B[38;5;241m=\u001B[39mmodel2_step_update_frequency)\n\u001B[1;32m     40\u001B[0m wandb\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1193\u001B[0m, in \u001B[0;36minit\u001B[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001B[0m\n\u001B[1;32m   1191\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m logger\n\u001B[1;32m   1192\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterrupted\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39me)\n\u001B[0;32m-> 1193\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1195\u001B[0m     error_seen \u001B[38;5;241m=\u001B[39m e\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:1170\u001B[0m, in \u001B[0;36minit\u001B[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001B[0m\n\u001B[1;32m   1168\u001B[0m except_exit \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39m_except_exit\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1170\u001B[0m     run \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39minit()\n\u001B[1;32m   1171\u001B[0m     except_exit \u001B[38;5;241m=\u001B[39m wi\u001B[38;5;241m.\u001B[39msettings\u001B[38;5;241m.\u001B[39m_except_exit\n\u001B[1;32m   1172\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:811\u001B[0m, in \u001B[0;36m_WandbInit.init\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    809\u001B[0m run_start_handle \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39minterface\u001B[38;5;241m.\u001B[39mdeliver_run_start(run\u001B[38;5;241m.\u001B[39m_run_obj)\n\u001B[1;32m    810\u001B[0m \u001B[38;5;66;03m# TODO: add progress to let user know we are doing something\u001B[39;00m\n\u001B[0;32m--> 811\u001B[0m run_start_result \u001B[38;5;241m=\u001B[39m run_start_handle\u001B[38;5;241m.\u001B[39mwait(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m)\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_start_result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    813\u001B[0m     run_start_handle\u001B[38;5;241m.\u001B[39mabandon()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:283\u001B[0m, in \u001B[0;36mMailboxHandle.wait\u001B[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001B[0m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interface\u001B[38;5;241m.\u001B[39m_transport_keepalive_failed():\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MailboxError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransport failed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 283\u001B[0m found, abandoned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slot\u001B[38;5;241m.\u001B[39m_get_and_clear(timeout\u001B[38;5;241m=\u001B[39mwait_timeout)\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m found:\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Always update progress to 100% when done\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m on_progress \u001B[38;5;129;01mand\u001B[39;00m progress_handle \u001B[38;5;129;01mand\u001B[39;00m progress_sent:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:130\u001B[0m, in \u001B[0;36m_MailboxSlot._get_and_clear\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_and_clear\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Optional[pb\u001B[38;5;241m.\u001B[39mResult], \u001B[38;5;28mbool\u001B[39m]:\n\u001B[1;32m    129\u001B[0m     found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait(timeout\u001B[38;5;241m=\u001B[39mtimeout):\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m             found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py:126\u001B[0m, in \u001B[0;36m_MailboxSlot._wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event\u001B[38;5;241m.\u001B[39mwait(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/threading.py:622\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    620\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 622\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cond\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/threading.py:324\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 324\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mTrue\u001B[39;00m, timeout)\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    326\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"frozenlake_slipperry_batches_convnet_random_map\",\n",
    "           name=\"Trying train with the same batch of 128 envs, without making new envs\",\n",
    "           reinit=True,\n",
    "           config={\"learning_rate\": learning_rate,\n",
    "                   \"gamma\": gamma,\n",
    "                   \"epsilon\": epsilon,\n",
    "                   \"epsilon_decay\": epsilon_decay,\n",
    "                   \"weight_decay\": weight_decay,\n",
    "                   \"dropout_p\": dropout_p,\n",
    "                   \"num_steps\": num_steps,\n",
    "                   \"random_map\": random_map,\n",
    "                   \"is_slippery\": is_slippery,\n",
    "                   \"forward_step_reward\": forward_step_reward,\n",
    "                   \"visited_step_reward\": visited_step_reward,\n",
    "                   \"hole_reward\": hole_reward,\n",
    "                   \"in_place_reward\": in_place_reward,\n",
    "                   \"step_reward\": step_reward,\n",
    "                   \"minimum_epsilon\": min_epsilon,\n",
    "                   \"gradient_clipping_max_norm\": gradient_clipping_max_norm,\n",
    "                   \"batch_size\": batch_size,\n",
    "                   \"buffer_alpha\": buffer_alpha,\n",
    "                   \"beta\": beta,\n",
    "                   \"beta_increment\": beta_increment,\n",
    "                   \"model2_step_update_frequency\": model2_step_update_frequency,\n",
    "                   \"layers\": layers})\n",
    "model, _ = train_model(model, create_convnet_model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_steps, device, n_states, \n",
    "                    random_map=random_map, is_slippery=is_slippery,\n",
    "                    forward_step_reward=forward_step_reward,\n",
    "                    visited_step_reward=visited_step_reward,\n",
    "                    hole_reward=hole_reward,\n",
    "                    in_place_reward=in_place_reward,\n",
    "                    step_reward=step_reward,\n",
    "                    minimum_epsilon=min_epsilon,\n",
    "                    gradient_clipping_max_norm=gradient_clipping_max_norm,\n",
    "                    batch_size=batch_size,\n",
    "                    buffer_alpha=buffer_alpha,\n",
    "                    beta=beta,\n",
    "                    beta_increment=beta_increment,\n",
    "                    model2_step_update_frequency=model2_step_update_frequency)\n",
    "wandb.finish()  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:45.036951Z"
    }
   },
   "id": "95783e41ab8de557"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"frozenlake_convnet_random_map_96.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T21:10:49.167557Z",
     "start_time": "2024-01-15T21:10:49.122559Z"
    }
   },
   "id": "b805a7bebe0e15d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try model finetune, with lower learning rate and no intermediate rewards\n",
    "# load model and finetune\n",
    "model = ConvNet(n_states, n_actions, dropout_p=dropout_p).to(device)\n",
    "model.load_state_dict(torch.load(\"frozenlake_convnet_random_map.pt\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate/10, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_episodes = 250\n",
    "epsilon = 0\n",
    "epsilon_decay = 0.99\n",
    "batch_size = 64\n",
    "wandb.init(project=\"frozenlake_slipperry_batches_convnet_random_map\",\n",
    "           name=f\"fine tune of previous model, with 10 lower lr, but with rewards and epsilon 0\",\n",
    "           reinit=True,\n",
    "           config={\"learning_rate\": learning_rate,\n",
    "                   \"gamma\": gamma,\n",
    "                   \"epsilon\": epsilon,\n",
    "                   \"epsilon_decay\": epsilon_decay,\n",
    "                   \"weight_decay\": weight_decay,\n",
    "                   \"dropout_p\": dropout_p,\n",
    "                   \"num_episodes\": num_episodes,\n",
    "                   \"random_map\": random_map,\n",
    "                   \"is_slippery\": is_slippery,\n",
    "                   \"forward_step_reward\": forward_step_reward,\n",
    "                   \"visited_step_reward\": visited_step_reward,\n",
    "                   \"hole_reward\": hole_reward,\n",
    "                   \"in_place_reward\": in_place_reward,\n",
    "                   \"step_reward\": step_reward,\n",
    "                   \"minimum_epsilon\": min_epsilon,\n",
    "                   \"gradient_clipping_max_norm\": gradient_clipping_max_norm,\n",
    "                   \"batch_size\": batch_size})\n",
    "model, _ = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, \n",
    "                    random_map=random_map, is_slippery=is_slippery,\n",
    "                    forward_step_reward=forward_step_reward,\n",
    "                    visited_step_reward=visited_step_reward,\n",
    "                    hole_reward=hole_reward,\n",
    "                    in_place_reward=in_place_reward,\n",
    "                    step_reward=step_reward,\n",
    "                    minimum_epsilon=min_epsilon,\n",
    "                    gradient_clipping_max_norm=gradient_clipping_max_norm,\n",
    "                    batch_size=batch_size)\n",
    "wandb.finish()  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:49.124253Z"
    }
   },
   "id": "2df048116ed947ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: setup infra to understand model and where it fails:\n",
    "    # keep in memory failures at each eval, and replay them to see improvements\n",
    "    # have something that shows only failures\n",
    "    # introspection into the buffer\n",
    "#TODO: try filling the buffer with correct runs before training, see if it improves learning    \n",
    "#DONE: try sampling other than argmax => softmax was shit, didn't learn, maybe at evaluation time it can be good\n",
    "#TODO: try intermediate goals instead of rewards, for exemple moving the goal closer to the start\n",
    "#TODO: try doing model hybrids: for exemple train on non slippery and fine tune for slippery\n",
    "#TODO: try other RL algorithms => 2Q learning done\n",
    "#TODO: maybe remove some intermediate rewards with time.\n",
    "#TODO: how to understand what is the issue in training: for exemple when the model is chasing a Q target that is moving(2Q not imlemented or bad hyper)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:49.126728Z"
    }
   },
   "id": "bc52420e839ddb8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "min_eval_episodes = 1000\n",
    "success_rate = evaluate_model(model, min_eval_episodes, device, n_states, batch_size=256, is_slippery=is_slippery, random_map=random_map)\n",
    "print(f\"Success Rate: {success_rate:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:49.128332Z"
    }
   },
   "id": "510270e518b47635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_color_map = {'S': 1, 'F': 0, 'H': 3, 'G': 2}\n",
    "def visualize_policy_on_random_maps(model, device, n_maps=5, n_states=16, is_slippery=False):\n",
    "    map_size = int(n_states**0.5)\n",
    "    for map_idx in range(n_maps):\n",
    "        env = gym.make('FrozenLake-v1', desc=generate_random_map(size=map_size), is_slippery=is_slippery)\n",
    "        desc = env.desc.astype(str)\n",
    "\n",
    "        numerical_grid_colors = np.vectorize(numerical_color_map.get)(desc)\n",
    "\n",
    "        state_tensors = [preprocess_state(state, env.desc).to(device) for state in range(n_states)]\n",
    "        batch_states = torch.stack(state_tensors)\n",
    "        policy_batch = torch.argmax(model(batch_states), dim=1).cpu().numpy()\n",
    "\n",
    "        action_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "        policy_symbols = np.vectorize(action_symbols.get)(policy_batch)\n",
    "        policy_grid = policy_symbols.reshape(env.desc.shape)\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        cmap = ListedColormap(['lightblue', 'lightgreen', 'yellow', 'black'])\n",
    "        plt.imshow(numerical_grid_colors, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "        # New: Trace the path\n",
    "        current_position = np.where(desc == 'S')\n",
    "        i, j = current_position[0][0], current_position[1][0]  # Extracting the start position\n",
    "        path_x = [j]\n",
    "        path_y = [i]\n",
    "\n",
    "        while True:\n",
    "            action = policy_grid[i, j]\n",
    "            if action == '←': j -= 1 \n",
    "            elif action == '→': j += 1\n",
    "            elif action == '↑': i -= 1\n",
    "            elif action == '↓': i += 1\n",
    "            i = min(max(i, 0), desc.shape[0] - 1)  # Ensuring the agent doesn't go out of bounds\n",
    "            j = min(max(j, 0), desc.shape[1] - 1)\n",
    "            if (i, j) in zip(path_y, path_x):\n",
    "                break\n",
    "            path_x.append(j)\n",
    "            path_y.append(i)\n",
    "            if desc[i, j] in ['H', 'G']:\n",
    "                break\n",
    "\n",
    "        # Draw the path\n",
    "        plt.plot(path_x, path_y, 'ro-', linewidth=2, markersize=10)\n",
    "\n",
    "        for i in range(desc.shape[0]):\n",
    "            for j in range(desc.shape[1]):\n",
    "                arrow = policy_grid[i, j]\n",
    "                arrow_color = 'white' if desc[i, j] in ['H', 'G'] else 'black'\n",
    "                plt.text(j, i, arrow, ha='center', va='center', fontsize=20, color=arrow_color)\n",
    "\n",
    "        plt.title(f'Policy Visualization for Map {map_idx+1}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_policy_on_random_maps(model, device, n_maps=20, n_states=n_states, is_slippery=is_slippery)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:49.129541Z"
    }
   },
   "id": "bcec4f1a4aa586bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-15T21:10:49.130288Z"
    }
   },
   "id": "1cb37ca18ba4e6b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
