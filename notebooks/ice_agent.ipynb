{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.325811Z",
     "start_time": "2023-12-28T14:49:29.299359Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.339317Z",
     "start_time": "2023-12-28T14:49:29.327800Z"
    }
   },
   "id": "655c4f8ad84a6e65"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# Neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(n_states, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, n_actions, dropout_p=None):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        # 1x1 conv for matching dimensions in residual connection\n",
    "        self.match_channels1 = nn.Conv2d(4, 16, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        # 1x1 conv for matching dimensions in residual connection\n",
    "        self.match_channels2 = nn.Conv2d(16, 32, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.fc = nn.Linear(32 * input_size, n_actions)\n",
    "        self.dropout_p = dropout_p\n",
    "        if dropout_p is not None:\n",
    "            self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block with residual connection\n",
    "        residual = self.match_channels1(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = out + residual\n",
    "\n",
    "        # Second convolutional block with residual connection\n",
    "        residual = self.match_channels2(x)\n",
    "        out = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = out + residual\n",
    "\n",
    "        # Flattening and passing through fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if self.dropout_p is not None:\n",
    "            x = self.dropout(x)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.349861Z",
     "start_time": "2023-12-28T14:49:29.342950Z"
    }
   },
   "id": "fc04ac7142783e42"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "8004"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter count\n",
    "model_to_count = ConvNet(n_states, n_actions)\n",
    "sum(p.numel() for p in model_to_count.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.355236Z",
     "start_time": "2023-12-28T14:49:29.345262Z"
    }
   },
   "id": "ac6c3e4432fc9106"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def actions_from_q_values(q_values, epsilon):\n",
    "    \"\"\"\n",
    "    Selects actions according to epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, n_actions - 1)\n",
    "    else:\n",
    "        # Apply softmax to convert q_values into probabilities\n",
    "        probabilities = torch.nn.functional.softmax(q_values, dim=1)\n",
    "        \n",
    "        # Sample actions based on the probabilities\n",
    "        actions = np.array([torch.multinomial(p, 1).item() for p in probabilities]).cpu().numpy()\n",
    "        return actions\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.357670Z",
     "start_time": "2023-12-28T14:49:29.351479Z"
    }
   },
   "id": "826dd32ca6657566"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.367085Z",
     "start_time": "2023-12-28T14:49:29.358257Z"
    }
   },
   "id": "475f097e7f7a6a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def preprocess_state(position, map_layout):\n",
    "    nrows, ncols = 4, 4  # Assuming a 4x4 map\n",
    "    num_statuses = 4      # Four statuses including the agent's position\n",
    "\n",
    "    # Initialize a 4x4x4 tensor for one-hot encoded state\n",
    "    state_tensor = np.zeros((nrows, ncols, num_statuses))\n",
    "\n",
    "    # Decode map layout\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3}  # Start 'S' also considered safe '0'\n",
    "\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            # Set the appropriate index in the one-hot vector\n",
    "            state_tensor[i, j, layout_to_val[map_layout[i][j]]] = 1\n",
    "\n",
    "    # Convert position to 2D coordinates and update in state tensor\n",
    "    row, col = divmod(position, ncols)\n",
    "    # Resetting the cell to a blank state before marking the current position\n",
    "    state_tensor[row, col] = np.array([0, 0, 0, 0])\n",
    "    state_tensor[row, col, 2] = 1  # Marking the current position with one-hot encoding\n",
    "\n",
    "    return torch.tensor(state_tensor, dtype=torch.float)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.368952Z",
     "start_time": "2023-12-28T14:49:29.362354Z"
    }
   },
   "id": "3501c2ac8576ba78"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "print(env.desc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.391645Z",
     "start_time": "2023-12-28T14:49:29.370679Z"
    }
   },
   "id": "abd04d238cc92983"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.alpha = alpha\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = max(self.priorities, default=1.0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max_priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        probabilities = np.array(self.priorities) ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priorities[idx] = error + 1e-5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e883ae8bf3476fd1"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def calculate_intermediate_reward(current_state, next_state, env, visited_states, forward_step_reward=0, visited_step_reward=0, in_place_reward=0):\n",
    "    \"\"\"\n",
    "    Calculate intermediate reward based on movement towards the goal.\n",
    "    \"\"\"\n",
    "    if next_state == current_state:\n",
    "        return in_place_reward\n",
    "    elif next_state in visited_states:\n",
    "        return visited_step_reward \n",
    "    else:\n",
    "        return forward_step_reward if next_state > current_state else 0\n",
    "\n",
    "\n",
    "def create_vectorized_environments(env_name, n_envs, random_map, is_slippery):\n",
    "    if random_map:\n",
    "        envs = [gym.make(env_name, desc=generate_random_map(size=4), is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    else:\n",
    "        envs = [gym.make(env_name, is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    return envs\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, optimizer, loss_fn, gamma, epsilon_start, epsilon_decay, num_episodes, device, n_states, random_map=False, is_slippery=False, hole_reward=0, forward_step_reward=0, visited_step_reward=0, in_place_reward=0, step_reward=0, minimum_epsilon=0.05, gradient_clipping_max_norm=10.0):\n",
    "    n_envs = 64\n",
    "    max_steps = 4 * 4 * 2\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    last_eval_success_rate = 0 # metric that is sparsely updated\n",
    "    weight_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() > 1)\n",
    "    bias_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() == 1)\n",
    "\n",
    "    percentage_update = 10\n",
    "    plot_update_frequency = int(num_episodes * (percentage_update/100))  # update plots every % of episodes\n",
    "    if plot_update_frequency == 0:\n",
    "        plot_update_frequency = 1\n",
    "\n",
    "    #double Q learning\n",
    "    #TODO: flexible model\n",
    "    model2 = ConvNet(n_states, n_actions).to(device)\n",
    "    model2.load_state_dict(model.state_dict())\n",
    "    model2.eval()\n",
    "    model2_target_update_frequency = 20\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        # Update model2 every model2_target_update_frequency episodes\n",
    "        if episode % model2_target_update_frequency == 0:\n",
    "            model2.load_state_dict(model.state_dict())\n",
    "            model2.eval()\n",
    "        \n",
    "        model.train()\n",
    "        # Create vectorized environments\n",
    "        envs = create_vectorized_environments('FrozenLake-v1', n_envs, random_map, is_slippery)\n",
    "\n",
    "\n",
    "        states = [env.reset()[0] for env in envs]\n",
    "        dones = [False] * n_envs\n",
    "        episode_rewards = [0] * n_envs\n",
    "        visited_states = [set() for _ in envs]\n",
    "\n",
    "        step_count = 0\n",
    "        ongoing_indices = list(range(n_envs))  # Initialize with all environment indices\n",
    "\n",
    "        # TODO: fix loss bug when len is 1\n",
    "        while len(ongoing_indices) > 1 and step_count < max_steps:\n",
    "            step_count += 1\n",
    "            # Filter out completed environments for action selection and state update\n",
    "            ongoing_envs = [envs[i] for i in ongoing_indices]\n",
    "            ongoing_states = [states[i] for i in ongoing_indices]\n",
    "\n",
    "\n",
    "            # Preprocess all states and convert them into tensors\n",
    "            state_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(ongoing_states, ongoing_envs)] \n",
    "            # Combine all state tensors into a single batch\n",
    "            state_tensor_batch = torch.stack(state_tensors)\n",
    "            \n",
    "            # Compute Q-values for the entire batch\n",
    "            q_values_batch = model(state_tensor_batch)\n",
    "            \n",
    "            # Iterate over each environment to select actions\n",
    "            actions = []\n",
    "            for i, (q_values, env) in enumerate(zip(q_values_batch, ongoing_envs)):\n",
    "                visited_states[ongoing_indices[i]].add(states[ongoing_indices[i]])\n",
    "            \n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = torch.argmax(q_values).item()\n",
    "            \n",
    "                actions.append(action)\n",
    "\n",
    "\n",
    "            next_states, rewards, next_dones = [], [], []\n",
    "            for i, (env, action) in enumerate(zip(ongoing_envs, actions)):\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                # Custom reward logic\n",
    "                if done and reward == 0:  # Agent fell into a hole\n",
    "                    reward = hole_reward\n",
    "                else:\n",
    "                    # Additional logic to calculate reward for moving towards the goal\n",
    "                    reward += calculate_intermediate_reward(states[ongoing_indices[i]], next_state, env, visited_states, forward_step_reward, visited_step_reward, in_place_reward)\n",
    "                \n",
    "                reward += step_reward\n",
    "                next_states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                next_dones.append(done)\n",
    "                episode_rewards[ongoing_indices[i]] += reward\n",
    "            \n",
    "            \n",
    "            next_states_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(next_states, ongoing_envs)]\n",
    "            next_states_batch = torch.stack(next_states_tensors)\n",
    "                        # Create batches for rewards, actions, and dones\n",
    "            reward_batch = torch.tensor(rewards, device=device)\n",
    "            action_batch = torch.tensor(actions, device=device)\n",
    "            done_batch = torch.tensor(next_dones, device=device)\n",
    "            \n",
    "            # Select the action using the main model\n",
    "            next_action_batch = torch.argmax(model(next_states_batch), dim=1)\n",
    "            # Evaluate the Q-value of this action using the target model\n",
    "            next_q_values_batch = model2(next_states_batch).gather(1, next_action_batch.unsqueeze(-1)).squeeze()\n",
    "\n",
    "            # Compute the target Q-values\n",
    "            target_q_values = reward_batch + gamma * next_q_values_batch * (1 - done_batch.float())\n",
    "\n",
    "            # Correct way to get predicted Q values for the taken actions\n",
    "            predicted_q_values = q_values_batch.gather(1, action_batch.unsqueeze(-1)).squeeze()\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(predicted_q_values, target_q_values)\n",
    "            \n",
    "            # Rest of your optimization logic\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clipping_max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update states of ongoing indices\n",
    "            for i, (next_state, done) in enumerate(zip(next_states, next_dones)):\n",
    "                if not done:\n",
    "                    states[ongoing_indices[i]] = next_state\n",
    "\n",
    "\n",
    "            grad_norm = sum(torch.norm(param.grad)**2 for param in model.parameters() if param.grad is not None)\n",
    "            # log metrics to wandb\n",
    "            wandb.log({\"loss\": loss.item(),\n",
    "                       \"weight_norm\": weight_norm.item(),\n",
    "                       \"bias_norm\": bias_norm.item(),\n",
    "                       \"grad_norm\": grad_norm.item(),\n",
    "                       \"epsilon\": epsilon,\n",
    "                       \"average_episode_rewards\": sum(episode_rewards)/len(episode_rewards),\n",
    "                       \"last_eval_success_rate\": last_eval_success_rate})\n",
    "            \n",
    "\n",
    "            # Update ongoing indices\n",
    "            new_ongoing_indices = []\n",
    "            for i, (done, reward) in enumerate(zip(next_dones, rewards)):\n",
    "                if not done:\n",
    "                    new_ongoing_indices.append(ongoing_indices[i])\n",
    "            ongoing_indices = new_ongoing_indices\n",
    "        # Close all environments at the end of the episode\n",
    "        for env in envs:\n",
    "            env.close()\n",
    "    \n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > minimum_epsilon:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if episode % plot_update_frequency == 0 or episode == num_episodes - 1:\n",
    "            # update some metrics  \n",
    "            last_eval_success_rate = evaluate_model(model, 1000, device, n_states, n_envs, is_slippery, random_map)\n",
    "            weight_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() > 1)\n",
    "            bias_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() == 1) \n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.394579Z",
     "start_time": "2023-12-28T14:49:29.372938Z"
    }
   },
   "id": "ca47908718e26a43"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def evaluate_model(model, num_eval_episodes, device, n_states, batch_size=64, is_slippery=False, random_map=False):\n",
    "    #TODO: give other metrics such as average episode length, average reward, etc.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        successful_episodes = 0\n",
    "        total_evaluated = 0\n",
    "    \n",
    "        while total_evaluated < num_eval_episodes:\n",
    "            envs = []\n",
    "            states = []\n",
    "            dones = []\n",
    "            for _ in range(min(batch_size, num_eval_episodes - total_evaluated)):\n",
    "                if random_map:\n",
    "                    env = gym.make('FrozenLake-v1', desc=generate_random_map(size=4), is_slippery=is_slippery)\n",
    "                else:\n",
    "                    env = gym.make('FrozenLake-v1', is_slippery=is_slippery)\n",
    "                envs.append(env)\n",
    "                states.append(env.reset()[0])\n",
    "                dones.append(False)\n",
    "    \n",
    "            max_steps = len(envs[0].desc) * len(envs[0].desc[0]) * 4\n",
    "            step_count = 0\n",
    "    \n",
    "            while step_count < max_steps and not all(dones):\n",
    "                step_count += 1\n",
    "                state_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(states, envs)]\n",
    "                state_batch = torch.stack(state_tensors)\n",
    "                q_values = model(state_batch)\n",
    "                actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "    \n",
    "                for i, env in enumerate(envs):\n",
    "                    if not dones[i]:\n",
    "                        next_state, reward, done, _, _ = env.step(actions[i])\n",
    "                        states[i] = next_state\n",
    "                        dones[i] = done\n",
    "    \n",
    "                        if done and reward > 0:\n",
    "                            successful_episodes += 1\n",
    "    \n",
    "            total_evaluated += len(envs)\n",
    "    \n",
    "        success_rate = successful_episodes / num_eval_episodes\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.395025Z",
     "start_time": "2023-12-28T14:49:29.386344Z"
    }
   },
   "id": "bca62607ea444a77"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    wandb.init(project=\"frozenlake_slipperry_optuna_reward_search_convnet_random_map\",\n",
    "               name=f\"trial_{trial.number}\",\n",
    "               config=trial.params,\n",
    "               reinit=True)\n",
    "    # find rewards\n",
    "    hole_reward = trial.suggest_float(\"hole_reward\", -1, 0)\n",
    "    forward_step_reward = trial.suggest_float(\"forward_step_reward\", 0, 1)\n",
    "    visited_step_reward = trial.suggest_float(\"visited_step_reward\", -1, 0)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.0001\n",
    "    gamma = 0.99\n",
    "    epsilon = 0.8\n",
    "    epsilon_decay = 0.999\n",
    "    num_episodes = 2500\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    model = ConvNet(n_states, n_actions).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "    random_map = True\n",
    "    is_slippery = False\n",
    "\n",
    "    trained_model = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, random_map=random_map, is_slippery=is_slippery,\n",
    "                                hole_reward=hole_reward, forward_step_reward=forward_step_reward, visited_step_reward=visited_step_reward)\n",
    "    \n",
    "    num_eval_episodes = 50 \n",
    "    success_rate = evaluate_model(trained_model, num_eval_episodes, device, n_states, is_slippery, random_map)\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.405686Z",
     "start_time": "2023-12-28T14:49:29.397005Z"
    }
   },
   "id": "da966d3235295555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "study"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bac2364ed794ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nstudy = optuna.create_study(direction=\\'maximize\\')\\nstudy.optimize(objective, n_trials=50)\\n\\nprint(\"Best trial:\")\\ntrial = study.best_trial\\nprint(f\" Value: {trial.value}\")\\nprint(\" Params: \")\\nfor key, value in trial.params.items():\\n    print(f\"    {key}: {value}\")\\n'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value: {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.406175Z",
     "start_time": "2023-12-28T14:49:29.399465Z"
    }
   },
   "id": "380fbbe2086bcac1"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99\n",
    "epsilon = 0.6   \n",
    "epsilon_decay = 0.95\n",
    "weight_decay = 1e-4\n",
    "dropout_p = 0.5\n",
    "forward_step_reward = 0.05\n",
    "visited_step_reward = 0\n",
    "hole_reward = -0.5\n",
    "in_place_reward = 0\n",
    "step_reward = -0.03\n",
    "min_epsilon = 0.01\n",
    "gradient_clipping_max_norm = 2.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.427502Z",
     "start_time": "2023-12-28T14:49:29.407784Z"
    }
   },
   "id": "907a4ada3bd611a3"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "random_map = True\n",
    "is_slippery = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:49:29.428102Z",
     "start_time": "2023-12-28T14:49:29.410005Z"
    }
   },
   "id": "40fe2a1e4824ecb7"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/loyd/PycharmProjects/PytorchPractice/notebooks/wandb/run-20231228_154931-578dhz3e</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/578dhz3e' target=\"_blank\">trying to get faster learning with more epsilon decay and less episodes</a></strong> to <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/578dhz3e' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/578dhz3e</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [11:09<00:00,  1.12it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcaeaa9ec1124948ab4145a0879fafd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_rewards</td><td>▁▂▂▃▂▂▂▂▁▂▁▁▂▃▃▃▆▄▃▇▇▃▃▇▇▃▃▃▇▃██▃▃▆▇█▇▃█</td></tr><tr><td>bias_norm</td><td>██████████▅▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epsilon</td><td>█▇▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>grad_norm</td><td>███▂▁▂▁▁▁▁▁▂▁█▅▃▇▆▆▃█▂▂▂█▂▁▁▅▁▁▃▂▁██▃▅▁▁</td></tr><tr><td>last_eval_success_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>loss</td><td>▆▂▃▂▁▂▁▁▁▁▁▂▁█▇▄▁▅▆▂▄▂▃▂▃▃▂▃▂▂▂▂▃▁▄▂▁▅▃▁</td></tr><tr><td>weight_norm</td><td>▄▄▄▄▄▄▄▄▄▄▁▁▁▁▁▁▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_rewards</td><td>0.99891</td></tr><tr><td>bias_norm</td><td>29.18907</td></tr><tr><td>epsilon</td><td>0.00991</td></tr><tr><td>grad_norm</td><td>1.71884</td></tr><tr><td>last_eval_success_rate</td><td>0.96</td></tr><tr><td>loss</td><td>0.01708</td></tr><tr><td>weight_norm</td><td>36.81149</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">trying to get faster learning with more epsilon decay and less episodes</strong> at: <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/578dhz3e' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/578dhz3e</a><br/> View job at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTgzMjg5Mw==/version_details/v14' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTgzMjg5Mw==/version_details/v14</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231228_154931-578dhz3e/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "num_episodes = 750 \n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "model = ConvNet(n_states, n_actions, dropout_p=dropout_p).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "wandb.init(project=\"frozenlake_slipperry_batches_convnet_random_map\",\n",
    "           name=\"trying to get faster learning with more epsilon decay and less episodes\",\n",
    "           reinit=True,\n",
    "           config={\"learning_rate\": learning_rate,\n",
    "                   \"gamma\": gamma,\n",
    "                   \"epsilon\": epsilon,\n",
    "                   \"epsilon_decay\": epsilon_decay,\n",
    "                   \"weight_decay\": weight_decay,\n",
    "                   \"dropout_p\": dropout_p,\n",
    "                   \"num_episodes\": num_episodes,\n",
    "                   \"random_map\": random_map,\n",
    "                   \"is_slippery\": is_slippery,\n",
    "                   \"forward_step_reward\": forward_step_reward,\n",
    "                   \"visited_step_reward\": visited_step_reward,\n",
    "                   \"hole_reward\": hole_reward,\n",
    "                   \"in_place_reward\": in_place_reward,\n",
    "                   \"step_reward\": step_reward,\n",
    "                   \"minimum_epsilon\": min_epsilon,\n",
    "                   \"gradient_clipping_max_norm\": gradient_clipping_max_norm})\n",
    "model = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, \n",
    "                    random_map=random_map, is_slippery=is_slippery,\n",
    "                    forward_step_reward=forward_step_reward,\n",
    "                    visited_step_reward=visited_step_reward,\n",
    "                    hole_reward=hole_reward,\n",
    "                    in_place_reward=in_place_reward,\n",
    "                    step_reward=step_reward,\n",
    "                    minimum_epsilon=min_epsilon,\n",
    "                    gradient_clipping_max_norm=gradient_clipping_max_norm)\n",
    "wandb.finish()  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:00:48.916931Z",
     "start_time": "2023-12-28T14:49:31.513394Z"
    }
   },
   "id": "95783e41ab8de557"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"frozenlake_convnet_random_map_95.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:14:53.077439Z",
     "start_time": "2023-12-28T14:14:53.073535Z"
    }
   },
   "id": "b805a7bebe0e15d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try model finetune, with lower learning rate and no intermediate rewards\n",
    "# load model and finetune\n",
    "model = ConvNet(n_states, n_actions, dropout_p=dropout_p).to(device)\n",
    "model.load_state_dict(torch.load(\"frozenlake_convnet_random_map_94.pt\"))\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate/10, weight_decay=weight_decay*10)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_episodes = 500\n",
    "epsilon = 0#0.3\n",
    "wandb.init(project=\"frozenlake_slipperry_batches_convnet_random_map\",\n",
    "           name=f\"fine tune of saved model, with lower lr, but with rewards and 0 epsilon\",\n",
    "           reinit=True)\n",
    "model = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, \n",
    "                    random_map=random_map, is_slippery=is_slippery,\n",
    "                    forward_step_reward=0.05,\n",
    "                    visited_step_reward=0,\n",
    "                    hole_reward=-0.5,\n",
    "                    in_place_reward=0,\n",
    "                    step_reward=-0.03, minimum_epsilon=0.01)\n",
    "wandb.finish()  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:32:15.233727Z",
     "start_time": "2023-12-28T12:32:15.230429Z"
    }
   },
   "id": "2df048116ed947ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: try to train faster while keeping perf around 0.95\n",
    "#TODO: experience replay buffer\n",
    "#TODO: try giving path to model\n",
    "#TODO: try step small negative reward to avoid loops=> not bad, i guess it works\n",
    "#TODO: try sampling other than argmax => softmax was shit, didn't learn, maybe at evaluation time it can be good\n",
    "#TODO: try intermediate goals instead of rewards, for exemple moving the goal closer to the start\n",
    "#TODO: try other RL algorithms => 2Q learning done\n",
    "#TODO: maybe remove some intermediate rewards with time."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T12:32:15.231859Z"
    }
   },
   "id": "bc52420e839ddb8a"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate:  0.881\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "num_eval_episodes = 1000\n",
    "success_rate = evaluate_model(model, num_eval_episodes, device, n_states, is_slippery=is_slippery, random_map=random_map)\n",
    "print(\"Success Rate: \", success_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:48:56.614638Z",
     "start_time": "2023-12-28T14:48:48.063584Z"
    }
   },
   "id": "510270e518b47635"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_color_map = {'S': 1, 'F': 0, 'H': 3, 'G': 2}\n",
    "def visualize_policy_on_random_maps(model, device, n_maps=5, n_states=16, is_slippery=False):\n",
    "    for map_idx in range(n_maps):\n",
    "        env = gym.make('FrozenLake-v1', desc=generate_random_map(size=4), is_slippery=is_slippery)\n",
    "        desc = env.desc.astype(str)\n",
    "\n",
    "        numerical_grid_colors = np.vectorize(numerical_color_map.get)(desc)\n",
    "\n",
    "        state_tensors = [preprocess_state(state, env.desc).to(device) for state in range(n_states)]\n",
    "        batch_states = torch.stack(state_tensors)\n",
    "        policy_batch = torch.argmax(model(batch_states), dim=1).cpu().numpy()\n",
    "\n",
    "        action_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "        policy_symbols = np.vectorize(action_symbols.get)(policy_batch)\n",
    "        policy_grid = policy_symbols.reshape(env.desc.shape)\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        cmap = ListedColormap(['lightblue', 'lightgreen', 'yellow', 'black'])\n",
    "        plt.imshow(numerical_grid_colors, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "        # New: Trace the path\n",
    "        current_position = np.where(desc == 'S')\n",
    "        i, j = current_position[0][0], current_position[1][0]  # Extracting the start position\n",
    "        path_x = [j]\n",
    "        path_y = [i]\n",
    "\n",
    "        while True:\n",
    "            action = policy_grid[i, j]\n",
    "            if action == '←': j -= 1 \n",
    "            elif action == '→': j += 1\n",
    "            elif action == '↑': i -= 1\n",
    "            elif action == '↓': i += 1\n",
    "            i = min(max(i, 0), desc.shape[0] - 1)  # Ensuring the agent doesn't go out of bounds\n",
    "            j = min(max(j, 0), desc.shape[1] - 1)\n",
    "            if (i, j) in zip(path_y, path_x):\n",
    "                break\n",
    "            path_x.append(j)\n",
    "            path_y.append(i)\n",
    "            if desc[i, j] in ['H', 'G']:\n",
    "                break\n",
    "\n",
    "        # Draw the path\n",
    "        plt.plot(path_x, path_y, 'ro-', linewidth=2, markersize=10)\n",
    "\n",
    "        for i in range(desc.shape[0]):\n",
    "            for j in range(desc.shape[1]):\n",
    "                arrow = policy_grid[i, j]\n",
    "                arrow_color = 'white' if desc[i, j] in ['H', 'G'] else 'black'\n",
    "                plt.text(j, i, policy_grid[i, j], ha='center', va='center', fontsize=20, color=arrow_color)\n",
    "\n",
    "        plt.title(f'Policy Visualization for Map {map_idx+1}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_policy_on_random_maps(model, device, n_maps=100, is_slippery=is_slippery)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T12:32:15.233692Z"
    }
   },
   "id": "bcec4f1a4aa586bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:32:15.235611Z",
     "start_time": "2023-12-28T12:32:15.234452Z"
    }
   },
   "id": "1cb37ca18ba4e6b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
