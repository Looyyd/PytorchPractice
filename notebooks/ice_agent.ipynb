{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.679934Z",
     "start_time": "2023-12-27T00:48:31.637443Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython import display\n",
    "from collections import Counter\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.681828Z",
     "start_time": "2023-12-27T00:48:31.662465Z"
    }
   },
   "id": "655c4f8ad84a6e65"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# Neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(n_states, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    # TODO: normalization layers, maybe layer norm because batch size of 1 ?\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Assuming input_size is the flattened size of the 2D state\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Flatten the output for the fully connected layer\n",
    "        self.fc = nn.Linear(32 * input_size, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.698398Z",
     "start_time": "2023-12-27T00:48:31.685510Z"
    }
   },
   "id": "fc04ac7142783e42"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.698737Z",
     "start_time": "2023-12-27T00:48:31.688582Z"
    }
   },
   "id": "475f097e7f7a6a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def preprocess_state(position, map_layout):\n",
    "    nrows, ncols = 4, 4  # Assuming a 4x4 map\n",
    "    state_matrix = np.zeros((nrows, ncols))\n",
    "    \n",
    "    # Decode map layout\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3}  # Start 'S' also considered safe '0'\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            state_matrix[i][j] = layout_to_val[map_layout[i][j]]\n",
    "\n",
    "    # Convert position to 2D coordinates and update in state matrix\n",
    "    row, col = divmod(position, ncols)\n",
    "    state_matrix[row][col] = 2  # Marking the current position\n",
    "\n",
    "    # Normalize\n",
    "    normalized_state = state_matrix / 3.0 - 0.5\n",
    "    return torch.tensor(normalized_state, dtype=torch.float).unsqueeze(0)  # Adds batch dimension\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.704195Z",
     "start_time": "2023-12-27T00:48:31.701455Z"
    }
   },
   "id": "3501c2ac8576ba78"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "print(env.desc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.712773Z",
     "start_time": "2023-12-27T00:48:31.704304Z"
    }
   },
   "id": "abd04d238cc92983"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def find_goal_position(env_desc):\n",
    "    \"\"\"\n",
    "    Find the position of the goal ('G') in the environment description.\n",
    "    \"\"\"\n",
    "    for row in range(len(env_desc)):\n",
    "        for col in range(len(env_desc[row])):\n",
    "            if env_desc[row][col] == b'G':\n",
    "                return row, col\n",
    "    return None\n",
    "\n",
    "def manhattan_distance(point_a, point_b):\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between two points.\n",
    "    \"\"\"\n",
    "    return abs(point_a[0] - point_b[0]) + abs(point_a[1] - point_b[1])\n",
    "\n",
    "def calculate_intermediate_reward(current_state, next_state, env, visited_states, forward_step_reward=0, visited_step_reward=0):\n",
    "    \"\"\"\n",
    "    Calculate intermediate reward based on movement towards the goal.\n",
    "    \"\"\"\n",
    "    if next_state in visited_states:\n",
    "        return visited_step_reward  # Negative reward for visiting a state that has already been visited\n",
    "    \n",
    "    #goal_position = find_goal_position(env.desc)\n",
    "    # Convert state to 2D coordinates\n",
    "    #ncols = len(env.desc[0])\n",
    "    #current_row, current_col = divmod(current_state, ncols)\n",
    "    #next_row, next_col = divmod(next_state, ncols)\n",
    "\n",
    "    #current_distance = manhattan_distance((current_row, current_col), goal_position)\n",
    "    #next_distance = manhattan_distance((next_row, next_col), goal_position)\n",
    "\n",
    "    return forward_step_reward if next_state > current_state else 0\n",
    "    #return forward_step_reward if next_distance < current_distance else 0\n",
    "\n",
    "\n",
    "def create_vectorized_environments(env_name, n_envs, random_map, is_slippery):\n",
    "    if random_map:\n",
    "        envs = [gym.make(env_name, desc=generate_random_map(size=4), is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    else:\n",
    "        envs = [gym.make(env_name, is_slippery=is_slippery) for _ in range(n_envs)]\n",
    "    return envs\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, optimizer, loss_fn, gamma, epsilon_start, epsilon_decay, num_episodes, device, n_states, random_map=False, is_slippery=False, hole_reward=-1, forward_step_reward=0, visited_step_reward=0):\n",
    "    n_envs = 8\n",
    "    max_steps = 4 * 4 * 4 # 4x the number of states seems like a reasonable upper bound\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    # Training loop\n",
    "    # Initialize lists to store metrics\n",
    "    losses = []\n",
    "    weight_norms = []\n",
    "    bias_norms = []\n",
    "    grad_norms = []\n",
    "    total_rewards = []\n",
    "    epsilon_values = []\n",
    "    episode_lengths = []\n",
    "    successful_episodes = 0\n",
    "    success_rate = []\n",
    "\n",
    "    plot_update_frequency = int(num_episodes * (5/100))  # update plots every 5% of episodes\n",
    "    if plot_update_frequency == 0:\n",
    "        plot_update_frequency = 1\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        model.train()\n",
    "        # Create vectorized environments\n",
    "        envs = create_vectorized_environments('FrozenLake-v1', n_envs, random_map, is_slippery)\n",
    "\n",
    "\n",
    "        states = [env.reset()[0] for env in envs]\n",
    "        dones = [False] * n_envs\n",
    "        episode_rewards = [0] * n_envs\n",
    "        visited_states = [set() for _ in envs]\n",
    "        episode_lengths = [0] * n_envs\n",
    "\n",
    "        step_count = 0\n",
    "        ongoing_indices = list(range(n_envs))  # Initialize with all environment indices\n",
    "\n",
    "        # TODO: fix loss bug when len is 1\n",
    "        while len(ongoing_indices) > 1 and step_count < max_steps:\n",
    "            step_count += 1\n",
    "            # Filter out completed environments for action selection and state update\n",
    "            ongoing_envs = [envs[i] for i in ongoing_indices]\n",
    "            ongoing_states = [states[i] for i in ongoing_indices]\n",
    "\n",
    "\n",
    "            # Preprocess all states and convert them into tensors\n",
    "            state_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(ongoing_states, ongoing_envs)] \n",
    "            # Combine all state tensors into a single batch\n",
    "            state_tensor_batch = torch.stack(state_tensors)\n",
    "            \n",
    "            # Compute Q-values for the entire batch\n",
    "            q_values_batch = model(state_tensor_batch)\n",
    "            \n",
    "            # Iterate over each environment to select actions\n",
    "            actions = []\n",
    "            for i, (q_values, env) in enumerate(zip(q_values_batch, ongoing_envs)):\n",
    "                visited_states[ongoing_indices[i]] .add(states[ongoing_indices[i]])\n",
    "            \n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = torch.argmax(q_values).item()\n",
    "            \n",
    "                actions.append(action)\n",
    "\n",
    "\n",
    "            next_states, rewards, next_dones = [], [], []\n",
    "            for i, (env, action) in enumerate(zip(ongoing_envs, actions)):\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                # Custom reward logic\n",
    "                if done and reward == 0:  # Agent fell into a hole\n",
    "                    reward = hole_reward\n",
    "                else:\n",
    "                    # Additional logic to calculate reward for moving towards the goal\n",
    "                    reward += calculate_intermediate_reward(states[ongoing_indices[i]], next_state, env, visited_states, forward_step_reward, visited_step_reward)\n",
    "\n",
    "                next_states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                next_dones.append(done)\n",
    "                episode_rewards[ongoing_indices[i]] += reward\n",
    "                episode_lengths[ongoing_indices[i]]  += 1\n",
    "            \n",
    "            \n",
    "            next_states_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(next_states, ongoing_envs)]\n",
    "            next_states_batch = torch.stack(next_states_tensors)\n",
    "                        # Create batches for rewards, actions, and dones\n",
    "            reward_batch = torch.tensor(rewards, device=device)\n",
    "            action_batch = torch.tensor(actions, device=device)\n",
    "            done_batch = torch.tensor(next_dones, device=device)\n",
    "            \n",
    "            # Compute Q-values for the entire batch\n",
    "            next_q_values_batch = model(next_states_batch)\n",
    "            max_next_state_q_values = torch.max(next_q_values_batch, dim=1)[0]\n",
    "            # Compute the target Q-values\n",
    "            target_q_values = reward_batch + gamma * max_next_state_q_values * (1 - done_batch.float())\n",
    "            #print(q_values_batch.shape) # (8, 4)\n",
    "            #print(action_batch.shape) # (8,)\n",
    "            #print(target_q_values.shape) # (8,)\n",
    "            # Correct way to get predicted Q values for the taken actions\n",
    "            predicted_q_values = q_values_batch.gather(1, action_batch.unsqueeze(-1)).squeeze()\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(predicted_q_values, target_q_values)\n",
    "            \n",
    "            # Rest of your optimization logic\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update states of ongoing indices\n",
    "            for i, (next_state, done) in enumerate(zip(next_states, next_dones)):\n",
    "                if not done:\n",
    "                    states[ongoing_indices[i]] = next_state\n",
    "\n",
    "            # Calculate and store loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Calculate and store weight and bias norms\n",
    "            weight_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() > 1)\n",
    "            bias_norm = sum(torch.norm(param)**2 for param in model.parameters() if param.dim() == 1)\n",
    "            weight_norms.append(weight_norm.item())\n",
    "            bias_norms.append(bias_norm.item())\n",
    "\n",
    "            # Calculate and store gradient norms\n",
    "            grad_norm = sum(torch.norm(param.grad)**2 for param in model.parameters() if param.grad is not None)\n",
    "            grad_norms.append(grad_norm.item())\n",
    "\n",
    "            new_ongoing_indices = []\n",
    "            for i, (done, reward) in enumerate(zip(next_dones, rewards)):\n",
    "                if done :\n",
    "                    if reward > 0:\n",
    "                        successful_episodes += 1\n",
    "                else:\n",
    "                    new_ongoing_indices.append(ongoing_indices[i])\n",
    "            ongoing_indices = new_ongoing_indices\n",
    "        # Close all environments at the end of the episode\n",
    "        for env in envs:\n",
    "            env.close()\n",
    "        \n",
    "        # Store total reward and epsilon value for the episode\n",
    "        total_rewards.append(episode_rewards)\n",
    "        epsilon_values.append(epsilon)\n",
    "        success_rate.append(successful_episodes / (episode + 1))  # Calculate success rate\n",
    "\n",
    "        # Decay epsilon\n",
    "        if epsilon > 0.01:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if episode % plot_update_frequency == 0 or episode == num_episodes - 1:\n",
    "            # log metrics to wandb  \n",
    "            eval_success_rate = evaluate_model(model, 100, device, n_states, n_envs, is_slippery, random_map)\n",
    "            wandb.log({\"loss\": loss.item(),\n",
    "                       \"weight_norm\": weight_norm.item(),\n",
    "                       \"bias_norm\": bias_norm.item(),\n",
    "                       \"grad_norm\": grad_norm.item(),\n",
    "                       \"epsilon\": epsilon,\n",
    "                       \"eval_success_rate\": eval_success_rate,\n",
    "                       \"episode\": episode,\n",
    "                       \"average_episode_length\": sum(episode_lengths)/len(episode_lengths),\n",
    "                       \"learning_rate\": learning_rate})\n",
    "                \n",
    "\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.722610Z",
     "start_time": "2023-12-27T00:48:31.706481Z"
    }
   },
   "id": "ca47908718e26a43"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def evaluate_model(model, num_eval_episodes, device, n_states, batch_size=10, is_slippery=False, random_map=False):\n",
    "    #TODO: give other metrics such as average episode length, average reward, etc.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        successful_episodes = 0\n",
    "        total_evaluated = 0\n",
    "    \n",
    "        while total_evaluated < num_eval_episodes:\n",
    "            envs = []\n",
    "            states = []\n",
    "            dones = []\n",
    "            for _ in range(min(batch_size, num_eval_episodes - total_evaluated)):\n",
    "                if random_map:\n",
    "                    env = gym.make('FrozenLake-v1', desc=generate_random_map(size=4), is_slippery=is_slippery)\n",
    "                else:\n",
    "                    env = gym.make('FrozenLake-v1', is_slippery=is_slippery)\n",
    "                envs.append(env)\n",
    "                states.append(env.reset()[0])\n",
    "                dones.append(False)\n",
    "    \n",
    "            max_steps = len(envs[0].desc) * len(envs[0].desc[0]) * 4\n",
    "            step_count = 0\n",
    "    \n",
    "            while step_count < max_steps and not all(dones):\n",
    "                step_count += 1\n",
    "                state_tensors = [preprocess_state(state, env.desc).to(device) for state, env in zip(states, envs)]\n",
    "                state_batch = torch.stack(state_tensors)\n",
    "                q_values = model(state_batch)\n",
    "                actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
    "    \n",
    "                for i, env in enumerate(envs):\n",
    "                    if not dones[i]:\n",
    "                        next_state, reward, done, _, _ = env.step(actions[i])\n",
    "                        states[i] = next_state\n",
    "                        dones[i] = done\n",
    "    \n",
    "                        if done and reward > 0:\n",
    "                            successful_episodes += 1\n",
    "    \n",
    "            total_evaluated += len(envs)\n",
    "    \n",
    "        success_rate = successful_episodes / num_eval_episodes\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.735475Z",
     "start_time": "2023-12-27T00:48:31.721082Z"
    }
   },
   "id": "bca62607ea444a77"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    wandb.init(project=\"frozenlake_slipperry_optuna_reward_search_convnet_random_map\",\n",
    "               name=f\"trial_{trial.number}\",\n",
    "               config=trial.params,\n",
    "               reinit=True)\n",
    "    # find rewards\n",
    "    hole_reward = trial.suggest_float(\"hole_reward\", -1, 0)\n",
    "    forward_step_reward = trial.suggest_float(\"forward_step_reward\", 0, 1)\n",
    "    visited_step_reward = trial.suggest_float(\"visited_step_reward\", -1, 0)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.0001\n",
    "    gamma = 0.99\n",
    "    epsilon = 0.8\n",
    "    epsilon_decay = 0.999\n",
    "    num_episodes = 2500\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_states = env.observation_space.n\n",
    "    model = ConvNet(n_states, n_actions).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "    random_map = True\n",
    "    is_slippery = False\n",
    "\n",
    "    trained_model = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, random_map=random_map, is_slippery=is_slippery,\n",
    "                                hole_reward=hole_reward, forward_step_reward=forward_step_reward, visited_step_reward=visited_step_reward)\n",
    "    \n",
    "    num_eval_episodes = 50 \n",
    "    success_rate = evaluate_model(trained_model, num_eval_episodes, device, n_states, is_slippery, random_map)\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "    return success_rate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.737006Z",
     "start_time": "2023-12-27T00:48:31.724823Z"
    }
   },
   "id": "da966d3235295555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "study"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bac2364ed794ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nstudy = optuna.create_study(direction=\\'maximize\\')\\nstudy.optimize(objective, n_trials=50)\\n\\nprint(\"Best trial:\")\\ntrial = study.best_trial\\nprint(f\" Value: {trial.value}\")\\nprint(\" Params: \")\\nfor key, value in trial.params.items():\\n    print(f\"    {key}: {value}\")\\n'"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\" Value: {trial.value}\")\n",
    "print(\" Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.737389Z",
     "start_time": "2023-12-27T00:48:31.727738Z"
    }
   },
   "id": "380fbbe2086bcac1"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.737698Z",
     "start_time": "2023-12-27T00:48:31.730271Z"
    }
   },
   "id": "907a4ada3bd611a3"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "random_map = True\n",
    "is_slippery = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:48:31.747404Z",
     "start_time": "2023-12-27T00:48:31.739019Z"
    }
   },
   "id": "40fe2a1e4824ecb7"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/loyd/PycharmProjects/PytorchPractice/notebooks/wandb/run-20231227_014831-1aoh3k4h</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/1aoh3k4h' target=\"_blank\">First batch attemps</a></strong> to <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/1aoh3k4h' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/1aoh3k4h</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:46<00:00,  2.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c24e60fba364905ac003914a02916c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B sync reduced upload amount by 16.1%             "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>▁▂▁▁▂▁▂▁▂▂▁▂▃▂▁▄▁▁█▁▁</td></tr><tr><td>bias_norm</td><td>▇███▇▇▆▅▄▄▄▃▂▂▂▂▂▂▂▁▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epsilon</td><td>█▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval_success_rate</td><td>▁▄▃▂▃▂▃▃▄▅▇▅▅▆█▇▅▆▁▇▄</td></tr><tr><td>grad_norm</td><td>▁▂██▆█▃▂▄▃▄▃█▅▃▁▃▃▁█▃</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁▁▅▄▃▃▂▃▂▁▂▂▄▂▂▁▂▁▁█▁</td></tr><tr><td>weight_norm</td><td>▁▁▂▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_episode_length</td><td>5.75</td></tr><tr><td>bias_norm</td><td>0.64172</td></tr><tr><td>episode</td><td>999</td></tr><tr><td>epsilon</td><td>0.00999</td></tr><tr><td>eval_success_rate</td><td>0.39</td></tr><tr><td>grad_norm</td><td>1.39275</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>loss</td><td>0.02263</td></tr><tr><td>weight_norm</td><td>22.57725</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">First batch attemps</strong> at: <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/1aoh3k4h' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/runs/1aoh3k4h</a><br/> View job at <a href='https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTgzMjg5Mw==/version_details/v3' target=\"_blank\">https://wandb.ai/loyd-team/frozenlake_slipperry_batches_convnet_random_map/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTgzMjg5Mw==/version_details/v3</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20231227_014831-1aoh3k4h/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "num_episodes = 1000\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "model = ConvNet(n_states, n_actions).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "wandb.init(project=\"frozenlake_slipperry_batches_convnet_random_map\",\n",
    "           name=f\"First batch attemps\",\n",
    "           reinit=True)\n",
    "model = train_model(model, optimizer, loss_fn, gamma, epsilon, epsilon_decay, num_episodes, device, n_states, \n",
    "                    random_map=random_map, is_slippery=is_slippery,\n",
    "                    forward_step_reward=0.05,\n",
    "                    visited_step_reward=0,\n",
    "                    hole_reward=0)# maybe try 0 and not -0.05 again and run for longer so that it learns to avoid holes\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:55:30.652056Z",
     "start_time": "2023-12-27T00:48:31.748932Z"
    }
   },
   "id": "95783e41ab8de557"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "#TODO: maybe try to give bigger rewards the closer we get to the goal\n",
    "#TODO: try batch normalization\n",
    "#TODO: try 1 hot encoding the state instead of the current encoding\n",
    "#TODO: have datapoints for loss every step, but only evaluate model every few steps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:55:30.658212Z",
     "start_time": "2023-12-27T00:55:30.652199Z"
    }
   },
   "id": "e56de81bff3d726c"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate:  0.48\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "num_eval_episodes = 100\n",
    "success_rate = evaluate_model(model, num_eval_episodes, device, n_states, batch_size=10, is_slippery=is_slippery, random_map=random_map)\n",
    "print(\"Success Rate: \", success_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:55:47.532256Z",
     "start_time": "2023-12-27T00:55:45.284273Z"
    }
   },
   "id": "510270e518b47635"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (32x16 and 512x4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 50\u001B[0m\n\u001B[1;32m     47\u001B[0m         plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPolicy Visualization for Map \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmap_idx\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     48\u001B[0m         plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m---> 50\u001B[0m visualize_policy_on_random_maps(model, device, n_maps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, is_slippery\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[61], line 18\u001B[0m, in \u001B[0;36mvisualize_policy_on_random_maps\u001B[0;34m(model, device, n_maps, n_states, is_slippery)\u001B[0m\n\u001B[1;32m     14\u001B[0m numerical_grid_colors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvectorize(numerical_color_map\u001B[38;5;241m.\u001B[39mget)(desc)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Extracting the policy from the model for the current map\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m policy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([torch\u001B[38;5;241m.\u001B[39margmax(model(preprocess_state(s, env\u001B[38;5;241m.\u001B[39mdesc)\u001B[38;5;241m.\u001B[39mto(device)))\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(env\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mn)])\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Mapping actions to symbols for visualization\u001B[39;00m\n\u001B[1;32m     21\u001B[0m action_symbols \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m←\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m↓\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m→\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m3\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m↑\u001B[39m\u001B[38;5;124m'\u001B[39m}\n",
      "Cell \u001B[0;32mIn[61], line 18\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     14\u001B[0m numerical_grid_colors \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvectorize(numerical_color_map\u001B[38;5;241m.\u001B[39mget)(desc)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Extracting the policy from the model for the current map\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m policy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([torch\u001B[38;5;241m.\u001B[39margmax(model(preprocess_state(s, env\u001B[38;5;241m.\u001B[39mdesc)\u001B[38;5;241m.\u001B[39mto(device)))\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(env\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mn)])\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Mapping actions to symbols for visualization\u001B[39;00m\n\u001B[1;32m     21\u001B[0m action_symbols \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m←\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m↓\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m→\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m3\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m↑\u001B[39m\u001B[38;5;124m'\u001B[39m}\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[48], line 24\u001B[0m, in \u001B[0;36mConvNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x))\n\u001B[1;32m     23\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: linear(): input and weight.T shapes cannot be multiplied (32x16 and 512x4)"
     ]
    }
   ],
   "source": [
    "def visualize_policy_on_random_maps(model, device, n_maps=5, n_states=16, is_slippery=False):\n",
    "    # Create a color map for the grid with your specified colors\n",
    "\n",
    "\n",
    "    for map_idx in range(n_maps):\n",
    "        # Create environment with a random map\n",
    "        env = gym.make('FrozenLake-v1', desc=generate_random_map(size=4), is_slippery=is_slippery)\n",
    "\n",
    "        # Convert env.desc to a string format\n",
    "        desc = env.desc.astype(str)\n",
    "\n",
    "        # Create a color map for the grid\n",
    "        numerical_color_map = {'S': 2, 'F': 1, 'H': 0, 'G': 3}\n",
    "        numerical_grid_colors = np.vectorize(numerical_color_map.get)(desc)\n",
    "\n",
    "\n",
    "        # Extracting the policy from the model for the current map\n",
    "        policy = np.array([torch.argmax(model(preprocess_state(s, env.desc).to(device))).item() for s in range(env.observation_space.n)])\n",
    "\n",
    "        # Mapping actions to symbols for visualization\n",
    "        action_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "        policy_symbols = np.vectorize(action_symbols.get)(policy)\n",
    "\n",
    "        # Reshape for grid visualization\n",
    "        policy_grid = policy_symbols.reshape(env.desc.shape)\n",
    "\n",
    "        # Plotting the policy grid\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        # Create a ListedColormap for custom colors\n",
    "        cmap = ListedColormap(['black', 'lightblue', 'lightblue', 'yellow'])  # Order: H, F, S, G\n",
    "        plt.imshow(numerical_grid_colors, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "        for i in range(desc.shape[0]):\n",
    "            for j in range(desc.shape[1]):\n",
    "                arrow = policy_grid[i, j]\n",
    "                arrow_color = 'white' if desc[i, j] in ['H', 'G'] else 'black'\n",
    "                \n",
    "                        # Checking if the arrow points to a hole\n",
    "                if desc[i, j] != 'H':\n",
    "                    if (arrow == '←' and j > 0 and desc[i, j-1] == 'H') or \\\n",
    "                       (arrow == '→' and j < desc.shape[1] - 1 and desc[i, j+1] == 'H') or \\\n",
    "                       (arrow == '↑' and i > 0 and desc[i-1, j] == 'H') or \\\n",
    "                       (arrow == '↓' and i < desc.shape[0] - 1 and desc[i+1, j] == 'H'):\n",
    "                        arrow_color = 'red'\n",
    "                \n",
    "                plt.text(j, i, policy_grid[i, j], ha='center', va='center', fontsize=20, color=arrow_color)\n",
    "        plt.title(f'Policy Visualization for Map {map_idx+1}')\n",
    "        plt.show()\n",
    "\n",
    "visualize_policy_on_random_maps(model, device, n_maps=5, is_slippery=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:55:32.912809Z",
     "start_time": "2023-12-27T00:55:32.858598Z"
    }
   },
   "id": "bcec4f1a4aa586bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T00:55:32.914514Z",
     "start_time": "2023-12-27T00:55:32.913744Z"
    }
   },
   "id": "1cb37ca18ba4e6b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
