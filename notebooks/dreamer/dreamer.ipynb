{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.757120Z",
     "start_time": "2024-01-20T15:04:21.665151Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchrl.data import PrioritizedReplayBuffer, ListStorage\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "FrozenLake environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f83b02f493c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "debug = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.773513Z",
     "start_time": "2024-01-20T15:04:21.717524Z"
    }
   },
   "id": "cc51746a6578d293",
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.781815Z",
     "start_time": "2024-01-20T15:04:21.764341Z"
    }
   },
   "id": "f6f1fa210c2ad0e",
   "execution_count": 198
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3,\n",
    "                     'F': 0, 'H': 1, 'S': 0, 'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout\n",
    "\n",
    "\n",
    "def convert_tensor_to_layout(tensor_layout):\n",
    "    # Define the mapping from tensor indices back to layout characters\n",
    "    index_to_layout = {0: b'F', 1: b'H', 2: b'S', 3: b'G'}\n",
    "    \n",
    "    # Get the size of the tensor\n",
    "    batch_size, nrows, ncols, _ = tensor_layout.size()\n",
    "    \n",
    "    # Initialize the layout list\n",
    "    layouts = []\n",
    "    \n",
    "    # Iterate over each layout in the batch\n",
    "    for b in range(batch_size):\n",
    "        # Initialize the layout for the current batch\n",
    "        layout = []\n",
    "        \n",
    "        # Iterate over each row\n",
    "        for i in range(nrows):\n",
    "            # Initialize the row\n",
    "            row = []\n",
    "            \n",
    "            # Iterate over each column\n",
    "            for j in range(ncols):\n",
    "                # Find the index of the maximum value in the last dimension (status)\n",
    "                status_index = tensor_layout[b, i, j].argmax().item()\n",
    "                \n",
    "                # Convert the index to a layout character and append to the row\n",
    "                row.append(index_to_layout[status_index])\n",
    "            \n",
    "            # Append the row (converted to a numpy array) to the layout\n",
    "            layout.append(row)\n",
    "        \n",
    "        # Append the layout (converted to a numpy array) to the layouts list\n",
    "        layouts.append(layout)\n",
    "    \n",
    "    # Convert the layouts list to a numpy array and return\n",
    "    return layouts\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.802831Z",
     "start_time": "2024-01-20T15:04:21.787057Z"
    }
   },
   "id": "44567364fcc9fa67",
   "execution_count": 199
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n",
      "tensor([[[[0., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = convert_layout_to_tensor(generate_random_map(4))\n",
    "print(tensor)\n",
    "updated_tensor = update_start_positions(tensor, [[0]])\n",
    "print(updated_tensor) # dimensions: batch_size x nrows x ncols x num_statuses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.836751Z",
     "start_time": "2024-01-20T15:04:21.804646Z"
    }
   },
   "id": "26914cb622923df9",
   "execution_count": 200
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of DeamerV3 model\n",
    "![DeamerV3 model](model_names.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd95da8c410c3ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_size, xt_size, gru_hidden_size, num_layers, zt_dim, num_categories, reward_size, continue_size):\n",
    "        super(WorldModel, self).__init__()\n",
    "\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, gru_hidden_size, num_layers, batch_first=True, bias=False)\n",
    "        # Input normalization layer for GRU\n",
    "        self.input_norm = nn.LayerNorm(input_size)\n",
    "        #self.input_norm = nn.BatchNorm1d(input_size)\n",
    "        #self.input_norm = nn.Identity()\n",
    "        # Hidden state normalization layer for GRU\n",
    "        self.hidden_norm = nn.LayerNorm(gru_hidden_size)\n",
    "\n",
    "        # Dynamics predictor\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, zt_dim * num_categories)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, reward_size)\n",
    "        )\n",
    "        \n",
    "        # Continue signal predictor\n",
    "        self.continue_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, continue_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder predictor\n",
    "        self.decoder_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, xt_size)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + xt_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, zt_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, ztat, xt, ht_1):\n",
    "        batch_size = ztat.size(0)\n",
    "        if ht_1 is None:\n",
    "            # Initialize hidden state with zeros if not provided\n",
    "            ht_1 = torch.zeros(self.num_layers, ztat.size(0), self.gru_hidden_size).to(xt.device)\n",
    "\n",
    "        # Normalize GRU input\n",
    "        normalized_ztat = self.input_norm(ztat)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        _, ht = self.gru(normalized_ztat, ht_1)\n",
    "        # Normalize GRU output (hidden state)\n",
    "        ht = self.hidden_norm(ht)\n",
    "       \n",
    "        zt = self.encode(xt, ht)\n",
    "        \n",
    "        zt_hat = self.dynamics_predictor(ht)\n",
    "        # clamp logits to avoid numerical instability\n",
    "        zt_hat = torch.clamp(zt_hat, -10, 10)\n",
    "\n",
    "        htzt = torch.cat((ht.view(batch_size, -1), zt.view(batch_size, -1)), dim=1)\n",
    "        rt_hat = self.reward_predictor(htzt)\n",
    "        ct_hat = self.continue_predictor(htzt)\n",
    "        xt_hat = self.decoder_predictor(htzt)\n",
    "    \n",
    "        return ht, zt_hat, rt_hat, ct_hat, xt_hat, zt\n",
    "    \n",
    "    def encode(self, xt, ht, temperature=1.0):\n",
    "        batch_size = xt.size(0)\n",
    "        ht = ht.view(batch_size, -1)  # it's not batch first\n",
    "        logits = self.encoder(torch.cat((xt, ht), dim=1))\n",
    "        # clamp logits to avoid numerical instability\n",
    "        logits = torch.clamp(logits, -10, 10)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.854935Z",
     "start_time": "2024-01-20T15:04:21.827447Z"
    }
   },
   "id": "cc7ed508cc0c5986",
   "execution_count": 201
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\"Symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Inverse of symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "class SymlogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymlogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Apply symlog transformation to the target\n",
    "\n",
    "        transformed_target = symlog(target)\n",
    "        # Compute MSE Loss between input and transformed target\n",
    "        return F.mse_loss(input, transformed_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.899760Z",
     "start_time": "2024-01-20T15:04:21.857774Z"
    }
   },
   "id": "c75090797e8abeae",
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.919168Z",
     "start_time": "2024-01-20T15:04:21.902708Z"
    }
   },
   "id": "7f334ff705d60e11",
   "execution_count": 203
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640b615c97d3cb37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"mps\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.960465Z",
     "start_time": "2024-01-20T15:04:21.919952Z"
    }
   },
   "id": "21168a3b14a069b2",
   "execution_count": 204
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "latent_size = 128 # aka zt size\n",
    "num_categories = 4 # number of categories in latent space\n",
    "num_layers = 1\n",
    "\n",
    "ht_size = 32\n",
    "xt_size = 4*4*4\n",
    "reward_size = 1\n",
    "continue_size = 1\n",
    "worldModel = WorldModel(latent_size * num_categories + n_actions, xt_size, ht_size, num_layers, latent_size, num_categories, reward_size, continue_size).to(device)\n",
    "worldModel_optimizer = optim.Adam(worldModel.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-5)\n",
    "\n",
    "actor = Actor(ht_size + latent_size*num_categories, n_actions).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-5, eps=1e-5, weight_decay=1e-5)\n",
    "critic = Critic(ht_size + latent_size*num_categories ).to(device)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-5, eps=1e-5, weight_decay=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:21.990531Z",
     "start_time": "2024-01-20T15:04:21.950803Z"
    }
   },
   "id": "6186b465c849ee41",
   "execution_count": 205
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_world_model(worldModel, worldModel_optimizer, replay_buffer, actor, batch_size=32, steps=15, logging=False):\n",
    "    actor.eval()\n",
    "    experiences, info = replay_buffer.sample(batch_size, return_info=True)\n",
    "    # Unpack experiences\n",
    "    \n",
    "    experience_losses = torch.zeros((batch_size, 1), device=device)\n",
    "    \n",
    "    states = experiences[0]\n",
    "    states = states.int()\n",
    "    \n",
    "    # get envs from states\n",
    "    envs = []\n",
    "    for state in states:\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        env.reset()\n",
    "        env.state = state\n",
    "        envs.append(env)\n",
    "    \n",
    "    first_prediction_loss = None\n",
    "    # hidden states are not batch first\n",
    "    hidden_states = torch.zeros((1, batch_size, ht_size), device=device, dtype=torch.float)\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc for env in envs]).to(device)\n",
    "\n",
    "\n",
    "    dones_bool = [False] * batch_size\n",
    "    for step in range(steps):\n",
    "        # reset done envs\n",
    "        for i, done in enumerate(dones_bool):\n",
    "            if done:\n",
    "                envs[i].reset()\n",
    "                states[i] = envs[i].state\n",
    "                hidden_states[0][i] = torch.zeros((1, ht_size), device=device, dtype=torch.float)\n",
    "                dones_bool[i] = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            states_tensor = update_start_positions(layout_tensor.clone(), states)\n",
    "            states_tensor = states_tensor.view(batch_size, -1).to(device)\n",
    "            zts = worldModel.encode(states_tensor, hidden_states)\n",
    "            actor_state = torch.cat((hidden_states.view(batch_size, -1), zts.view(batch_size,-1)), dim=1)\n",
    "            action_probs = actor(actor_state)\n",
    "            actions = torch.multinomial(action_probs, num_samples=1)\n",
    "\n",
    "        next_states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for env, action in zip(envs, actions):\n",
    "            #TODO: if done should reset\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            # try non sparse reward:\n",
    "            reward = reward + (next_state%4 + next_state//4)/(16 * 3) \n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            next_states.append(next_state)\n",
    "        \n",
    "        dones_bool = torch.tensor(dones, device=device, dtype=torch.bool)\n",
    "        dones = torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        # print number of rewards equal to 1\n",
    "        #count = (rewards >= 1).sum()\n",
    "        #if count >0:\n",
    "        #    print(f\"Number of rewards equal to 1: {count}, in step {step}\")\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            next_states_tensor = update_start_positions(layout_tensor.clone(), next_states).view(batch_size, -1).to(device)   \n",
    "\n",
    "            zts = worldModel.encode(states_tensor, hidden_states)\n",
    "    \n",
    "        # Integrated model loss\n",
    "        action_one_hot = F.one_hot(actions.detach(), n_actions).view(batch_size, -1).to(device)\n",
    "    \n",
    "\n",
    "        integratedModel_input = torch.cat((zts, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(batch_size, 1, -1)\n",
    "        \n",
    "   \n",
    "        ht, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, states_tensor, hidden_states)\n",
    "        xt = next_states_tensor\n",
    "        rt = rewards.view(batch_size, 1)\n",
    "        ct = (1 - dones).view(batch_size, 1)\n",
    "        #loss_fn = SymlogLoss()\n",
    "        # Define the loss functions with reduction set to 'none' to keep individual losses\n",
    "        xt_loss_fn = nn.MSELoss(reduction='none')\n",
    "        rt_loss_fn = nn.MSELoss(reduction='none')\n",
    "        ct_loss_fn = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        # Compute the losses for each element in the batch (without reducing them)\n",
    "        xt_loss = xt_loss_fn(xt_hat, xt)\n",
    "        rt_loss = rt_loss_fn(rt_hat, rt)\n",
    "        ct_loss = ct_loss_fn(ct_hat, ct)\n",
    "        \n",
    "        # You can now use xt_loss, rt_loss, and ct_loss to update buffer weights\n",
    "        # Each *_loss tensor contains the individual losses for each example in the batch\n",
    "        \n",
    "        # If you want a single loss value for backward pass, you can reduce the losses manually\n",
    "        prediction_loss = (xt_loss.mean() + rt_loss.mean() + ct_loss.mean())\n",
    "        \n",
    "        zt = zt.view(batch_size, -1)\n",
    "        zt_hat = zt_hat.view(batch_size, -1)\n",
    "        threshold = 0.02 # TODO: not following paper here, should be 1 threshold and kl loss\n",
    "        #kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "        #dynamic_loss = torch.max(torch.tensor(threshold), kl_loss(F.log_softmax(zt_hat, dim=1), zt.detach()))\n",
    "        #representation_loss = torch.max(torch.tensor(threshold), kl_loss(F.log_softmax(zt, dim=1) , zt_hat.detach()))\n",
    "        # try MSE\n",
    "        dynamic_loss = torch.max(torch.tensor(threshold), F.mse_loss(zt_hat, zt.detach()))\n",
    "        representation_loss = torch.max(torch.tensor(threshold), F.mse_loss(zt, zt_hat.detach()))\n",
    "\n",
    "        \n",
    "        worldModel_loss = Bpred * prediction_loss + Bdyn * dynamic_loss + Brep * representation_loss\n",
    "        \n",
    "        if debug:\n",
    "            if torch.isnan(prediction_loss).any(): print(\"Prediction losses: \", prediction_loss)\n",
    "            if torch.isnan(dynamic_loss).any(): print(\"Dynamic losses: \", dynamic_loss)\n",
    "            if torch.isnan(representation_loss).any(): print(\"Representation losses: \", representation_loss)\n",
    "            if torch.isnan(worldModel_loss).any(): print(\"World model losses: \", worldModel_loss)\n",
    "            if torch.isnan(xt).any(): print(\"Xt: \", xt)\n",
    "            if torch.isnan(xt_hat).any(): print(\"Xt_hat: \", xt_hat)\n",
    "            if torch.isnan(rt).any(): print(\"Rt: \", rt)\n",
    "            if torch.isnan(rt_hat).any(): print(\"Rt_hat: \", rt_hat)\n",
    "            if torch.isnan(ct).any(): print(\"Ct: \", ct)\n",
    "            if torch.isnan(ct_hat).any(): print(\"Ct_hat: \", ct_hat)\n",
    "            if torch.isnan(zt).any(): print(\"Zt: \", zt)\n",
    "            if torch.isnan(zt_hat).any(): print(\"Zt_hat: \", zt_hat)\n",
    "            if torch.isnan(ht).any(): print(\"Ht: \", ht)\n",
    "            if torch.isnan(hidden_states).any(): print(\"Ht-1: \", hidden_states)\n",
    "            if torch.isnan(actions).any(): print(\"Action: \", actions)\n",
    "            if torch.isnan(integratedModel_input).any(): print(\"Model input: \", integratedModel_input)\n",
    "            if torch.isnan(zts).any(): print(\"zts: \", zts)\n",
    "            if torch.isnan(states_tensor).any(): print(\"States tensor: \", states_tensor)\n",
    "\n",
    "        \n",
    "        worldModel_optimizer.zero_grad()\n",
    "        worldModel_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worldModel.parameters(), 10)#TODO: change back to paper values\n",
    "        worldModel_optimizer.step()\n",
    "        \n",
    "        states = next_states\n",
    "        hidden_states = ht.detach()\n",
    "        # reset done envs\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                envs[i].reset()\n",
    "                states[i] = envs[i].state\n",
    "                hidden_states[0][i] = torch.zeros((1, ht_size), device=device, dtype=torch.float)\n",
    "                dones[i] = False\n",
    "\n",
    "\n",
    "        experience_losses += rt_loss.detach() + ct_loss.detach()\n",
    "\n",
    "        \n",
    "        if logging:\n",
    "            grad_norm = sum([torch.norm(param.grad) for param in worldModel.parameters()])\n",
    "            weight_norm = sum([torch.norm(param) for param in worldModel.parameters()])\n",
    "            wandb.log({\"worldModel_loss\": worldModel_loss.item(),\n",
    "                       \"worldModel_grads\": grad_norm,\n",
    "                       \"representation_loss\": representation_loss.item(),\n",
    "                       \"dynamic_loss\": dynamic_loss.item(),\n",
    "                       \"prediction_loss\": prediction_loss.item(),\n",
    "                       \"WorldModel_weight_norm\": weight_norm})\n",
    "            \n",
    "    experience_losses /= steps#TODO: should be individual steps?\n",
    "    #print(first_prediction_loss)\n",
    "    actor.train()\n",
    "    # Update priorities in the replay buffer\n",
    "    # Ensure you have the indices of the sampled experiences. This is typically returned by the sample method of the replay buffer.\n",
    "    indices = info['index']\n",
    "    #print(f\"Indices: {indices}\")\n",
    "    #print(\"Experience losses: \", experience_losses)\n",
    "    replay_buffer.update_priority(indices, experience_losses.cpu().numpy())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:22.010664Z",
     "start_time": "2024-01-20T15:04:21.992844Z"
    }
   },
   "id": "965aa3f2e9e5d8fd",
   "execution_count": 206
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_actor_critic(actor, critic, actor_optimizer, critic_optimizer, worldModel, actor_critics_replay_buffer, batch_size=32, logging=False):\n",
    "    worldModel.eval()\n",
    "    \n",
    "    #sample from buffer\n",
    "    experiences, info = actor_critics_replay_buffer.sample(batch_size, return_info=True)\n",
    "    # Unpack experiences\n",
    "    ht, zt, state_tensors, rewards, dones = experiences # actor states is zt and ht\n",
    "    \n",
    "\n",
    "    dones = dones.float().to(device).view(batch_size, 1).detach()\n",
    "    rewards = rewards.float().to(device).view(batch_size, 1).detach()\n",
    "    \n",
    "    ht = ht.view(batch_size, 1, -1).to(device)\n",
    "\n",
    "    \n",
    "    # Critic loss\n",
    "    imagination_horizon = 4 # TODO: 15 in paper\n",
    "    gamma = 0.997\n",
    "    return_lambda = 0.95\n",
    "    original_input_state = torch.cat((ht.view(batch_size, -1), zt.view(batch_size,-1)), dim=1)\n",
    "    critic_estimation = critic(original_input_state.detach()).view(batch_size, 1)\n",
    "    value = critic_estimation.clone()\n",
    "    lookahead_reward = rewards + (gamma * (1 - return_lambda) * critic_estimation * (1 - dones))\n",
    "    with torch.no_grad():\n",
    "        imagined_dones = dones.clone()\n",
    "        im_actor_states = original_input_state\n",
    "        imagined_xt = state_tensors.view(batch_size, -1)\n",
    "        imagined_ht = ht.view(1, batch_size, -1) # not batch first\n",
    "        imagined_zt = zt\n",
    "        for t in range(0, imagination_horizon):\n",
    "            if imagined_dones.all():\n",
    "                break\n",
    "            \n",
    "            imagined_action_probs = actor(im_actor_states)\n",
    "            imagined_action = torch.multinomial(imagined_action_probs, num_samples=1)\n",
    "            \n",
    "            \n",
    "            im_action_one_hot = F.one_hot(imagined_action, n_actions).view(batch_size, -1).to(device)\n",
    "\n",
    "            imagined_integratedModel_input = torch.cat((imagined_zt.view(batch_size, -1), im_action_one_hot.detach()), dim=1)\n",
    "            # add batch because input needs to have batch, seq, input_size\n",
    "            imagined_integratedModel_input = imagined_integratedModel_input.view(batch_size, 1, -1)\n",
    "            \n",
    "            # Now, use the flattened input for your integrated model\n",
    "            im_next_hidden_state, im_zt_hat, im_rt_hat, im_ct_hat, im_xt_hat , _ = worldModel(imagined_integratedModel_input, imagined_xt, imagined_ht)\n",
    "\n",
    "            im_next_hidden_state = im_next_hidden_state.view(batch_size, -1)\n",
    "            \n",
    "            # Create a mask where 0 indicates the state is done, and 1 indicates it's not done\n",
    "            not_done_mask = 1 - imagined_dones\n",
    "            \n",
    "            # zero rewards if previously done\n",
    "            # Element-wise multiplication between the reward tensor and the not-done mask\n",
    "            im_rt_hat = im_rt_hat * not_done_mask\n",
    "\n",
    "            \n",
    "            imagined_dones = torch.logical_or(imagined_dones, im_ct_hat < 0.5).float()\n",
    "            im_actor_states_hat = torch.cat((imagined_ht.view(batch_size, -1), imagined_zt.view(batch_size,-1)), dim=1)\n",
    "            critic_estimation = critic(im_actor_states_hat.detach())\n",
    "            imagined_reward =  im_rt_hat + (1 - return_lambda) * imagined_dones * critic_estimation\n",
    "\n",
    "            #print(\"--- Imagined step ---\")\n",
    "            #print(\"Imagined reward: \", imagined_reward)\n",
    "            #print(\"Imagined done: \", imagined_dones)\n",
    "            #print(\"Imagined critic estimation: \", critic_estimation)\n",
    "            #print(\"Imagined ct_hat: \", im_ct_hat)\n",
    "            #print(\"Imagined rt_hat: \", im_rt_hat)\n",
    "            #print(\"Imagined zt_hat NAN: \", torch.isnan(im_zt_hat).any())\n",
    "            #print(\"Imagined zt NAN: \", torch.isnan(imagined_zt).any())\n",
    "            #print(\"Imagined ht NAN: \", torch.isnan(imagined_ht).any())\n",
    "            #print(\"Imagined ht: \", imagined_ht)\n",
    "            #print(\"Imagined ht avg: \", imagined_ht.mean())\n",
    "        \n",
    "            \n",
    "            lookahead_reward +=  (gamma ** t) * (return_lambda ** t) * imagined_reward\n",
    "            \n",
    "            \n",
    "            imagined_ht = im_next_hidden_state.view(1, batch_size, -1)\n",
    "            imagined_zt = im_zt_hat\n",
    "            imagined_xt = im_xt_hat\n",
    "            im_actor_states = im_actor_states_hat\n",
    "\n",
    "        # one last critic estimation\n",
    "        if not imagined_dones.all():\n",
    "            lookahead_reward += imagined_dones * (gamma ** imagination_horizon) * (return_lambda ** imagination_horizon-1) * critic(im_actor_states_hat.detach()) #TODO: i think should go to next state, not previous critic estimation.\n",
    "    \n",
    "    target_value = lookahead_reward\n",
    "    critic_loss = F.mse_loss(value, target_value.detach())\n",
    "    \n",
    "    # Compute TD-errors for the sampled experiences\n",
    "    td_errors = torch.abs(target_value.detach() - value.detach()).squeeze()\n",
    "    # Actor loss\n",
    "    advantage = target_value - value.clone().detach()\n",
    "    \n",
    "\n",
    "    # Calculate Actor Loss\n",
    "    # Assuming actions are in the form of indices of the chosen actions\n",
    "    action_probs = actor( torch.cat((ht.view(batch_size, -1), zt.view(batch_size,-1)), dim=1).detach() )  # Probabilities of actions from the actor network\n",
    "    # take actions from probs\n",
    "    actions = torch.multinomial(action_probs, num_samples=1)\n",
    "    \n",
    "    # Gather the probabilities of the actions taken\n",
    "    gathered_probs = action_probs.gather(1, actions.view(-1, 1))\n",
    "    \n",
    "    # Add a small number to probabilities to avoid log(0)\n",
    "    actor_loss = -torch.log(gathered_probs + 1e-8) * advantage.detach()\n",
    "    actor_loss = actor_loss.mean()\n",
    "\n",
    "    #print(\"Advantage: \", advantage)\n",
    "    #print(\"Target value: \", target_value)\n",
    "    #print(\"Value: \", value)\n",
    "    #print(\"Actor loss: \", actor_loss)\n",
    "    #print(\"Critic loss: \", critic_loss)\n",
    "    #print(\"TD errors: \", td_errors)\n",
    "    #print(\"Actions: \", actions)\n",
    "    #print(\"Action probs: \", action_probs)\n",
    "    #print(\"Gathered probs: \", gathered_probs)\n",
    "\n",
    "    # Backpropagation\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 10)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 10)\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    indices = info['index']\n",
    "    actor_critics_replay_buffer.update_priority(indices, td_errors.cpu().numpy())\n",
    "\n",
    "    worldModel.train()\n",
    "\n",
    "    if logging:\n",
    "        actor_grads = sum([torch.norm(param.grad) for param in actor.parameters()])\n",
    "        critic_grads = sum([torch.norm(param.grad) for param in critic.parameters()])\n",
    "        actor_weights_norm = sum([torch.norm(param) for param in actor.parameters()])\n",
    "        critic_weights_norm = sum([torch.norm(param) for param in critic.parameters()])\n",
    "        wandb.log({\"critic_loss\": critic_loss.item(),\n",
    "                    \"critic_grads\": critic_grads,\n",
    "                    \"critic_weights_norm\": critic_weights_norm,\n",
    "                    \"actor_loss\": actor_loss.item(),\n",
    "                    \"actor_grads\": actor_grads,\n",
    "                    \"actor_weights_norm\": actor_weights_norm})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:22.030559Z",
     "start_time": "2024-01-20T15:04:22.013802Z"
    }
   },
   "id": "f446ad77a4fd9f3d",
   "execution_count": 207
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collect_experience(encoded_state_flat, hidden_state, envs, n_envs):\n",
    "    \"\"\"\n",
    "    Handles interaction with the environment and collects experience.\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_state = hidden_state.to(device)\n",
    "    \n",
    "    actor_state = torch.cat((hidden_state.view(n_envs, -1), encoded_state_flat), dim=1)\n",
    "    \n",
    "    action_probs = actor(actor_state)\n",
    "    actions = torch.multinomial(action_probs, 1)\n",
    "    \n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    for env,action in zip(envs,actions):\n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "        # try non sparse reward:\n",
    "        reward = reward + (next_state%4 + next_state//4)/(16 * 3)\n",
    "        next_states.append(next_state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "    \n",
    "    return actions, rewards, next_states, dones\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:04:22.042678Z",
     "start_time": "2024-01-20T15:04:22.019084Z"
    }
   },
   "id": "2ae8c26b1d8d2833",
   "execution_count": 208
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:zj01n0ce) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f54902b8e794801a0835883069dbef1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>sum_rewards</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>sum_rewards</td><td>0.0625</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Trying non sparse rewards.</strong> at: <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/zj01n0ce' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/zj01n0ce</a><br/> View job at <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMDQ0MzAzMQ==/version_details/v22' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMDQ0MzAzMQ==/version_details/v22</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240120_160218-zj01n0ce/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:zj01n0ce). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167378242438039, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fc3955cf41a42d48e518646a64cb9af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/loyd/PycharmProjects/PytorchPractice/notebooks/dreamer/wandb/run-20240120_160422-mhogkcon</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/mhogkcon' target=\"_blank\">Trying non sparse rewards.</a></strong> to <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/mhogkcon' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/mhogkcon</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [04:44<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.006 MB uploaded\\r'), FloatProgress(value=0.16369191188293938, max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1eee6faaf92743779ece45b8161addda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>WorldModel_weight_norm</td><td>██▇▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>actor_grads</td><td>█▃▂▃▁▅▂▂▂▃▃▁▁▂▂▃▄▁▂▃▂▁▂▅▂▂▁▃▅▁▂▂▅▆▃▂▂▂▂▂</td></tr><tr><td>actor_loss</td><td>▁▅▇▇▆▆▆▄▅▆▇▆▇▆▇▅▇▆▆▄▆▅▅█▄▅▆▆▆▆▆▆▇█▅▆▆▆▄▅</td></tr><tr><td>actor_weights_norm</td><td>▇▇███████▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▂▂▁▁▂▃▄</td></tr><tr><td>critic_grads</td><td>█▂▃▄▁▂▁▃▂▂▂▁▄▂▃▂▃▁▂▃▂▁▂▅▃▂▁▂▁▁▁▁▂▄▂▁▂▁▃▂</td></tr><tr><td>critic_loss</td><td>█▁▂▂▁▂▁▂▁▁▁▁▂▁▂▂▂▁▁▂▁▁▁▃▂▁▁▁▃▁▁▁▃▄▁▁▁▁▂▁</td></tr><tr><td>critic_weights_norm</td><td>▂▅█▇▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▂▂▁▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>dynamic_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>prediction_loss</td><td>▇▅▂▄█▄▄▄█▄▁▇▄▇▆▁▃▄▁▇▄▄▆▂▄▄▁▄▃▇▄▅▄▆█▅▄▇▄▅</td></tr><tr><td>representation_loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sum_rewards</td><td>▅▂▆▃▂▃▂▃▃▃▄▃▂▃▄▅▅▃▅▄▄▃▃▂▆▄▄▆▇█▅▂▆▃▅▃▅▁▂▅</td></tr><tr><td>worldModel_grads</td><td>▄▄▃▃▆▄▁▂▆▄▁▃▁▃▄▃▃▆▅▅▂▃▇▃▆█▅█▅█▆▆▃██▅▆▅▅▆</td></tr><tr><td>worldModel_loss</td><td>█▅▂▃▆▃▄▃▆▃▁▆▃▅▅▁▂▄▁▆▃▃▅▂▃▃▁▄▃▆▃▄▃▅▆▄▄▆▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>WorldModel_weight_norm</td><td>222.57423</td></tr><tr><td>actor_grads</td><td>0.1721</td></tr><tr><td>actor_loss</td><td>0.04325</td></tr><tr><td>actor_weights_norm</td><td>19.42466</td></tr><tr><td>critic_grads</td><td>1.33402</td></tr><tr><td>critic_loss</td><td>0.00493</td></tr><tr><td>critic_weights_norm</td><td>18.74509</td></tr><tr><td>dynamic_loss</td><td>0.01</td></tr><tr><td>prediction_loss</td><td>0.17509</td></tr><tr><td>representation_loss</td><td>0.01</td></tr><tr><td>sum_rewards</td><td>0.27083</td></tr><tr><td>worldModel_grads</td><td>16.37866</td></tr><tr><td>worldModel_loss</td><td>0.18109</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Trying non sparse rewards.</strong> at: <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/mhogkcon' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/runs/mhogkcon</a><br/> View job at <a href='https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMDQ0MzAzMQ==/version_details/v23' target=\"_blank\">https://wandb.ai/loyd-team/Dreamerv3%20Reproduction/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMDQ0MzAzMQ==/version_details/v23</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240120_160422-mhogkcon/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 16\n",
    "training_delta = 100 # number of episodes to train world model before training actor critic\n",
    "n_steps = batch_size + training_delta + 200\n",
    "gamma = 0.997\n",
    "Bpred = 1\n",
    "Bdyn = 0.5\n",
    "Brep = 0.1\n",
    "\n",
    "logging = True\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"Dreamerv3 Reproduction\",\n",
    "               name=\"Trying non sparse rewards.\",\n",
    "               reinit=True,\n",
    "               config={\"n_steps\": n_steps,\n",
    "                       \"gamma\": gamma,\n",
    "                       \"latent_size\": latent_size,\n",
    "                       \"num_categories\": num_categories,\n",
    "                       \"ht_size\": ht_size,\n",
    "                       \"reward_size\": reward_size,\n",
    "                       \"continue_size\": continue_size,\n",
    "                       \"worldModel_optimizer\": worldModel_optimizer,\n",
    "                       \"actor_optimizer\": actor_optimizer,\n",
    "                       \"critic_optimizer\": critic_optimizer})\n",
    "\n",
    "\n",
    "# Parameters\n",
    "buffer_size = 10000  # Adjust as needed\n",
    "alpha_world = 1  # Adjust as needed for prioritization (0 for uniform, 1 for fully prioritized)\n",
    "alpha_actor_critic = 1\n",
    "\n",
    "# Initialize buffer\n",
    "world_model_replay_buffer = PrioritizedReplayBuffer(alpha=alpha_world, storage=ListStorage(buffer_size), beta=0.4)\n",
    "actor_critics_replay_buffer = PrioritizedReplayBuffer(alpha=alpha_actor_critic, storage=ListStorage(buffer_size), beta=0.4)\n",
    "\n",
    "env_name = 'FrozenLake-v1'\n",
    "n_envs = batch_size//2\n",
    "\n",
    "\n",
    "# Reset environment and episode-specific variables\n",
    "envs = [gym.make(env_name) for _ in range(n_envs)]\n",
    "layouts = convert_layout_to_tensor([env.desc for env in envs])\n",
    "states = [env.reset()[0] for env in envs]\n",
    "episode_steps_count = [0] * n_envs\n",
    "dones = [False] * n_envs\n",
    "\n",
    "\n",
    "hidden_states = torch.zeros((1, n_envs, ht_size), device=device, dtype=torch.float)\n",
    "max_steps = n_states * n_actions\n",
    "\n",
    "for step in tqdm(range(n_steps)):\n",
    "    # --- Reset the environment if done ---\n",
    "    for i in range(n_envs):\n",
    "        episode_steps_count[i] += 1\n",
    "        if episode_steps_count[i] > max_steps or dones[i]:\n",
    "            states[i] = envs[i].reset()[0]\n",
    "            episode_steps_count[i] = 0\n",
    "            dones[i] = False\n",
    "            hidden_states[0][i] = torch.zeros((1, ht_size), device=device, dtype=torch.float)\n",
    "            \n",
    "\n",
    "    # --- Collect Experience ---\n",
    "    state_tensors = update_start_positions(layouts.clone(), states)\n",
    "    state_tensors = state_tensors.view(n_envs, -1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # Encode the state\n",
    "        encoded_states = worldModel.encode(state_tensors, hidden_states)\n",
    "\n",
    "    actions, rewards, next_states, dones = collect_experience(encoded_states, hidden_states, envs, n_envs)\n",
    "    \n",
    "    # Add experience to world model buffer\n",
    "    for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "        #print(f\"S: {state}, A: {action}, R: {reward}, S': {next_state}, D: {done}\")\n",
    "        world_model_replay_buffer.add((state, action.item(), reward, next_state, bool(done)))\n",
    "    \n",
    "    \n",
    "    # --- World Model Update ---\n",
    "    if len(world_model_replay_buffer) > batch_size:\n",
    "        update_world_model(worldModel, worldModel_optimizer, world_model_replay_buffer, actor, batch_size=batch_size, logging=logging)\n",
    "    \n",
    "    # --- Actor-Critic Update ---\n",
    "    if step > batch_size + training_delta:\n",
    "        for hidden_state, encoded_state, state_tensor, reward, done in zip(hidden_states[0], encoded_states, state_tensors, rewards, dones):\n",
    "            actor_critics_replay_buffer.add((hidden_state, encoded_state, state_tensor, reward, done))\n",
    "        if len(world_model_replay_buffer) > batch_size:\n",
    "            update_actor_critic(actor, critic, actor_optimizer, critic_optimizer, worldModel, actor_critics_replay_buffer, batch_size=batch_size, logging=logging)\n",
    "    \n",
    "    # --- Logging and State Updates ---\n",
    "    if logging:\n",
    "        wandb.log({\"sum_rewards\": sum(rewards)})\n",
    "    \n",
    "    states = next_states\n",
    "    integratedModel_input = torch.cat((encoded_states, F.one_hot(actions, n_actions).view(n_envs, -1).to(device)), dim=1)\n",
    "    integratedModel_input = integratedModel_input.view(n_envs, 1, -1)\n",
    "    hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensors, hidden_states)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "if logging:\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:10:36.924781Z",
     "start_time": "2024-01-20T15:04:22.053764Z"
    }
   },
   "id": "c61c1ee5397043e1",
   "execution_count": 209
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "num_episodes = 10\n",
    "successes = 0\n",
    "for _ in range(num_episodes):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "    autoencoder_losses = []\n",
    "    \n",
    "    max_steps = 100\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor.clone(), [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "        encoded_state_flat = encoded_state.view(1, -1)\n",
    "        \n",
    "        actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "        action_probs = actor(actor_state)[0]\n",
    "\n",
    "        action = torch.argmax(action_probs).item()\n",
    "        action_one_hot = F.one_hot(torch.tensor(action), n_actions).view(1, -1).to(device)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor.clone(), [next_state])\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        integratedModel_input = torch.cat((encoded_state_flat, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, state_tensor  ,hidden_state)\n",
    "        \n",
    "        state = next_state\n",
    "        max_steps -= 1\n",
    "\n",
    "    \n",
    "    if reward >= 1:\n",
    "        successes += 1\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/num_episodes}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:10:42.355082Z",
     "start_time": "2024-01-20T15:10:36.927733Z"
    }
   },
   "id": "6e49f3d1bbeb16c9",
   "execution_count": 210
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0\n",
      "Layout:\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.04,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.02, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.02, ct_hat: 1.04,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.01, ct_hat: 1.04,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 1\n",
      "Layout:\n",
      "[b'F', b'S', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.02, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.02, ct_hat: 1.10,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.03, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.03, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 2\n",
      "Layout:\n",
      "[b'F', b'F', b'S', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.07,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.04,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.04, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 3\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'S']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.05, ct_hat: 1.08,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.06, ct_hat: 1.06,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.05, ct_hat: 1.08,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 4\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'S', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.02, ct_hat: 1.10,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.02, ct_hat: 1.15,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'S', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.02, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.01, ct_hat: 1.09,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 5\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'S', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.11,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.03, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 6\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'S', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.10,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.02, ct_hat: 1.12,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.03, ct_hat: 1.03,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.02, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 7\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'S']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.04, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.11,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.04,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.03, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 8\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'S', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.04, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.04, ct_hat: 1.07,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'S', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.06, ct_hat: 1.08,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.05, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 9\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'S', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.02, ct_hat: 1.12,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.03, ct_hat: 1.03,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.02, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 10\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'S', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.04, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.04, ct_hat: 1.13,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.03, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 11\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'S']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.03, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.10,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.02,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.04, ct_hat: 1.04,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 12\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'S', b'F', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.04, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.12,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.03, ct_hat: 1.08,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 13\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'S', b'F', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.05, ct_hat: 0.96,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.05, ct_hat: 1.03,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.06, ct_hat: 0.91,\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.05, ct_hat: 0.96,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 14\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'S', b'G']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.08, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.08, ct_hat: 1.10,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.08, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.07, ct_hat: 1.06,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "State 15\n",
      "Layout:\n",
      "[b'F', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'S']\n",
      "World model predictions for action LEFT:\n",
      "rt_hat: 0.04, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action DOWN:\n",
      "rt_hat: 0.03, ct_hat: 1.12,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action RIGHT:\n",
      "rt_hat: 0.04, ct_hat: 1.05,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n",
      "World model predictions for action UP:\n",
      "rt_hat: 0.04, ct_hat: 1.07,\n",
      "[b'S', b'F', b'F', b'F']\n",
      "[b'F', b'H', b'F', b'H']\n",
      "[b'F', b'F', b'F', b'H']\n",
      "[b'H', b'F', b'F', b'G']\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def test_model_with_diagnostics(env, actor, critic, worldModel, device, n_actions, convert_layout_to_tensor, ht_size):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    action_space = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for state in range(n_states):\n",
    "            env.reset()\n",
    "            env.state = state\n",
    "            layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "            hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "\n",
    "            state_tensor = update_start_positions(layout_tensor.clone(), [state])\n",
    "            state_tensor = state_tensor.view(1, -1).to(device)\n",
    "            \n",
    "            print(f\"State {state}\")\n",
    "            print(f\"Layout:\")\n",
    "            layout = convert_tensor_to_layout(state_tensor.view(1, 4, 4, 4))\n",
    "            for line in layout[0]:\n",
    "                print(line)\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "            encoded_state_flat = encoded_state.view(1, -1)\n",
    "            \n",
    "            actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "            action_probs = actor(actor_state)[0]\n",
    "            #print(f\"Action probabilities : {np.round(action_probs.cpu().numpy(),2)}\")\n",
    "    \n",
    "            for action in range(n_actions):\n",
    "                action_one_hot = F.one_hot(torch.tensor(action), n_actions).view(1, -1).to(device)\n",
    "                integratedModel_input = torch.cat((encoded_state_flat, action_one_hot), dim=1)\n",
    "                # add batch because input needs to have batch, seq, input_size\n",
    "                integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "                hidden_state, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, state_tensor, hidden_state)\n",
    "                \n",
    "                layout = convert_tensor_to_layout(xt_hat.round().int().view(1, 4, 4, 4))\n",
    "                print(f\"World model predictions for action {action_space[action]}:\")\n",
    "                print(f\"rt_hat: {rt_hat.item():.2f}, ct_hat: {ct_hat.item():.2f},\")\n",
    "                for line in layout[0]:\n",
    "                    print(line)\n",
    "                #print(f\"Xt_hat: {xt_hat.round().int().view(1, 4, 4, 4)}\")\n",
    "                #print(f\"Zt_hat: {zt_hat.round().int()}\")\n",
    "                #print(f\"Zt: {zt.round().int()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Call the function\n",
    "test_model_with_diagnostics(env, actor, critic, worldModel, device, n_actions, convert_layout_to_tensor, ht_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:10:43.072991Z",
     "start_time": "2024-01-20T15:10:42.357390Z"
    }
   },
   "id": "41413356ea1b0b78",
   "execution_count": 211
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "print(env.desc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:10:43.075620Z",
     "start_time": "2024-01-20T15:10:43.072252Z"
    }
   },
   "id": "a087eb7c0efbafc2",
   "execution_count": 212
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T15:10:43.078052Z",
     "start_time": "2024-01-20T15:10:43.074562Z"
    }
   },
   "id": "71963b996e0527fd",
   "execution_count": 212
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
