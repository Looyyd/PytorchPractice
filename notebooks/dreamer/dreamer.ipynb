{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.113008Z",
     "start_time": "2024-01-16T15:44:25.092760Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchrl.data import PrioritizedReplayBuffer, ListStorage\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "FrozenLake environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f83b02f493c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.124039Z",
     "start_time": "2024-01-16T15:44:25.114342Z"
    }
   },
   "id": "f6f1fa210c2ad0e",
   "execution_count": 475
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3,\n",
    "                     'F': 0, 'H': 1, 'S': 0, 'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.141425Z",
     "start_time": "2024-01-16T15:44:25.126433Z"
    }
   },
   "id": "44567364fcc9fa67",
   "execution_count": 476
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n",
      "tensor([[[[0., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = convert_layout_to_tensor(generate_random_map(4))\n",
    "print(tensor)\n",
    "updated_tensor = update_start_positions(tensor, [[0]])\n",
    "print(updated_tensor) # dimensions: batch_size x nrows x ncols x num_statuses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.147641Z",
     "start_time": "2024-01-16T15:44:25.129940Z"
    }
   },
   "id": "26914cb622923df9",
   "execution_count": 477
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of DeamerV3 model\n",
    "![DeamerV3 model](model_names.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd95da8c410c3ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sequence model that learns the dynamics of the latent space\n",
    "# takes as input h t-1, z t-1, a t-1 and outputs ht\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.160728Z",
     "start_time": "2024-01-16T15:44:25.135120Z"
    }
   },
   "id": "d35de25f9599dd11",
   "execution_count": 478
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_size, xt_size, gru_hidden_size, num_layers, zt_dim, num_categories, reward_size, continue_size):\n",
    "        super(WorldModel, self).__init__()\n",
    "\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, gru_hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "        # Dynamics predictor\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, zt_dim * num_categories)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, reward_size)\n",
    "        )\n",
    "        \n",
    "        # Continue signal predictor\n",
    "        self.continue_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, continue_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder predictor\n",
    "        self.decoder_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, xt_size)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + xt_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, zt_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, ztat, xt, ht_1):\n",
    "        batch_size = ztat.size(0)\n",
    "        if ht_1 is None:\n",
    "            # Initialize hidden state with zeros if not provided\n",
    "            ht_1 = torch.zeros(self.num_layers, ztat.size(0), self.gru_hidden_size).to(xt.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        _ , ht = self.gru(ztat, ht_1)\n",
    "       \n",
    "        zt = self.encode(xt, ht)\n",
    "        \n",
    "        zt_hat = self.dynamics_predictor(ht)\n",
    "\n",
    "        htzt = torch.cat((ht.view(batch_size, -1), zt.view(batch_size, -1)), dim=1)\n",
    "        rt_hat = self.reward_predictor(htzt)\n",
    "        ct_hat = self.continue_predictor(htzt)\n",
    "        xt_hat = self.decoder_predictor(htzt)\n",
    "    \n",
    "        return ht, zt_hat, rt_hat, ct_hat, xt_hat, zt\n",
    "    \n",
    "    def encode(self, xt, ht, temperature=1.0):\n",
    "        # Encode input and apply Gumbel Softmax\n",
    "        # ht has is 1,1,32 need to be 1,32\n",
    "        ht = ht.view(-1, self.gru_hidden_size)\n",
    "        encoded = self.encoder(torch.cat((xt, ht), dim=1))\n",
    "        #encoded = encoded.view(-1, self.num_categories)\n",
    "        softmax = nn.functional.gumbel_softmax(encoded, tau=temperature, hard=True)\n",
    "        return softmax\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.162547Z",
     "start_time": "2024-01-16T15:44:25.142730Z"
    }
   },
   "id": "cc7ed508cc0c5986",
   "execution_count": 479
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\"Symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Inverse of symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "class SymlogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymlogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Apply symlog transformation to the target\n",
    "\n",
    "        transformed_target = symlog(target)\n",
    "        # Compute MSE Loss between input and transformed target\n",
    "        return F.mse_loss(input, transformed_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.173719Z",
     "start_time": "2024-01-16T15:44:25.164008Z"
    }
   },
   "id": "c75090797e8abeae",
   "execution_count": 480
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.180328Z",
     "start_time": "2024-01-16T15:44:25.167261Z"
    }
   },
   "id": "7f334ff705d60e11",
   "execution_count": 481
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640b615c97d3cb37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"mps\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.180616Z",
     "start_time": "2024-01-16T15:44:25.170820Z"
    }
   },
   "id": "21168a3b14a069b2",
   "execution_count": 482
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "latent_size = 32 # aka zt size\n",
    "num_categories = 4 # number of categories in latent space\n",
    "\n",
    "ht_size = 32\n",
    "xt_size = 4*4*4\n",
    "reward_size = 1\n",
    "continue_size = 1\n",
    "worldModel = WorldModel(latent_size * num_categories + n_actions, xt_size, ht_size, 1, latent_size, num_categories, reward_size, continue_size).to(device)\n",
    "worldModel_optimizer = optim.Adam(worldModel.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "actor = Actor(ht_size + latent_size*num_categories, n_actions).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-5, eps=1e-5)\n",
    "critic = Critic(ht_size).to(device)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-5, eps=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.197532Z",
     "start_time": "2024-01-16T15:44:25.176785Z"
    }
   },
   "id": "6186b465c849ee41",
   "execution_count": 483
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_world_model(worldModel, replay_buffer, batch_size=32, steps=3):\n",
    "    loss_fn = SymlogLoss()\n",
    "    experiences, info = replay_buffer.sample(batch_size, return_info=True)\n",
    "    # Unpack experiences\n",
    "\n",
    "    states = experiences[0]\n",
    "\n",
    "    \n",
    "    # get envs from states\n",
    "    envs = []\n",
    "    for state in states:\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        env.reset()\n",
    "        env.state = state\n",
    "        envs.append(env)\n",
    "    \n",
    "    # hidden states are not batch first\n",
    "    hidden_states = torch.zeros((1, batch_size, ht_size), device=device, dtype=torch.float)\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc for env in envs]).to(device)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # step through envs\n",
    "        #TODO: maybe not random steps\n",
    "        actions = torch.tensor([env.action_space.sample() for env in envs], device=device)\n",
    "        next_states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for env, action in zip(envs, actions):\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            next_states.append(next_state)\n",
    "        \n",
    "        dones = torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        \n",
    "        next_states_tensor = update_start_positions(layout_tensor, next_states).view(batch_size, -1).to(device)    \n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            states_tensor = update_start_positions(layout_tensor, states)\n",
    "            states_tensor = states_tensor.view(batch_size, -1).to(device)\n",
    "            \n",
    "\n",
    "            zts = worldModel.encode(states_tensor, hidden_states)\n",
    "    \n",
    "        # Integrated model loss\n",
    "        action_one_hot = F.one_hot(actions.detach(), n_actions).view(batch_size, -1).to(device)\n",
    "    \n",
    "\n",
    "        integratedModel_input = torch.cat((zts, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(batch_size, 1, -1)\n",
    "        \n",
    "\n",
    "        ht, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, states_tensor, hidden_states)\n",
    "        xt = next_states_tensor\n",
    "        rt = rewards.view(batch_size, 1)\n",
    "        ct = (1 - dones).view(batch_size, 1)\n",
    "        prediction_loss = (loss_fn(xt_hat, xt) \n",
    "                                + loss_fn(rt_hat, rt) \n",
    "                                + loss_fn(ct_hat, ct))\n",
    "        \n",
    "        zt = zt.view(batch_size, 1, -1)\n",
    "        \n",
    "        threshold = 1\n",
    "        dynamic_loss = torch.max(torch.tensor(threshold), kl_loss(zt_hat, zt.detach()))\n",
    "        representation_loss = torch.max(torch.tensor(threshold), kl_loss(zt, zt_hat.detach()))\n",
    "        \n",
    "        worldModel_loss = Bpred * prediction_loss + Bdyn * dynamic_loss + Brep * representation_loss\n",
    "        \n",
    "        worldModel_optimizer.zero_grad()\n",
    "        worldModel_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worldModel.parameters(), 1000)\n",
    "        worldModel_optimizer.step()\n",
    "        \n",
    "        states = next_states\n",
    "        hidden_states = ht.detach()\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:44:25.204067Z",
     "start_time": "2024-01-16T15:44:25.201045Z"
    }
   },
   "id": "965aa3f2e9e5d8fd",
   "execution_count": 484
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:45<1:34:05,  5.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[485], line 72\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# Check if buffer is ready for sampling\u001B[39;00m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(replay_buffer) \u001B[38;5;241m>\u001B[39m batch_size:\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# Sample a batch of experiences from the buffer\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m     update_world_model(worldModel, replay_buffer, batch_size)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m#TODO: add buffer to this\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m#Trying actor critic training after world model has 100 episodes of training\u001B[39;00m\n\u001B[1;32m     76\u001B[0m critic_grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "Cell \u001B[0;32mIn[484], line 29\u001B[0m, in \u001B[0;36mupdate_world_model\u001B[0;34m(worldModel, replay_buffer, batch_size, steps)\u001B[0m\n\u001B[1;32m     27\u001B[0m dones \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m env, action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(envs, actions):\n\u001B[0;32m---> 29\u001B[0m     next_state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     30\u001B[0m     rewards\u001B[38;5;241m.\u001B[39mappend(reward)\n\u001B[1;32m     31\u001B[0m     dones\u001B[38;5;241m.\u001B[39mappend(done)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.997\n",
    "Bpred = 1\n",
    "Bdyn = 0.5\n",
    "Brep = 0.1\n",
    "loss_fn = SymlogLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"Dreamerv3 Reproduction\",\n",
    "               name=\"Zt in actor state\",\n",
    "               reinit=True,\n",
    "               config={\"num_episodes\": num_episodes,\n",
    "                       \"gamma\": gamma,\n",
    "                       \"latent_size\": latent_size,\n",
    "                       \"num_categories\": num_categories,\n",
    "                       \"ht_size\": ht_size,\n",
    "                       \"reward_size\": reward_size,\n",
    "                       \"continue_size\": continue_size,\n",
    "                       \"worldModel_optimizer\": worldModel_optimizer,\n",
    "                       \"actor_optimizer\": actor_optimizer,\n",
    "                       \"critic_optimizer\": critic_optimizer})\n",
    "\n",
    "\n",
    "previous_episode_reward = 0\n",
    "# Parameters\n",
    "buffer_size = 10000  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "alpha = 0.6  # Adjust as needed for prioritization (0 for uniform, 1 for fully prioritized)\n",
    "\n",
    "# Initialize buffer\n",
    "replay_buffer = PrioritizedReplayBuffer(alpha=alpha, storage=ListStorage(buffer_size), beta=0.4)\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc]).to(device)\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    max_steps = 100\n",
    "\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)# could keep it from last output.\n",
    "        encoded_state_flat = encoded_state.view(1, -1) \n",
    "\n",
    "        \n",
    "        hidden_state = hidden_state.to(device)\n",
    "        actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "\n",
    "        action_probs = actor(actor_state)[0] # remove batch dimension\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state]).view(1, -1).to(device)\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        # Store the experience in the buffer\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        replay_buffer.add(experience)\n",
    "\n",
    "        # Check if buffer is ready for sampling\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            # Sample a batch of experiences from the buffer\n",
    "            update_world_model(worldModel, replay_buffer, batch_size)\n",
    "        \n",
    "        #TODO: add buffer to this\n",
    "        #Trying actor critic training after world model has 100 episodes of training\n",
    "        critic_grads = 0\n",
    "        actor_grads = 0\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        if episode > 100:\n",
    "            # Critic loss\n",
    "            imagination_horizon = 3 # TODO: 15 in paper\n",
    "            return_lambda = 0.95\n",
    "            lookahead_reward = reward + (gamma * (1 - return_lambda) * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "            with torch.no_grad():\n",
    "                imagined_done = done\n",
    "                imagined_ht = next_hidden_state\n",
    "                imagined_zt = worldModel.encode(next_state_tensor, imagined_ht)\n",
    "                imagined_xt = next_state_tensor\n",
    "                for t in range(1, imagination_horizon+1):\n",
    "                    if imagined_done:\n",
    "                        break\n",
    "                    \n",
    "                    im_actor_state = torch.cat((imagined_ht.view(1, -1), imagined_zt.view(1,-1)), dim=1)\n",
    "                    imagined_action_probs = actor(im_actor_state)[0]\n",
    "                    imagined_action = torch.multinomial(imagined_action_probs, 1).item()\n",
    "                    \n",
    "                    im_action_one_hot = F.one_hot(torch.tensor(imagined_action), n_actions).view(1, -1).to(device)\n",
    "    \n",
    "                    imagined_integratedModel_input = torch.cat((imagined_zt.view(1, -1), im_action_one_hot.detach()), dim=1)\n",
    "                    # add batch because input needs to have batch, seq, input_size\n",
    "                    imagined_integratedModel_input = imagined_integratedModel_input.view(1, 1, -1)\n",
    "                    \n",
    "                    # Now, use the flattened input for your integrated model\n",
    "                    im_next_hidden_state, im_zt_hat, im_rt_hat, im_ct_hat, im_xt_hat , _ = worldModel(imagined_integratedModel_input, imagined_xt, imagined_ht)\n",
    "                    imagined_reward = (im_rt_hat + im_ct_hat*(critic(im_next_hidden_state.detach())))\n",
    "                    lookahead_reward +=  (gamma ** t) * (return_lambda ** (t-1)) * imagined_reward\n",
    "                    \n",
    "                    imagined_done = imagined_done or im_ct_hat > 0.5\n",
    "                    imagined_ht = im_next_hidden_state\n",
    "                    imagined_zt = im_zt_hat\n",
    "                    imagined_xt = im_xt_hat\n",
    "                # one last critic estimation\n",
    "                if not imagined_done:\n",
    "                    lookahead_reward += (gamma ** imagination_horizon+1) * (return_lambda ** imagination_horizon) * critic(im_next_hidden_state.detach())\n",
    "            \n",
    "            #target_value = reward + (gamma * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "            target_value = lookahead_reward\n",
    "            value = critic(hidden_state)\n",
    "            critic_loss = F.mse_loss(value, target_value.detach())\n",
    "    \n",
    "            # Actor loss\n",
    "            advantage = target_value - value\n",
    "            actor_loss = -torch.log(action_probs.squeeze(0)[action] + 1e-8) * advantage.detach()\n",
    "        \n",
    "            # Backpropagation\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), 100)\n",
    "            critic_optimizer.step()\n",
    "    \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), 100)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # get gradients norms\n",
    "            critic_grads = sum([torch.norm(param.grad) for param in critic.parameters()])\n",
    "            actor_grads = sum([torch.norm(param.grad) for param in actor.parameters()])\n",
    "    \n",
    "\n",
    "        \n",
    "        if logging and episode%10 == 0:\n",
    "            worldModel_grads = sum([torch.norm(param.grad) for param in worldModel.parameters()])\n",
    "        \n",
    "            # get weights norms\n",
    "            critic_weights_norm = sum([torch.norm(param) for param in critic.parameters()])\n",
    "            actor_weights_norm = sum([torch.norm(param) for param in actor.parameters()])\n",
    "            worldModel_weights_norm = sum([torch.norm(param) for param in worldModel.parameters()])\n",
    "\n",
    "            wandb.log({\"critic_loss\": critic_loss.item(),\n",
    "                        \"critic_grads\": critic_grads,\n",
    "                        \"critic_weights_norm\": critic_weights_norm,\n",
    "                        \"actor_loss\": actor_loss.item(),\n",
    "                        \"actor_grads\": actor_grads,\n",
    "                        \"actor_weights_norm\": actor_weights_norm,\n",
    "                        \"worldModel_loss\": worldModel_loss.item(),\n",
    "                        \"worldModel_grads\": worldModel_grads,\n",
    "                        \"representation_loss\": representation_loss.item(),\n",
    "                        \"dynamic_loss\": dynamic_loss.item(),\n",
    "                        \"prediction_loss\": prediction_loss.item(),\n",
    "                        \"worldModel_weights_norm\": worldModel_weights_norm,\n",
    "                       \"previous_episode_reward\": previous_episode_reward})\n",
    "        \n",
    "        state = next_state\n",
    "        hidden_state = next_hidden_state.detach()\n",
    "        max_steps -= 1\n",
    "    \n",
    "    previous_episode_reward = sum(episode_rewards)\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "if logging:\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T15:45:10.774564Z",
     "start_time": "2024-01-16T15:44:25.214097Z"
    }
   },
   "id": "c61c1ee5397043e1",
   "execution_count": 485
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the model\n",
    "num_episodes = 100\n",
    "successes = 0\n",
    "for _ in range(num_episodes):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "    autoencoder_losses = []\n",
    "    \n",
    "    max_steps = 100\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "        encoded_state_flat = encoded_state.view(1, -1)\n",
    "        \n",
    "        actor_state = torch.cat((hidden_state, encoded_state_flat), dim=1)\n",
    "        action_probs = actor(actor_state)[0]\n",
    "        value = critic(hidden_state)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state])\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        integratedModel_input = torch.cat((encoded_state_flat, action_probs.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensor  ,hidden_state)\n",
    "        \n",
    "        state = next_state\n",
    "        max_steps -= 1\n",
    "\n",
    "    \n",
    "    if reward == 1:\n",
    "        successes += 1\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/num_episodes}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T15:45:10.775789Z"
    }
   },
   "id": "6e49f3d1bbeb16c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T15:45:10.776682Z"
    }
   },
   "id": "41413356ea1b0b78",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
