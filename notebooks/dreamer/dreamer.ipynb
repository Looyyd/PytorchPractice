{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.440829Z",
     "start_time": "2024-01-16T14:02:34.429176Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorchrl.data import PrioritizedReplayBuffer\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "FrozenLake environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f83b02f493c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.465246Z",
     "start_time": "2024-01-16T14:02:34.441380Z"
    }
   },
   "id": "f6f1fa210c2ad0e",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3,\n",
    "                     'F': 0, 'H': 1, 'S': 0, 'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.474809Z",
     "start_time": "2024-01-16T14:02:34.468301Z"
    }
   },
   "id": "44567364fcc9fa67",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n",
      "tensor([[[[0., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = convert_layout_to_tensor(generate_random_map(4))\n",
    "print(tensor)\n",
    "updated_tensor = update_start_positions(tensor, [[0]])\n",
    "print(updated_tensor) # dimensions: batch_size x nrows x ncols x num_statuses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.485636Z",
     "start_time": "2024-01-16T14:02:34.476950Z"
    }
   },
   "id": "26914cb622923df9",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of DeamerV3 model\n",
    "![DeamerV3 model](model_names.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd95da8c410c3ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sequence model that learns the dynamics of the latent space\n",
    "# takes as input h t-1, z t-1, a t-1 and outputs ht\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.487465Z",
     "start_time": "2024-01-16T14:02:34.482713Z"
    }
   },
   "id": "d35de25f9599dd11",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_size, xt_size, gru_hidden_size, num_layers, zt_dim, num_categories, reward_size, continue_size):\n",
    "        super(WorldModel, self).__init__()\n",
    "\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, gru_hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "        # Dynamics predictor\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, zt_dim * num_categories)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, reward_size)\n",
    "        )\n",
    "        \n",
    "        # Continue signal predictor\n",
    "        self.continue_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, continue_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder predictor\n",
    "        self.decoder_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, xt_size)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + xt_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, zt_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, ztat, xt, ht_1):\n",
    "        if ht_1 is None:\n",
    "            # Initialize hidden state with zeros if not provided\n",
    "            ht_1 = torch.zeros(self.num_layers, ztat.size(0), self.gru_hidden_size).to(xt.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        _ , ht = self.gru(ztat, ht_1)\n",
    "       \n",
    "        zt = self.encode(xt, ht)\n",
    "        \n",
    "        zt_hat = self.dynamics_predictor(ht)\n",
    "\n",
    "        htzt = torch.cat((ht.view(1, -1), zt.view(1, -1)), dim=1)\n",
    "        rt_hat = self.reward_predictor(htzt)\n",
    "        ct_hat = self.continue_predictor(htzt)\n",
    "        xt_hat = self.decoder_predictor(htzt)\n",
    "    \n",
    "        return ht, zt_hat, rt_hat, ct_hat, xt_hat, zt\n",
    "    \n",
    "    def encode(self, xt, ht, temperature=1.0):\n",
    "        # Encode input and apply Gumbel Softmax\n",
    "        # ht has is 1,1,32 need to be 1,32\n",
    "        ht = ht.view(-1, self.gru_hidden_size)\n",
    "        encoded = self.encoder(torch.cat((xt, ht), dim=1))\n",
    "        encoded = encoded.view(-1, self.num_categories)\n",
    "        softmax = nn.functional.gumbel_softmax(encoded, tau=temperature, hard=True)\n",
    "        return softmax\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.502781Z",
     "start_time": "2024-01-16T14:02:34.487057Z"
    }
   },
   "id": "cc7ed508cc0c5986",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\"Symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Inverse of symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "class SymlogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymlogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Apply symlog transformation to the target\n",
    "        transformed_target = symlog(target)\n",
    "        # Compute MSE Loss between input and transformed target\n",
    "        return F.mse_loss(input, transformed_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.503107Z",
     "start_time": "2024-01-16T14:02:34.494646Z"
    }
   },
   "id": "c75090797e8abeae",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.503492Z",
     "start_time": "2024-01-16T14:02:34.498808Z"
    }
   },
   "id": "7f334ff705d60e11",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640b615c97d3cb37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"mps\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.505157Z",
     "start_time": "2024-01-16T14:02:34.502285Z"
    }
   },
   "id": "21168a3b14a069b2",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "latent_size = 32 # aka zt size\n",
    "num_categories = 4 # number of categories in latent space\n",
    "\n",
    "ht_size = 32\n",
    "xt_size = 4*4*4\n",
    "reward_size = 1\n",
    "continue_size = 1\n",
    "worldModel = WorldModel(latent_size * num_categories + n_actions, xt_size, ht_size, 1, latent_size, num_categories, reward_size, continue_size).to(device)\n",
    "worldModel_optimizer = optim.Adam(worldModel.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "actor = Actor(ht_size + latent_size*num_categories, n_actions).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-5, eps=1e-5)\n",
    "critic = Critic(ht_size).to(device)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-5, eps=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:34.577100Z",
     "start_time": "2024-01-16T14:02:34.505853Z"
    }
   },
   "id": "6186b465c849ee41",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:05<04:39,  3.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 87\u001B[0m\n\u001B[1;32m     85\u001B[0m worldModel_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     86\u001B[0m worldModel_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 87\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(worldModel\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m     88\u001B[0m worldModel_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m#Trying actor critic training after world model has 100 episodes of training\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:82\u001B[0m, in \u001B[0;36mclip_grad_norm_\u001B[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001B[0m\n\u001B[1;32m     80\u001B[0m         clip_coef_clamped_device \u001B[38;5;241m=\u001B[39m clip_coef_clamped\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m grads:\n\u001B[0;32m---> 82\u001B[0m             g\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mmul_(clip_coef_clamped_device)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_norm\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.997\n",
    "Bpred = 1\n",
    "Bdyn = 0.5\n",
    "Brep = 0.1\n",
    "loss_fn = SymlogLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"Dreamerv3 Reproduction\",\n",
    "               name=\"Zt in actor state\",\n",
    "               reinit=True,\n",
    "               config={\"num_episodes\": num_episodes,\n",
    "                       \"gamma\": gamma,\n",
    "                       \"latent_size\": latent_size,\n",
    "                       \"num_categories\": num_categories,\n",
    "                       \"ht_size\": ht_size,\n",
    "                       \"reward_size\": reward_size,\n",
    "                       \"continue_size\": continue_size,\n",
    "                       \"worldModel_optimizer\": worldModel_optimizer,\n",
    "                       \"actor_optimizer\": actor_optimizer,\n",
    "                       \"critic_optimizer\": critic_optimizer})\n",
    "\n",
    "\n",
    "previous_episode_reward = 0\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc]).to(device)\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    max_steps = 100\n",
    "\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)# could keep it from last output.\n",
    "        encoded_state_flat = encoded_state.view(1, -1) \n",
    "\n",
    "        \n",
    "        hidden_state = hidden_state.to(device)\n",
    "        actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "\n",
    "        action_probs = actor(actor_state)[0] # remove batch dimension\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state]).view(1, -1).to(device)\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "\n",
    "        action_probs = action_probs.view(1, -1)\n",
    "        \n",
    "        # Integrated model loss\n",
    "        action_one_hot = F.one_hot(torch.tensor(action), n_actions).view(1, -1).to(device)\n",
    "\n",
    "        integratedModel_input = torch.cat((encoded_state_flat, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        \n",
    "        # Now, use the flattened input for your integrated model\n",
    "        next_hidden_state, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, state_tensor, hidden_state)\n",
    "        xt = next_state_tensor.view(1, -1)\n",
    "        rt = torch.tensor(reward, device=device, dtype=torch.float).view(1, -1)\n",
    "        ct = torch.tensor(1 - int(done), device=device, dtype=torch.float).view(1, -1)\n",
    "        prediction_loss = (loss_fn(xt_hat, xt) \n",
    "                                + loss_fn(rt_hat, rt) \n",
    "                                + loss_fn(ct_hat, ct))\n",
    "        \n",
    "        zt = zt.view(1, 1, -1)\n",
    "        \n",
    "        threshold = 1\n",
    "        dynamic_loss = torch.max(torch.tensor(threshold), kl_loss(zt_hat, zt.detach()))\n",
    "        representation_loss = torch.max(torch.tensor(threshold), kl_loss(zt, zt_hat.detach()))\n",
    "        \n",
    "        worldModel_loss = Bpred * prediction_loss + Bdyn * dynamic_loss + Brep * representation_loss\n",
    "        \n",
    "        worldModel_optimizer.zero_grad()\n",
    "        worldModel_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worldModel.parameters(), 1000)\n",
    "        worldModel_optimizer.step()\n",
    "        \n",
    "        #Trying actor critic training after world model has 100 episodes of training\n",
    "        critic_grads = 0\n",
    "        actor_grads = 0\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        if episode > 100:\n",
    "            # Critic loss\n",
    "            imagination_horizon = 3 # TODO: 15 in paper\n",
    "            return_lambda = 0.95\n",
    "            lookahead_reward = reward + (gamma * (1 - return_lambda) * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "            with torch.no_grad():\n",
    "                imagined_done = done\n",
    "                imagined_ht = next_hidden_state\n",
    "                imagined_zt = worldModel.encode(next_state_tensor, imagined_ht)\n",
    "                imagined_xt = next_state_tensor\n",
    "                for t in range(1, imagination_horizon+1):\n",
    "                    if imagined_done:\n",
    "                        break\n",
    "                    \n",
    "                    im_actor_state = torch.cat((imagined_ht.view(1, -1), imagined_zt.view(1,-1)), dim=1)\n",
    "                    imagined_action_probs = actor(im_actor_state)[0]\n",
    "                    imagined_action = torch.multinomial(action_probs, 1).item()\n",
    "                    \n",
    "                    im_action_one_hot = F.one_hot(torch.tensor(imagined_action), n_actions).view(1, -1).to(device)\n",
    "    \n",
    "                    imagined_integratedModel_input = torch.cat((imagined_zt.view(1, -1), im_action_one_hot.detach()), dim=1)\n",
    "                    # add batch because input needs to have batch, seq, input_size\n",
    "                    imagined_integratedModel_input = imagined_integratedModel_input.view(1, 1, -1)\n",
    "                    \n",
    "                    # Now, use the flattened input for your integrated model\n",
    "                    im_next_hidden_state, im_zt_hat, im_rt_hat, im_ct_hat, im_xt_hat , _ = worldModel(imagined_integratedModel_input, imagined_xt, imagined_ht)\n",
    "                    imagined_reward = (im_rt_hat + im_ct_hat*(critic(im_next_hidden_state.detach())))\n",
    "                    lookahead_reward +=  (gamma ** t) * (return_lambda ** (t-1)) * imagined_reward\n",
    "                    \n",
    "                    imagined_done = imagined_done or im_ct_hat > 0.5\n",
    "                    imagined_ht = im_next_hidden_state\n",
    "                    imagined_zt = im_zt_hat\n",
    "                    imagined_xt = im_xt_hat\n",
    "                # one last critic estimation\n",
    "                if not imagined_done:\n",
    "                    lookahead_reward += (gamma ** imagination_horizon+1) * (return_lambda ** imagination_horizon) * critic(im_next_hidden_state.detach())\n",
    "            \n",
    "            #target_value = reward + (gamma * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "            target_value = lookahead_reward\n",
    "            value = critic(hidden_state)\n",
    "            critic_loss = F.mse_loss(value, target_value.detach())\n",
    "    \n",
    "            # Actor loss\n",
    "            advantage = target_value - value\n",
    "            actor_loss = -torch.log(action_probs.squeeze(0)[action] + 1e-8) * advantage.detach()\n",
    "        \n",
    "            # Backpropagation\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic.parameters(), 100)\n",
    "            critic_optimizer.step()\n",
    "    \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor.parameters(), 100)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # get gradients norms\n",
    "            critic_grads = sum([torch.norm(param.grad) for param in critic.parameters()])\n",
    "            actor_grads = sum([torch.norm(param.grad) for param in actor.parameters()])\n",
    "    \n",
    "\n",
    "        \n",
    "        if logging and episode%10 == 0:\n",
    "            worldModel_grads = sum([torch.norm(param.grad) for param in worldModel.parameters()])\n",
    "        \n",
    "            # get weights norms\n",
    "            critic_weights_norm = sum([torch.norm(param) for param in critic.parameters()])\n",
    "            actor_weights_norm = sum([torch.norm(param) for param in actor.parameters()])\n",
    "            worldModel_weights_norm = sum([torch.norm(param) for param in worldModel.parameters()])\n",
    "\n",
    "            wandb.log({\"critic_loss\": critic_loss.item(),\n",
    "                        \"critic_grads\": critic_grads,\n",
    "                        \"critic_weights_norm\": critic_weights_norm,\n",
    "                        \"actor_loss\": actor_loss.item(),\n",
    "                        \"actor_grads\": actor_grads,\n",
    "                        \"actor_weights_norm\": actor_weights_norm,\n",
    "                        \"worldModel_loss\": worldModel_loss.item(),\n",
    "                        \"worldModel_grads\": worldModel_grads,\n",
    "                        \"representation_loss\": representation_loss.item(),\n",
    "                        \"dynamic_loss\": dynamic_loss.item(),\n",
    "                        \"prediction_loss\": prediction_loss.item(),\n",
    "                        \"worldModel_weights_norm\": worldModel_weights_norm,\n",
    "                       \"previous_episode_reward\": previous_episode_reward})\n",
    "        \n",
    "        state = next_state\n",
    "        hidden_state = next_hidden_state.detach()\n",
    "        max_steps -= 1\n",
    "    \n",
    "    previous_episode_reward = sum(episode_rewards)\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "if logging:\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:40.760847Z",
     "start_time": "2024-01-16T14:02:34.592628Z"
    }
   },
   "id": "c61c1ee5397043e1",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the model\n",
    "num_episodes = 100\n",
    "successes = 0\n",
    "for _ in range(num_episodes):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "    autoencoder_losses = []\n",
    "    \n",
    "    max_steps = 100\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "        encoded_state_flat = encoded_state.view(1, -1)\n",
    "        \n",
    "        actor_state = torch.cat((hidden_state, encoded_state_flat), dim=1)\n",
    "        action_probs = actor(actor_state)[0]\n",
    "        value = critic(hidden_state)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state])\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        integratedModel_input = torch.cat((encoded_state_flat, action_probs.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensor  ,hidden_state)\n",
    "        \n",
    "        state = next_state\n",
    "        max_steps -= 1\n",
    "\n",
    "    \n",
    "    if reward == 1:\n",
    "        successes += 1\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/num_episodes}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T14:02:40.763106Z",
     "start_time": "2024-01-16T14:02:40.761393Z"
    }
   },
   "id": "6e49f3d1bbeb16c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T14:02:40.762673Z"
    }
   },
   "id": "41413356ea1b0b78",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
