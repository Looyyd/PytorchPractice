{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.525737Z",
     "start_time": "2024-01-16T11:16:32.480828Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "FrozenLake environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f83b02f493c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.536231Z",
     "start_time": "2024-01-16T11:16:32.496717Z"
    }
   },
   "id": "f6f1fa210c2ad0e",
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3,\n",
    "                     'F': 0, 'H': 1, 'S': 0, 'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.542880Z",
     "start_time": "2024-01-16T11:16:32.540127Z"
    }
   },
   "id": "44567364fcc9fa67",
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n",
      "tensor([[[[0., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = convert_layout_to_tensor(generate_random_map(4))\n",
    "print(tensor)\n",
    "updated_tensor = update_start_positions(tensor, [[0]])\n",
    "print(updated_tensor) # dimensions: batch_size x nrows x ncols x num_statuses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.553069Z",
     "start_time": "2024-01-16T11:16:32.542650Z"
    }
   },
   "id": "26914cb622923df9",
   "execution_count": 196
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of DeamerV3 model\n",
    "![DeamerV3 model](model_names.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd95da8c410c3ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Autoencoder that learns the latent space of the data\n",
    "class CategoricalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim, num_categories):\n",
    "        super(CategoricalAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_categories = num_categories\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, latent_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim * num_categories, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, temperature=1.0):\n",
    "        # Encode input and apply Gumbel Softmax\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.view(-1, self.num_categories)\n",
    "        softmax = nn.functional.gumbel_softmax(encoded, tau=temperature, hard=True)\n",
    "        return softmax\n",
    "\n",
    "    def forward(self, x, temperature=1.0):\n",
    "        batch_size = x.size(0)\n",
    "        softmax = self.encode(x, temperature)\n",
    "        softmax = softmax.view(batch_size, -1)\n",
    "\n",
    "        x = self.decoder(softmax)\n",
    "        return x, softmax\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.553425Z",
     "start_time": "2024-01-16T11:16:32.548441Z"
    }
   },
   "id": "19b3561be99fe27f",
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sequence model that learns the dynamics of the latent space\n",
    "# takes as input h t-1, z t-1, a t-1 and outputs ht\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.560768Z",
     "start_time": "2024-01-16T11:16:32.552017Z"
    }
   },
   "id": "d35de25f9599dd11",
   "execution_count": 198
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_size, xt_size, gru_hidden_size, num_layers, zt_dim, num_categories, reward_size, continue_size):\n",
    "        super(WorldModel, self).__init__()\n",
    "\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, gru_hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "        # Dynamics predictor\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, zt_dim * num_categories)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, reward_size)\n",
    "        )\n",
    "        \n",
    "        # Continue signal predictor\n",
    "        self.continue_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, continue_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder predictor\n",
    "        self.decoder_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, xt_size)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + xt_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, zt_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, ztat, xt, ht_1):\n",
    "        if ht_1 is None:\n",
    "            # Initialize hidden state with zeros if not provided\n",
    "            ht_1 = torch.zeros(self.num_layers, ztat.size(0), self.gru_hidden_size).to(xt.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        _ , ht = self.gru(ztat, ht_1)\n",
    "       \n",
    "        zt = self.encode(xt, ht)\n",
    "        \n",
    "        zt_hat = self.dynamics_predictor(ht)\n",
    "\n",
    "        htzt = torch.cat((ht.view(1, -1), zt.view(1, -1)), dim=1)\n",
    "        rt_hat = self.reward_predictor(htzt)\n",
    "        ct_hat = self.continue_predictor(htzt)\n",
    "        xt_hat = self.decoder_predictor(htzt)\n",
    "    \n",
    "        return ht, zt_hat, rt_hat, ct_hat, xt_hat, zt\n",
    "    \n",
    "    def encode(self, xt, ht, temperature=1.0):\n",
    "        # Encode input and apply Gumbel Softmax\n",
    "        # ht has is 1,1,32 need to be 1,32\n",
    "        ht = ht.view(-1, self.gru_hidden_size)\n",
    "        encoded = self.encoder(torch.cat((xt, ht), dim=1))\n",
    "        encoded = encoded.view(-1, self.num_categories)\n",
    "        softmax = nn.functional.gumbel_softmax(encoded, tau=temperature, hard=True)\n",
    "        return softmax\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.564721Z",
     "start_time": "2024-01-16T11:16:32.558051Z"
    }
   },
   "id": "cc7ed508cc0c5986",
   "execution_count": 199
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\"Symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Inverse of symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "class SymlogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymlogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Apply symlog transformation to the target\n",
    "        transformed_target = symlog(target)\n",
    "        # Compute MSE Loss between input and transformed target\n",
    "        return F.mse_loss(input, transformed_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.577026Z",
     "start_time": "2024-01-16T11:16:32.561729Z"
    }
   },
   "id": "c75090797e8abeae",
   "execution_count": 200
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.577699Z",
     "start_time": "2024-01-16T11:16:32.565812Z"
    }
   },
   "id": "7f334ff705d60e11",
   "execution_count": 201
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640b615c97d3cb37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"mps\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.591496Z",
     "start_time": "2024-01-16T11:16:32.578252Z"
    }
   },
   "id": "21168a3b14a069b2",
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "latent_size = 32 # aka zt size\n",
    "num_categories = 4 # number of categories in latent space\n",
    "#autoencoder = CategoricalAutoencoder(4*4*4, latent_size, num_categories).to(device)\n",
    "#autoencoder_optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "ht_size = 32\n",
    "xt_size = 4*4*4\n",
    "reward_size = 1\n",
    "continue_size = 1\n",
    "worldModel = WorldModel(latent_size * num_categories + n_actions, xt_size, ht_size, 1, latent_size, num_categories, reward_size, continue_size).to(device)\n",
    "worldModel_optimizer = optim.Adam(worldModel.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "actor = Actor(ht_size, n_actions).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-5, eps=1e-5)\n",
    "critic = Critic(ht_size).to(device)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-5, eps=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T11:16:32.606294Z",
     "start_time": "2024-01-16T11:16:32.581804Z"
    }
   },
   "id": "6186b465c849ee41",
   "execution_count": 203
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 62/1000 [00:29<10:51,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.997\n",
    "Bpred = 1\n",
    "Bdyn = 0.5\n",
    "Brep = 0.1\n",
    "loss_fn = SymlogLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"Dreamerv3 Reproduction\",\n",
    "               name=\"Refactor to follow paper model definitions and losses\",\n",
    "               reinit=True,\n",
    "               config={\"num_episodes\": num_episodes,\n",
    "                       \"gamma\": gamma,\n",
    "                       \"latent_size\": latent_size,\n",
    "                       \"num_categories\": num_categories,\n",
    "                       \"ht_size\": ht_size,\n",
    "                       \"reward_size\": reward_size,\n",
    "                       \"continue_size\": continue_size,\n",
    "                       \"worldModel_optimizer\": worldModel_optimizer,\n",
    "                       \"actor_optimizer\": actor_optimizer,\n",
    "                       \"critic_optimizer\": critic_optimizer})\n",
    "\n",
    "\n",
    "previous_episode_reward = 0\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc]).to(device)\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    max_steps = 100\n",
    "\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "\n",
    "        \n",
    "        hidden_state = hidden_state.to(device)\n",
    "        # TODO: actor should use  ht + zt\n",
    "        action_probs = actor(hidden_state)[0] # remove batch dimension\n",
    "        value = critic(hidden_state)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state]).view(1, -1).to(device)\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)# could keep it from last output.\n",
    "\n",
    "        # Adjusting shapes\n",
    "        encoded_state_expanded = encoded_state.view(1, -1) \n",
    "        action_probs = action_probs.view(1, -1)\n",
    "        \n",
    "        # Concatenating tensors\n",
    "        # Note: All tensors now have a batch dimension of 1\n",
    "        # Integrated model loss\n",
    "        #TODO: should take action not action_probs\n",
    "        integratedModel_input = torch.cat((encoded_state_expanded, action_probs.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        \n",
    "        # Now, use the flattened input for your integrated model\n",
    "        next_hidden_state, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, state_tensor, hidden_state)\n",
    "        xt = next_state_tensor.view(1, -1)\n",
    "        rt = torch.tensor(reward, device=device, dtype=torch.float).view(1, -1)\n",
    "        ct = torch.tensor(1 - int(done), device=device, dtype=torch.float).view(1, -1)\n",
    "        prediction_loss = (loss_fn(xt_hat, xt) \n",
    "                                + loss_fn(rt_hat, rt) \n",
    "                                + loss_fn(ct_hat, ct))\n",
    "        \n",
    "        zt = zt.view(1, 1, -1)\n",
    "        \n",
    "        threshold = 1\n",
    "        dynamic_loss = torch.max(torch.tensor(threshold), kl_loss(zt_hat, zt.detach()))\n",
    "        representation_loss = torch.max(torch.tensor(threshold), kl_loss(zt, zt_hat.detach()))\n",
    "        \n",
    "        worldModel_loss = Bpred * prediction_loss + Bdyn * dynamic_loss + Brep * representation_loss\n",
    "        \n",
    "        # Critic loss\n",
    "        imagination_horizon = 3 # TODO: 15 in paper\n",
    "        return_lambda = 0.95\n",
    "        #TODO: add lookahead using world model+actor\n",
    "        lookahead_reward = reward + (gamma * (1 - return_lambda) * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "        with torch.no_grad():\n",
    "            imagined_done = done\n",
    "            imagined_ht = next_hidden_state\n",
    "            imagined_zt = worldModel.encode(next_state_tensor, imagined_ht)\n",
    "            imagined_xt = next_state_tensor\n",
    "            for t in range(1, imagination_horizon+1):\n",
    "                if imagined_done:\n",
    "                    break\n",
    "\n",
    "                imagined_action_probs = actor(imagined_ht)[0]\n",
    "                imagined_action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "                imagined_integratedModel_input = torch.cat((imagined_zt.view(1, -1), imagined_action_probs.detach()), dim=1)\n",
    "                # add batch because input needs to have batch, seq, input_size\n",
    "                imagined_integratedModel_input = imagined_integratedModel_input.view(1, 1, -1)\n",
    "                \n",
    "                # Now, use the flattened input for your integrated model\n",
    "                im_next_hidden_state, im_zt_hat, im_rt_hat, im_ct_hat, im_xt_hat , _ = worldModel(imagined_integratedModel_input, imagined_xt, imagined_ht)\n",
    "                imagined_reward = (im_rt_hat + im_ct_hat*(critic(im_next_hidden_state.detach())))\n",
    "                lookahead_reward +=  (gamma ** t) * (return_lambda ** (t-1)) * imagined_reward\n",
    "                \n",
    "                imagined_done = imagined_done or im_ct_hat > 0.5\n",
    "                imagined_ht = im_next_hidden_state\n",
    "                imagined_zt = im_zt_hat\n",
    "                imagined_xt = im_xt_hat\n",
    "            # one last critic estimation\n",
    "            if not imagined_done:\n",
    "                lookahead_reward += (gamma ** imagination_horizon+1) * (return_lambda ** imagination_horizon) * critic(im_next_hidden_state.detach())\n",
    "        \n",
    "        #target_value = reward + (gamma * critic(next_hidden_state.detach()) * (1 - int(done)))\n",
    "        target_value = lookahead_reward\n",
    "        critic_loss = F.mse_loss(value, target_value.detach())\n",
    "\n",
    "        # Actor loss\n",
    "        advantage = target_value - value\n",
    "        actor_loss = -torch.log(action_probs.squeeze(0)[action]) * advantage.detach()\n",
    "        \n",
    "    \n",
    "        # Backpropagation\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critic.parameters(), 100)\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(actor.parameters(), 100)\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        \n",
    "        worldModel_optimizer.zero_grad()\n",
    "        worldModel_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worldModel.parameters(), 1000)\n",
    "        worldModel_optimizer.step()\n",
    "    \n",
    "        # get gradients norms\n",
    "        critic_grads = sum([torch.norm(param.grad) for param in critic.parameters()])\n",
    "        actor_grads = sum([torch.norm(param.grad) for param in actor.parameters()])\n",
    "        worldModel_grads = sum([torch.norm(param.grad) for param in worldModel.parameters()])\n",
    "        \n",
    "        # get weights norms\n",
    "        critic_weights_norm = sum([torch.norm(param) for param in critic.parameters()])\n",
    "        actor_weights_norm = sum([torch.norm(param) for param in actor.parameters()])\n",
    "        worldModel_weights_norm = sum([torch.norm(param) for param in worldModel.parameters()])\n",
    "        \n",
    "        if logging:\n",
    "            wandb.log({\"critic_loss\": critic_loss.item(),\n",
    "                        \"critic_grads\": critic_grads,\n",
    "                        \"critic_weights_norm\": critic_weights_norm,\n",
    "                        \"actor_loss\": actor_loss.item(),\n",
    "                        \"actor_grads\": actor_grads,\n",
    "                        \"actor_weights_norm\": actor_weights_norm,\n",
    "                        \"integratedModel_loss\": worldModel_loss.item(),\n",
    "                        \"integratedModel_grads\": worldModel_grads,\n",
    "                        \"representation_loss\": representation_loss.item(),\n",
    "                        \"dynamic_loss\": dynamic_loss.item(),\n",
    "                        \"prediction_loss\": prediction_loss.item(),\n",
    "                        \"integratedModel_weights_norm\": worldModel_weights_norm,\n",
    "                       \"previous_episode_reward\": previous_episode_reward})\n",
    "        \n",
    "        state = next_state\n",
    "        hidden_state = next_hidden_state.detach()\n",
    "        max_steps -= 1\n",
    "    \n",
    "    previous_episode_reward = sum(episode_rewards)\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "if logging:\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-16T11:16:32.614232Z"
    }
   },
   "id": "c61c1ee5397043e1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the model\n",
    "num_episodes = 100\n",
    "successes = 0\n",
    "for _ in range(num_episodes):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "    autoencoder_losses = []\n",
    "    \n",
    "    max_steps = 100\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "        encoded_state_expanded = encoded_state.view(1, -1)\n",
    "        \n",
    "        action_probs = actor(hidden_state)[0]\n",
    "        value = critic(hidden_state)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state])\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        integratedModel_input = torch.cat((encoded_state_expanded, action_probs.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensor  ,hidden_state)\n",
    "        \n",
    "        state = next_state\n",
    "        max_steps -= 1\n",
    "\n",
    "    \n",
    "    if reward == 1:\n",
    "        successes += 1\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/num_episodes}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e49f3d1bbeb16c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "41413356ea1b0b78",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
