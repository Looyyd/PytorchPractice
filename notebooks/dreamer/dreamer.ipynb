{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.228280Z",
     "start_time": "2024-01-17T13:42:22.187445Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchrl.data import PrioritizedReplayBuffer, ListStorage\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import optuna\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.vector import AsyncVectorEnv, SyncVectorEnv\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "FrozenLake environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f83b02f493c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.230377Z",
     "start_time": "2024-01-17T13:42:22.219629Z"
    }
   },
   "id": "f6f1fa210c2ad0e",
   "execution_count": 469
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_layout_to_tensor(map_layouts):\n",
    "    nrows, ncols = len(map_layouts[0]), len(map_layouts[0][0])\n",
    "    num_statuses = 4\n",
    "    batch_size = len(map_layouts)\n",
    "\n",
    "    # Initialize a tensor for the batch of layouts\n",
    "    layout_tensor = torch.zeros((batch_size, nrows, ncols, num_statuses), device='cpu', dtype=torch.float)\n",
    "\n",
    "    layout_to_val = {b'F': 0, b'H': 1, b'S': 0, b'G': 3,\n",
    "                     'F': 0, 'H': 1, 'S': 0, 'G': 3}\n",
    "\n",
    "    # Precompute all indices for the batch\n",
    "    all_indices = [\n",
    "        layout_to_val[item]\n",
    "        for map_layout in map_layouts\n",
    "        for row in map_layout\n",
    "        for item in row\n",
    "    ]\n",
    "    indices_tensor = torch.tensor(all_indices, device='cpu').view(batch_size, nrows, ncols)\n",
    "\n",
    "    # Update the tensor using advanced indexing\n",
    "    layout_tensor.scatter_(3, indices_tensor.unsqueeze(3), 1)\n",
    "\n",
    "    return layout_tensor\n",
    "\n",
    "\n",
    "def update_start_positions(tensor_layout:Tensor, positions):\n",
    "    nrows, ncols, _ = tensor_layout.size()[1:4]\n",
    "\n",
    "    # Convert positions to a PyTorch tensor if it's not already one\n",
    "    if not isinstance(positions, torch.Tensor):\n",
    "        positions = torch.tensor(positions, dtype=torch.long, device=tensor_layout.device)\n",
    "\n",
    "    # Calculate rows and columns for all positions\n",
    "    rows = positions // ncols\n",
    "    cols = positions % ncols\n",
    "\n",
    "    # Reset the cells to [0, 0, 0, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols] = torch.tensor([0, 0, 0, 0], dtype=tensor_layout.dtype, device=tensor_layout.device)\n",
    "\n",
    "    # Set the start cells to [0, 0, 1, 0]\n",
    "    tensor_layout[torch.arange(positions.size(0)), rows, cols, 2] = 1\n",
    "\n",
    "    return tensor_layout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.242157Z",
     "start_time": "2024-01-17T13:42:22.233971Z"
    }
   },
   "id": "44567364fcc9fa67",
   "execution_count": 470
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n",
      "tensor([[[[0., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = convert_layout_to_tensor(generate_random_map(4))\n",
    "print(tensor)\n",
    "updated_tensor = update_start_positions(tensor, [[0]])\n",
    "print(updated_tensor) # dimensions: batch_size x nrows x ncols x num_statuses\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.242508Z",
     "start_time": "2024-01-17T13:42:22.236508Z"
    }
   },
   "id": "26914cb622923df9",
   "execution_count": 471
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementation of DeamerV3 model\n",
    "![DeamerV3 model](model_names.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd95da8c410c3ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Sequence model that learns the dynamics of the latent space\n",
    "# takes as input h t-1, z t-1, a t-1 and outputs ht\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.256971Z",
     "start_time": "2024-01-17T13:42:22.243801Z"
    }
   },
   "id": "d35de25f9599dd11",
   "execution_count": 472
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, input_size, xt_size, gru_hidden_size, num_layers, zt_dim, num_categories, reward_size, continue_size):\n",
    "        super(WorldModel, self).__init__()\n",
    "\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size, gru_hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "\n",
    "        # Dynamics predictor\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, zt_dim * num_categories)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, reward_size)\n",
    "        )\n",
    "        \n",
    "        # Continue signal predictor\n",
    "        self.continue_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, continue_size)\n",
    "        )\n",
    "        \n",
    "        # Decoder predictor\n",
    "        self.decoder_predictor = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + zt_dim * num_categories, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, xt_size)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(gru_hidden_size + xt_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, zt_dim * num_categories)  # Output logits for each category in each latent dimension\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, ztat, xt, ht_1):\n",
    "        batch_size = ztat.size(0)\n",
    "        if ht_1 is None:\n",
    "            # Initialize hidden state with zeros if not provided\n",
    "            ht_1 = torch.zeros(self.num_layers, ztat.size(0), self.gru_hidden_size).to(xt.device)\n",
    "\n",
    "        # Forward propagate GRU\n",
    "        _ , ht = self.gru(ztat, ht_1)\n",
    "       \n",
    "        zt = self.encode(xt, ht)\n",
    "        \n",
    "        zt_hat = self.dynamics_predictor(ht)\n",
    "\n",
    "        htzt = torch.cat((ht.view(batch_size, -1), zt.view(batch_size, -1)), dim=1)\n",
    "        rt_hat = self.reward_predictor(htzt)\n",
    "        ct_hat = self.continue_predictor(htzt)\n",
    "        xt_hat = self.decoder_predictor(htzt)\n",
    "    \n",
    "        return ht, zt_hat, rt_hat, ct_hat, xt_hat, zt\n",
    "    \n",
    "    def encode(self, xt, ht, temperature=1.0):\n",
    "        batch_size = xt.size(0)\n",
    "        \n",
    "        ht = ht.view(batch_size, -1)# it's not batch first\n",
    "        encoded = self.encoder(torch.cat((xt, ht), dim=1))\n",
    "        softmax = nn.functional.gumbel_softmax(encoded, tau=temperature, hard=True)\n",
    "        return softmax\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.269271Z",
     "start_time": "2024-01-17T13:42:22.260656Z"
    }
   },
   "id": "cc7ed508cc0c5986",
   "execution_count": 473
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\"Symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * torch.log(torch.abs(x) + 1)\n",
    "\n",
    "def symexp(x):\n",
    "    \"\"\"Inverse of symmetric log transformation\"\"\"\n",
    "    return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)\n",
    "\n",
    "class SymlogLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SymlogLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Apply symlog transformation to the target\n",
    "\n",
    "        transformed_target = symlog(target)\n",
    "        # Compute MSE Loss between input and transformed target\n",
    "        return F.mse_loss(input, transformed_target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.277792Z",
     "start_time": "2024-01-17T13:42:22.264333Z"
    }
   },
   "id": "c75090797e8abeae",
   "execution_count": 474
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.ln1(self.fc1(state))\n",
    "        x = self.silu(x)\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.280492Z",
     "start_time": "2024-01-17T13:42:22.268565Z"
    }
   },
   "id": "7f334ff705d60e11",
   "execution_count": 475
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "640b615c97d3cb37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"mps\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.280750Z",
     "start_time": "2024-01-17T13:42:22.271080Z"
    }
   },
   "id": "21168a3b14a069b2",
   "execution_count": 476
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "latent_size = 32 # aka zt size\n",
    "num_categories = 4 # number of categories in latent space\n",
    "\n",
    "ht_size = 32\n",
    "xt_size = 4*4*4\n",
    "reward_size = 1\n",
    "continue_size = 1\n",
    "worldModel = WorldModel(latent_size * num_categories + n_actions, xt_size, ht_size, 1, latent_size, num_categories, reward_size, continue_size).to(device)\n",
    "worldModel_optimizer = optim.Adam(worldModel.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "actor = Actor(ht_size + latent_size*num_categories, n_actions).to(device)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=3e-5, eps=1e-5)\n",
    "critic = Critic(ht_size).to(device)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=3e-5, eps=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.301937Z",
     "start_time": "2024-01-17T13:42:22.282953Z"
    }
   },
   "id": "6186b465c849ee41",
   "execution_count": 477
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_world_model(worldModel, worldModel_optimizer, replay_buffer, batch_size=32, steps=3, logging=False):\n",
    "    loss_fn = SymlogLoss()\n",
    "    experiences, info = replay_buffer.sample(batch_size, return_info=True)\n",
    "    # Unpack experiences\n",
    "\n",
    "    states = experiences[0]\n",
    "\n",
    "    \n",
    "    # get envs from states\n",
    "    envs = []\n",
    "    for state in states:\n",
    "        env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "        env.reset()\n",
    "        env.state = state\n",
    "        envs.append(env)\n",
    "    \n",
    "    # hidden states are not batch first\n",
    "    hidden_states = torch.zeros((1, batch_size, ht_size), device=device, dtype=torch.float)\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc for env in envs]).to(device)\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # step through envs\n",
    "        #TODO: maybe not random steps\n",
    "        actions = torch.tensor([env.action_space.sample() for env in envs], device=device)\n",
    "        next_states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for env, action in zip(envs, actions):\n",
    "            #TODO: if done should reset\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            next_states.append(next_state)\n",
    "        \n",
    "        dones = torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        \n",
    "        next_states_tensor = update_start_positions(layout_tensor, next_states).view(batch_size, -1).to(device)    \n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            states_tensor = update_start_positions(layout_tensor, states)\n",
    "            states_tensor = states_tensor.view(batch_size, -1).to(device)\n",
    "            \n",
    "\n",
    "            zts = worldModel.encode(states_tensor, hidden_states)\n",
    "    \n",
    "        # Integrated model loss\n",
    "        action_one_hot = F.one_hot(actions.detach(), n_actions).view(batch_size, -1).to(device)\n",
    "    \n",
    "\n",
    "        integratedModel_input = torch.cat((zts, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(batch_size, 1, -1)\n",
    "        \n",
    "            \n",
    "        ht, zt_hat, rt_hat, ct_hat, xt_hat, zt = worldModel(integratedModel_input, states_tensor, hidden_states)\n",
    "        xt = next_states_tensor\n",
    "        rt = rewards.view(batch_size, 1)\n",
    "        ct = (1 - dones).view(batch_size, 1)\n",
    "        prediction_loss = (loss_fn(xt_hat, xt) \n",
    "                                + loss_fn(rt_hat, rt) \n",
    "                                + loss_fn(ct_hat, ct))\n",
    "        \n",
    "        zt = zt.view(batch_size, 1, -1)\n",
    "        \n",
    "        threshold = 1\n",
    "        dynamic_loss = torch.max(torch.tensor(threshold), kl_loss(zt_hat, zt.detach()))\n",
    "        representation_loss = torch.max(torch.tensor(threshold), kl_loss(zt, zt_hat.detach()))\n",
    "        \n",
    "        worldModel_loss = Bpred * prediction_loss + Bdyn * dynamic_loss + Brep * representation_loss\n",
    "        \n",
    "        worldModel_optimizer.zero_grad()\n",
    "        worldModel_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(worldModel.parameters(), 1000)\n",
    "        worldModel_optimizer.step()\n",
    "        \n",
    "        states = next_states\n",
    "        hidden_states = ht.detach()\n",
    "        \n",
    "        if logging:\n",
    "            grad_norm = sum([torch.norm(param.grad) for param in worldModel.parameters()])\n",
    "            wandb.log({\"worldModel_loss\": worldModel_loss.item(),\n",
    "                       \"worldModel_grads\": grad_norm,\n",
    "                       \"representation_loss\": representation_loss.item(),\n",
    "                       \"dynamic_loss\": dynamic_loss.item(),\n",
    "                       \"prediction_loss\": prediction_loss.item()})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.313524Z",
     "start_time": "2024-01-17T13:42:22.304897Z"
    }
   },
   "id": "965aa3f2e9e5d8fd",
   "execution_count": 478
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x2ca809810>"
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.313901Z",
     "start_time": "2024-01-17T13:42:22.307350Z"
    }
   },
   "id": "cfea68ef4bdce772",
   "execution_count": 479
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update_actor_critic(actor, critic, actor_optimizer, critic_optimizer, worldModel, actor_critics_replay_buffer, batch_size=32, logging=False):\n",
    "    #sample from buffer\n",
    "    experiences, info = actor_critics_replay_buffer.sample(batch_size, return_info=True)\n",
    "    # Unpack experiences\n",
    "    ht, zt, state_tensors, rewards, dones = experiences # actor states is zt and ht\n",
    "    \n",
    "\n",
    "    dones = dones.float().to(device).view(batch_size, 1).detach()\n",
    "    rewards = rewards.float().to(device).view(batch_size, 1).detach()\n",
    "    \n",
    "    ht = ht.view(batch_size, 1, -1).to(device)\n",
    "\n",
    "    \n",
    "    # Critic loss\n",
    "    imagination_horizon = 3 # TODO: 15 in paper\n",
    "    gamma = 0.997\n",
    "    return_lambda = 0.95\n",
    "    critic_estimation = critic(ht.detach()).view(batch_size, 1)\n",
    "    value = critic_estimation.clone()\n",
    "    lookahead_reward = rewards + (gamma * (1 - return_lambda) * critic_estimation * (1 - dones))\n",
    "    with torch.no_grad():\n",
    "        imagined_dones = dones.clone()\n",
    "        im_actor_states = torch.cat((ht.view(batch_size, -1), zt.view(batch_size,-1)), dim=1)\n",
    "        imagined_xt = state_tensors.view(batch_size, -1)\n",
    "        imagined_ht = ht.view(1, batch_size, -1) # not batch first\n",
    "        imagined_zt = zt\n",
    "        for t in range(1, imagination_horizon+1):\n",
    "            if imagined_dones.all():\n",
    "                break\n",
    "            \n",
    "            imagined_action_probs = actor(im_actor_states)\n",
    "            imagined_action = torch.multinomial(imagined_action_probs, num_samples=1)\n",
    "            \n",
    "            \n",
    "            im_action_one_hot = F.one_hot(imagined_action, n_actions).view(batch_size, -1).to(device)\n",
    "\n",
    "            imagined_integratedModel_input = torch.cat((imagined_zt.view(batch_size, -1), im_action_one_hot.detach()), dim=1)\n",
    "            # add batch because input needs to have batch, seq, input_size\n",
    "            imagined_integratedModel_input = imagined_integratedModel_input.view(batch_size, 1, -1)\n",
    "            \n",
    "            # Now, use the flattened input for your integrated model\n",
    "            im_next_hidden_state, im_zt_hat, im_rt_hat, im_ct_hat, im_xt_hat , _ = worldModel(imagined_integratedModel_input, imagined_xt, imagined_ht)\n",
    "\n",
    "            im_next_hidden_state = im_next_hidden_state.view(batch_size, -1)\n",
    "            \n",
    "            imagined_dones = torch.logical_or(imagined_dones, im_ct_hat > 0.5)\n",
    "            critic_estimation = critic(im_next_hidden_state.detach())\n",
    "            imagined_reward = im_rt_hat + imagined_dones * critic_estimation\n",
    "\n",
    "\n",
    "            lookahead_reward +=  (gamma ** t) * (return_lambda ** (t-1)) * imagined_reward\n",
    "            \n",
    "            \n",
    "            imagined_ht = im_next_hidden_state.view(1, batch_size, -1)\n",
    "            imagined_zt = im_zt_hat\n",
    "            imagined_xt = im_xt_hat\n",
    "            im_actor_states = torch.cat((imagined_ht.view(batch_size, -1), imagined_zt.view(batch_size,-1)), dim=1)\n",
    "\n",
    "        # one last critic estimation\n",
    "        if not imagined_dones.all():\n",
    "            lookahead_reward += imagined_dones * (gamma ** imagination_horizon+1) * (return_lambda ** imagination_horizon) * critic(im_next_hidden_state.detach())\n",
    "    \n",
    "    target_value = lookahead_reward\n",
    "    critic_loss = F.mse_loss(value, target_value.detach())\n",
    "\n",
    "    # Actor loss\n",
    "    advantage = target_value - value.clone().detach()\n",
    "    # Calculate Actor Loss\n",
    "    # Assuming actions are in the form of indices of the chosen actions\n",
    "    action_probs = actor( torch.cat((ht.view(batch_size, -1), zt.view(batch_size,-1)), dim=1).detach() )  # Probabilities of actions from the actor network\n",
    "    # take actions from probs\n",
    "    actions = torch.multinomial(action_probs, num_samples=1)\n",
    "    \n",
    "    # Gather the probabilities of the actions taken\n",
    "    gathered_probs = action_probs.gather(1, actions.view(-1, 1))\n",
    "    \n",
    "    # Add a small number to probabilities to avoid log(0)\n",
    "    actor_loss = -torch.log(gathered_probs + 1e-8) * advantage.detach()\n",
    "    actor_loss = actor_loss.mean()\n",
    "\n",
    "    # Backpropagation\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), 100)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), 100)\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    if logging:\n",
    "        actor_grads = sum([torch.norm(param.grad) for param in actor.parameters()])\n",
    "        critic_grads = sum([torch.norm(param.grad) for param in critic.parameters()])\n",
    "        actor_weights_norm = sum([torch.norm(param) for param in actor.parameters()])\n",
    "        critic_weights_norm = sum([torch.norm(param) for param in critic.parameters()])\n",
    "        wandb.log({\"critic_loss\": critic_loss.item(),\n",
    "                    \"critic_grads\": critic_grads,\n",
    "                    \"critic_weights_norm\": critic_weights_norm,\n",
    "                    \"actor_loss\": actor_loss.item(),\n",
    "                    \"actor_grads\": actor_grads,\n",
    "                    \"actor_weights_norm\": actor_weights_norm})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.322328Z",
     "start_time": "2024-01-17T13:42:22.309995Z"
    }
   },
   "id": "f446ad77a4fd9f3d",
   "execution_count": 480
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collect_experience(encoded_state_flat, hidden_state, env):\n",
    "    \"\"\"\n",
    "    Handles interaction with the environment and collects experience.\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_state = hidden_state.to(device)\n",
    "    \n",
    "    actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "    \n",
    "    action_probs = actor(actor_state)[0]  # remove batch dimension\n",
    "    action = torch.multinomial(action_probs, 1).item()\n",
    "    \n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    return state_tensor, action, reward, next_state, done\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:42:22.323125Z",
     "start_time": "2024-01-17T13:42:22.320046Z"
    }
   },
   "id": "2ae8c26b1d8d2833",
   "execution_count": 481
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [02:24<00:00,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 16\n",
    "training_delta = 10 # number of episodes to train world model before training actor critic\n",
    "num_episodes = batch_size + training_delta + 30\n",
    "gamma = 0.997\n",
    "Bpred = 1\n",
    "Bdyn = 0.5\n",
    "Brep = 0.1\n",
    "loss_fn = SymlogLoss()\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"Dreamerv3 Reproduction\",\n",
    "               name=\"Trying per + batches\",\n",
    "               reinit=True,\n",
    "               config={\"num_episodes\": num_episodes,\n",
    "                       \"gamma\": gamma,\n",
    "                       \"latent_size\": latent_size,\n",
    "                       \"num_categories\": num_categories,\n",
    "                       \"ht_size\": ht_size,\n",
    "                       \"reward_size\": reward_size,\n",
    "                       \"continue_size\": continue_size,\n",
    "                       \"worldModel_optimizer\": worldModel_optimizer,\n",
    "                       \"actor_optimizer\": actor_optimizer,\n",
    "                       \"critic_optimizer\": critic_optimizer})\n",
    "\n",
    "\n",
    "previous_episode_reward = 0\n",
    "# Parameters\n",
    "buffer_size = 10000  # Adjust as needed\n",
    "alpha = 0.6  # Adjust as needed for prioritization (0 for uniform, 1 for fully prioritized)\n",
    "\n",
    "# Initialize buffer\n",
    "world_model_replay_buffer = PrioritizedReplayBuffer(alpha=alpha, storage=ListStorage(buffer_size), beta=0.4)\n",
    "actor_critics_replay_buffer = PrioritizedReplayBuffer(alpha=alpha, storage=ListStorage(buffer_size), beta=0.4)\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    # Reset environment and episode-specific variables\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc]).to(device)\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    max_steps = n_states * n_actions\n",
    "    \n",
    "    while not done and max_steps > 0:\n",
    "        # --- Collect Experience ---\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "        # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "\n",
    "        state_tensor, action, reward, next_state, done = collect_experience(encoded_state, hidden_state, env)\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        # Add experience to world model buffer\n",
    "        world_model_replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # --- World Model Update ---\n",
    "        if len(world_model_replay_buffer) > batch_size:\n",
    "            update_world_model(worldModel, worldModel_optimizer, world_model_replay_buffer, batch_size=batch_size, logging=logging)\n",
    "        \n",
    "        # --- Actor-Critic Update ---\n",
    "        if episode > batch_size + training_delta:\n",
    "            experience_actor_critic = (hidden_state, encoded_state, state_tensor, reward, done)\n",
    "            actor_critics_replay_buffer.add(experience_actor_critic)\n",
    "            if len(world_model_replay_buffer) > batch_size:\n",
    "                update_actor_critic(actor, critic, actor_optimizer, critic_optimizer, worldModel, actor_critics_replay_buffer, batch_size=batch_size, logging=logging)\n",
    "        \n",
    "        # --- Logging and State Updates ---\n",
    "        if logging:\n",
    "            wandb.log({\"previous_episode_reward\": previous_episode_reward})\n",
    "        \n",
    "        state = next_state\n",
    "        integratedModel_input = torch.cat((encoded_state, F.one_hot(torch.tensor(action), n_actions).view(1, -1).to(device)), dim=1)\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensor, hidden_state)\n",
    "        max_steps -= 1\n",
    "    \n",
    "    previous_episode_reward = sum(episode_rewards)\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "if logging:\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:44:47.215965Z",
     "start_time": "2024-01-17T13:42:22.322270Z"
    }
   },
   "id": "c61c1ee5397043e1",
   "execution_count": 482
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "num_episodes = 100\n",
    "successes = 0\n",
    "for _ in range(num_episodes):\n",
    "    actor.eval()\n",
    "    critic.eval()\n",
    "    worldModel.eval()\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    layout_tensor = convert_layout_to_tensor([env.desc])\n",
    "    hidden_state = torch.zeros((1, 1, ht_size), device=device, dtype=torch.float)\n",
    "    \n",
    "    autoencoder_losses = []\n",
    "    \n",
    "    max_steps = 100\n",
    "    while not done and max_steps > 0:\n",
    "        state_tensor = update_start_positions(layout_tensor, [state])\n",
    "        state_tensor = state_tensor.view(1, -1).to(device)\n",
    "        with torch.no_grad():\n",
    "            # Encode the state\n",
    "            encoded_state = worldModel.encode(state_tensor, hidden_state)\n",
    "        encoded_state_flat = encoded_state.view(1, -1)\n",
    "        \n",
    "        actor_state = torch.cat((hidden_state.view(1, -1), encoded_state_flat), dim=1)\n",
    "        action_probs = actor(actor_state)[0]\n",
    "        value = critic(hidden_state)\n",
    "\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        action_one_hot = F.one_hot(torch.tensor(action), n_actions).view(1, -1).to(device)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state_tensor = update_start_positions(layout_tensor, [next_state])\n",
    "\n",
    "        # Store rewards for later use\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        integratedModel_input = torch.cat((encoded_state_flat, action_one_hot.detach()), dim=1)\n",
    "        # add batch because input needs to have batch, seq, input_size\n",
    "        integratedModel_input = integratedModel_input.view(1, 1, -1)\n",
    "        hidden_state, _, _, _, _, _ = worldModel(integratedModel_input, state_tensor  ,hidden_state)\n",
    "        \n",
    "        state = next_state\n",
    "        max_steps -= 1\n",
    "\n",
    "    \n",
    "    if reward == 1:\n",
    "        successes += 1\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/num_episodes}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:45:05.543430Z",
     "start_time": "2024-01-17T13:44:47.214862Z"
    }
   },
   "id": "6e49f3d1bbeb16c9",
   "execution_count": 483
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:45:05.550059Z",
     "start_time": "2024-01-17T13:45:05.544510Z"
    }
   },
   "id": "41413356ea1b0b78",
   "execution_count": 483
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
