{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.quantization import quantize_dynamic\n",
    "torch.backends.quantized.engine = 'qnnpack' # https://github.com/pytorch/pytorch/issues/29327\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:04.337309Z",
     "start_time": "2024-01-02T11:49:04.319318Z"
    }
   },
   "id": "9d8b416ccd7e5a5d"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:04.349455Z",
     "start_time": "2024-01-02T11:49:04.339497Z"
    }
   },
   "id": "feaf0cb8cd1a7ace"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Define a simple Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=28*28, encoding_dim=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Layer sizes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer1_size = 128\n",
    "        self.hidden_layer2_size = 64\n",
    "        self.encoding_dim = encoding_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_layer1_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.hidden_layer1_size, self.hidden_layer2_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.hidden_layer2_size, self.encoding_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.encoding_dim, self.hidden_layer2_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.hidden_layer2_size, self.hidden_layer1_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.hidden_layer1_size, self.input_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:04.350003Z",
     "start_time": "2024-01-02T11:49:04.343485Z"
    }
   },
   "id": "dfdf19bfd61458e7"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2)\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:04.394910Z",
     "start_time": "2024-01-02T11:49:04.346448Z"
    }
   },
   "id": "85aa079eff32acbb"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:04.415938Z",
     "start_time": "2024-01-02T11:49:04.394744Z"
    }
   },
   "id": "9031eb59fb3ec59a"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2648, Time: 2.41s\n",
      "Epoch [2/5], Loss: 0.2606, Time: 2.32s\n",
      "Epoch [3/5], Loss: 0.2788, Time: 2.26s\n",
      "Epoch [4/5], Loss: 0.2739, Time: 2.28s\n",
      "Epoch [5/5], Loss: 0.2721, Time: 2.35s\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(model, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Time: {time.time() - start_time:.2f}s')\n",
    "\n",
    "# Train the model\n",
    "train(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:16.045614Z",
     "start_time": "2024-01-02T11:49:04.416082Z"
    }
   },
   "id": "3dc20d5e0a1ee9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Baseline perf:\n",
    "Epoch [1/5], Loss: 0.2267, Time: 2.40s\n",
    "Epoch [2/5], Loss: 0.2169, Time: 2.30s\n",
    "Epoch [3/5], Loss: 0.1915, Time: 2.26s\n",
    "Epoch [4/5], Loss: 0.1854, Time: 2.27s\n",
    "Epoch [5/5], Loss: 0.1928, Time: 2.34s\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb72ba768ed7eed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Post-Training Quantization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48ef3a179e65b345"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def quantize_model(model):\n",
    "    # Specify the layers to be quantized\n",
    "    quantized_model = quantize_dynamic(\n",
    "        model, \n",
    "        {nn.Linear},  # Specify the layers you want to quantize\n",
    "        dtype=torch.qint8,\n",
    "    )\n",
    "    return quantized_model\n",
    "def evaluate_inference_time(model, data_loader, device):\n",
    "    model= model.to(device)\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "            model(img)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "def validate_model(model, data_loader, criterion, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, _ in data_loader:\n",
    "            img = img.view(img.size(0), -1).to(device)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:16.050087Z",
     "start_time": "2024-01-02T11:49:16.042692Z"
    }
   },
   "id": "1d71900347a23721"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model - Inference Time: 2.173731803894043, Validation Loss: 0.26968853175640106\n",
      "Quantized Model - Inference Time: 1.3928959369659424, Validation Loss: 0.26968856155872345\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a validation loader named 'val_loader' and a criterion\n",
    "# Quantize the model\n",
    "quantized_autoencoder = quantize_model(model.to('cpu'))\n",
    "\n",
    "quant_device = 'cpu'  # cpu or mps\n",
    "# Measure inference time\n",
    "original_time = evaluate_inference_time(model, val_loader, quant_device)\n",
    "quantized_time = evaluate_inference_time(quantized_autoencoder, val_loader, quant_device)\n",
    "\n",
    "# Validate the models\n",
    "original_val_loss = validate_model(model, val_loader, criterion, quant_device)\n",
    "quantized_val_loss = validate_model(quantized_autoencoder, val_loader, criterion, quant_device)\n",
    "\n",
    "print(f\"Original Model - Inference Time: {original_time}, Validation Loss: {original_val_loss}\")\n",
    "print(f\"Quantized Model - Inference Time: {quantized_time}, Validation Loss: {quantized_val_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:49:23.284277Z",
     "start_time": "2024-01-02T11:49:16.047651Z"
    }
   },
   "id": "576745cc38dedaee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "On cpu:\n",
    "Original Model - Inference Time: 1.9714100360870361, Validation Loss: 0.2335045002400875\n",
    "Quantized Model(qint8)- Inference Time: 1.4053568840026855, Validation Loss: 0.23385296761989594"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b46456d81b349e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quantization Aware Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ed1e9aa5dc499f"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def prepare_model_for_qat(model):\n",
    "    # Fuse Conv, bn and relu\n",
    "    # (Add if you have layers that can be fused)\n",
    "\n",
    "    # Prepare the model for quantization aware training\n",
    "    model.train()\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    torch.quantization.prepare_qat(model, inplace=True)\n",
    "    return model\n",
    "def train_with_qat(model, train_loader, num_epochs=5):\n",
    "    model.train()\n",
    "    model.to(\"mps\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            img, _ = data\n",
    "            img = img.view(img.size(0), -1)\n",
    "            output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Freeze quantizer parameters\n",
    "        if epoch > 3:\n",
    "            model.apply(torch.quantization.disable_observer)\n",
    "        if epoch > 2:\n",
    "            model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "        # Print loss\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Convert to quantized model\n",
    "    torch.quantization.convert(model, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:56:22.407414Z",
     "start_time": "2024-01-02T11:56:22.401600Z"
    }
   },
   "id": "31324ac83d794487"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [MPS, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/mps/MPSFallback.mm:75 [backend fallback]\nQuantizedCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1137 [kernel]\nBackendSelect: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[1;32m      7\u001B[0m qat_model \u001B[38;5;241m=\u001B[39m qat_model\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m train_with_qat(qat_model, train_loader)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Validate the QAT model\u001B[39;00m\n\u001B[1;32m     10\u001B[0m qat_val_loss \u001B[38;5;241m=\u001B[39m validate_model(qat_model, val_loader, criterion, quant_device)\n",
      "Cell \u001B[0;32mIn[67], line 17\u001B[0m, in \u001B[0;36mtrain_with_qat\u001B[0;34m(model, train_loader, num_epochs)\u001B[0m\n\u001B[1;32m     15\u001B[0m img, _ \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     16\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mview(img\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m output \u001B[38;5;241m=\u001B[39m model(img)\n\u001B[1;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, img)\n\u001B[1;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[51], line 31\u001B[0m, in \u001B[0;36mAutoencoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 31\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[1;32m     32\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(x)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:168\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mquantized\u001B[38;5;241m.\u001B[39mlinear(\n\u001B[1;32m    169\u001B[0m         x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_packed_params\u001B[38;5;241m.\u001B[39m_packed_params, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzero_point)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/PytorchPractice/lib/python3.11/site-packages/torch/_ops.py:692\u001B[0m, in \u001B[0;36mOpOverloadPacket.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001B[39;00m\n\u001B[1;32m    689\u001B[0m     \u001B[38;5;66;03m# is still callable from JIT\u001B[39;00m\n\u001B[1;32m    690\u001B[0m     \u001B[38;5;66;03m# We save the function ptr as the `op` attribute on\u001B[39;00m\n\u001B[1;32m    691\u001B[0m     \u001B[38;5;66;03m# OpOverloadPacket to access it here.\u001B[39;00m\n\u001B[0;32m--> 692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_op(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs \u001B[38;5;129;01mor\u001B[39;00m {})\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [MPS, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/mps/MPSFallback.mm:75 [backend fallback]\nQuantizedCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:1137 [kernel]\nBackendSelect: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_5ae0635zuj/croot/pytorch-select_1700511177724/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the model for QAT\n",
    "# The operator 'aten::_fused_moving_avg_obs_fq_helper' is not currently implemented for the MPS device\n",
    "qat_model = prepare_model_for_qat(model.to(\"mps\"))\n",
    "\n",
    "# Train the model with QAT\n",
    "criterion = nn.MSELoss()\n",
    "qat_model = qat_model.to(\"mps\")\n",
    "train_with_qat(qat_model, train_loader)\n",
    "# Validate the QAT model\n",
    "qat_val_loss = validate_model(qat_model, val_loader, criterion, quant_device)\n",
    "qat_inference_time = evaluate_inference_time(qat_model, val_loader, quant_device)\n",
    "\n",
    "print(f\"QAT Model - Inference Time: {qat_inference_time}, Validation Loss: {qat_val_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:56:24.969345Z",
     "start_time": "2024-01-02T11:56:23.653780Z"
    }
   },
   "id": "bdca119510228bf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-02T11:50:32.859813Z"
    }
   },
   "id": "279732e6fbec912b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
